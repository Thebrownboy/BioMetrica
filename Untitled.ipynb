{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a082f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  بسم الله الرحمن الرحيم \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d351ef7",
   "metadata": {},
   "source": [
    "- visulaize the imbalancing in the data \n",
    "- dealing with this probelm \n",
    "- detect the outliers \n",
    "- deal with them \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cca6f",
   "metadata": {},
   "source": [
    "## importing important packages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311c0a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Solarize_Light2',\n",
       " '_classic_test_patch',\n",
       " '_mpl-gallery',\n",
       " '_mpl-gallery-nogrid',\n",
       " 'bmh',\n",
       " 'classic',\n",
       " 'dark_background',\n",
       " 'fast',\n",
       " 'fivethirtyeight',\n",
       " 'ggplot',\n",
       " 'grayscale',\n",
       " 'seaborn',\n",
       " 'seaborn-bright',\n",
       " 'seaborn-colorblind',\n",
       " 'seaborn-dark',\n",
       " 'seaborn-dark-palette',\n",
       " 'seaborn-darkgrid',\n",
       " 'seaborn-deep',\n",
       " 'seaborn-muted',\n",
       " 'seaborn-notebook',\n",
       " 'seaborn-paper',\n",
       " 'seaborn-pastel',\n",
       " 'seaborn-poster',\n",
       " 'seaborn-talk',\n",
       " 'seaborn-ticks',\n",
       " 'seaborn-white',\n",
       " 'seaborn-whitegrid',\n",
       " 'tableau-colorblind10']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns \n",
    "plt.style.use('default')\n",
    "#-------------------------------------------------------------------------------------\n",
    "from sklearn.utils import resample \n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers \n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "plt.style.available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e47aa2",
   "metadata": {},
   "source": [
    "## Reading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c949b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/body_level_classification_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d83fb",
   "metadata": {},
   "source": [
    "## exploring the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b0f69",
   "metadata": {},
   "source": [
    "### exploring the imbalances in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab472ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df[\"Body_Level\"].value_counts() \\\n",
    "     .plot(kind='bar', title='Body_Level Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ab399e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Male      746\n",
       "Female    731\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1237a611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df.plot.box(column='Age', by='Gender', figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e12e97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGxCAYAAACEFXd4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA17UlEQVR4nO3deXxU5b3H8e+YZdiSSAhkkksMsQYUAi6kQhBlCQSDLIJeaKkSJFqURVLghYC3FVpLABWwUlCvMQgIwQXQlqUEWSwC9wYKslgR2wCJJKZiyGaYhHDuH1ymDgkQhgkzOfm8X6/zquecZ575ned1NN8+85wZi2EYhgAAAEzqJk8XAAAAUJcIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIO0A9snTpUlksFsfm6+ur1q1b64knntA333xTrd3evXs9WO2V3XPPPbJYLHr55Zdd7mPDhg2aOXNmrdv37NlTMTExV213/PhxWSwWLV261OXaAHgPwg5QD6Wnp2v37t3KzMzUU089pVWrVun+++9XWVmZp0urlQMHDmj//v2SpLS0NJf72bBhg2bNmuWushzCwsK0e/duPfTQQ27vG8CNR9gB6qGYmBh17dpVvXr10gsvvKCpU6cqOztb69at83RptfLWW29Jkh566CF9+eWX2rVrl4crcma1WtW1a1e1bNnS06VUU1VVJbvd7ukygHqFsAOYQNeuXSVJJ06ccDpeUlKiZ555RiEhIWrRooWGDh2qU6dOOc4nJycrODhYP/zwQ7U+e/furQ4dOjj233//fXXp0kVBQUFq0qSJbr31Vo0ePfqaaz179qxWrlypzp07a8GCBZKkt99+u8a2mzZtUnx8vOM977jjDqWmpkqSRo0apT/+8Y+S5PTR3vHjx69aQ1ZWlu6//37HdcyZM0fnz593nK/pY6x//etf+uUvf6mIiAhZrVa1bNlS9913n7Zs2eLU99tvv60777xTjRo1UnBwsIYMGaK///3v1Wr47//+b7Vt21ZWq1Xt27fXypUrNWrUKLVp06ZaHfPmzdOLL76oqKgoWa1Wbdu2TWfPntXkyZN11113KSgoSMHBwYqLi9NHH31U7b0sFovGjx+v9PR0tWvXTo0bN1ZsbKz27NkjwzD00ksvKSoqSs2aNVPv3r319ddfX3UMgfqEsAOYwMU/TpfORDz55JPy8/PTypUrNW/ePG3fvl2PPfaY4/zEiRNVWFiolStXOr3uiy++0LZt2zRu3DhJ0u7duzV8+HDdeuutysjI0Pr16/Wb3/xG586du+Za16xZo8LCQo0ePVrR0dHq3r27Vq9erdLSUqd2aWlp6t+/v86fP6/XX39df/rTn/Tss88qNzdXkvTrX/9ajz76qKO+i1tYWNgV3z8/P1+/+MUv9Nhjj+njjz9WYmKipk+frhUrVlzxdY8//rjWrVun3/zmN9q8ebPeeust9enTR6dPn3a0SU1NVXJysjp06KA1a9bo1Vdf1cGDBxUXF6djx4452r355pv65S9/qU6dOmnNmjX6r//6L82aNUvbt2+v8b3/8Ic/aOvWrXr55Ze1ceNG3X777bLb7fr+++81ZcoUrVu3TqtWrVL37t01dOhQLVu2rFoff/7zn/XWW29pzpw5WrVqlUpKSvTQQw9p8uTJ+uyzz7Ro0SK9+eab+uKLL/TII4/IMIwrjgdQrxgA6o309HRDkrFnzx6jsrLSKCkpMf785z8bLVu2NAICAoz8/HyndmPHjnV6/bx58wxJRl5enuNYjx49jLvuusup3TPPPGMEBgYaJSUlhmEYxssvv2xIMs6cOXPd19C7d2+jUaNGRmFhoVOtaWlpjjYlJSVGYGCg0b17d+P8+fOX7WvcuHHGtfxnrEePHoYk43/+53+cjrdv397o16+fYz87O9uQZKSnpzuONWvWzEhJSbls34WFhUbjxo2N/v37Ox0/efKkYbVajREjRhiGYRhVVVWGzWYzunTp4tTuxIkThp+fnxEZGVmtjp/85CdGRUXFFa/t3LlzRmVlpZGcnGzcfffdTuckGTabzSgtLXUcW7dunSHJuOuuu5zGeOHChYYk4+DBg1d8P6A+YWYHqIe6du0qPz8/BQQEaMCAAbLZbNq4caNCQ0Od2g0aNMhpv1OnTpKcP+6aOHGiDhw4oM8++0ySVFxcrOXLlyspKUnNmjWTJP30pz+VJA0bNkzvvfee05Nf1yI7O1vbtm3T0KFDdfPNN0uS/vM//1MBAQFOH2Xt2rVLxcXFGjt2rCwWi0vvdTk2m0333nuv07FOnTpV+wjwUvfee6+WLl2qF198UXv27FFlZaXT+d27d6u8vFyjRo1yOh4REaHevXvrk08+kSQdPXpU+fn5GjZsmFO7W265Rffdd1+N7z1o0CD5+flVO/7+++/rvvvuU7NmzeTr6ys/Pz+lpaXV+LFZr1691LRpU8f+HXfcIUlKTEx0GuOLx682HkB9QtgB6qFly5YpKytL+/fv16lTp3Tw4MEa/1C2aNHCad9qtUqSysvLHccGDx6sNm3aONa/LF26VGVlZY6PsCTpgQce0Lp163Tu3DmNHDlSrVu3VkxMjFatWnVNdb/99tsyDEOPPvqozpw5ozNnzqiyslKDBg3SZ599pi+//FLShfUxktS6detr6r82Lh0T6cK4/HhMarJ69WolJSXprbfeUlxcnIKDgzVy5Ejl5+dLkuPjrJo+RgsPD3ecv/i/lwbTyx27XJ9r1qzRsGHD9B//8R9asWKFdu/eraysLI0ePVpnz56t1j44ONhp39/f/4rHa+oDqK8IO0A9dMcddyg2NlZ33XXXVdeoXM1NN92kcePG6YMPPlBeXp4WL16s+Ph4tWvXzqnd4MGD9cknn6ioqEjbt29X69atNWLECO3evbtW73P+/HnHgt+hQ4eqefPmju3dd9+V9O+FyhfXHl1cn+MNQkJCtHDhQh0/flwnTpxQamqq1qxZ45jJuRii8vLyqr321KlTCgkJcWr37bffVmt3MThdqqbZrRUrVigqKkqrV6/Www8/rK5duyo2NpYntYAaEHYA6Mknn5S/v79+8Ytf6OjRoxo/fvxl21qtVvXo0UNz586VJMf35VzNX/7yF+Xm5mrcuHHatm1bta1Dhw5atmyZzp07p27duikoKEivv/76FRfK1jRTdSPccsstGj9+vPr27au//e1vkqS4uDg1bty42kLn3Nxcbd26VfHx8ZKkdu3ayWaz6b333nNqd/LkyWt6BN9iscjf398pCOXn59f4NBbQ0Pl6ugAAnnfzzTdr5MiRWrJkiSIjIzVw4ECn87/5zW+Um5ur+Ph4tW7dWmfOnNGrr74qPz8/9ejRo1bvkZaWJl9fX82YMUPh4eHVzo8ZM0bPPvus1q9fr8GDB+uVV17Rk08+qT59+uipp55SaGiovv76a33++edatGiRJKljx46SpLlz5yoxMVE+Pj7q1KmT46MYdykqKlKvXr00YsQI3X777QoICFBWVpY2bdqkoUOHSrowhr/+9a81Y8YMjRw5Uj//+c91+vRpzZo1S40aNdILL7wg6cJM2qxZszRmzBg9+uijGj16tM6cOaNZs2YpLCxMN91Uu/8POmDAAK1Zs0Zjx47Vo48+qpycHP3ud79TWFiY05NfAAg7AP7f8OHDtWTJEj3zzDPV/uB26dJFe/fu1XPPPad//etfuvnmmxUbG6utW7c6fRfP5Xz33Xf605/+pAEDBtQYdKQLj3Y/99xzSktL0+DBg5WcnKzw8HDNnTtXTz75pAzDUJs2bZSUlOR4zYgRI/TZZ59p8eLF+u1vfyvDMJSdne30XTXu0KhRI3Xp0kXLly/X8ePHVVlZqVtuuUXPPfecpk6d6mg3ffp0tWrVSn/4wx+0evVqNW7cWD179tTs2bMVHR3taPfLX/7S8f05Q4YMUZs2bTRt2jR99NFHOnnyZK1qeuKJJ1RQUKDXX39db7/9tm699VZNmzZNubm5dfKt0kB9ZjGuNEcMoMGYPHmylixZopycnBoX8aJunTlzRm3bttXDDz+sN99809PlAKbCzA7QwO3Zs0dfffWVFi9erDFjxhB0boD8/Hz9/ve/V69evdSiRQudOHFCCxYsUElJiSZOnOjp8gDTYWYHaOAsFouaNGmi/v37Kz093fHdOteiqqrqiguJLRaLfHx8rqdMUyksLNTIkSOVlZWl77//Xk2aNFHXrl01a9YsdenSxdPlAaZD2AFw3dq0aXPFL6Hr0aPHZX8KAQDqGh9jAbhuf/rTn674/S4BAQE3sBoAcMbMDgAAMDW+VBAAAJgaH2PpwtfYnzp1SgEBAW7/0UEAAFA3DMNQSUmJwsPDr/iFnIQdXfjdmoiICE+XAQAAXJCTk3PFHw4m7OjfiydzcnIUGBjo4WoAAEBtFBcXKyIi4qoPQRB29O9fFA4MDCTsAABQz1xtCQoLlAEAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKn5eroAs2szbX2d9X18zkN11jcAAGbBzA4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1j4adJUuWqFOnTgoMDFRgYKDi4uK0ceNGx/lRo0bJYrE4bV27dnXqw263a8KECQoJCVHTpk01aNAg5ebm3uhLAQAAXsqjYad169aaM2eO9u7dq71796p3794aPHiwjhw54mjz4IMPKi8vz7Ft2LDBqY+UlBStXbtWGRkZ2rlzp0pLSzVgwABVVVXd6MsBAABeyNeTbz5w4ECn/d///vdasmSJ9uzZow4dOkiSrFarbDZbja8vKipSWlqali9frj59+kiSVqxYoYiICG3ZskX9+vWr2wsAAABez2vW7FRVVSkjI0NlZWWKi4tzHN++fbtatWqltm3b6qmnnlJBQYHj3L59+1RZWamEhATHsfDwcMXExGjXrl2XfS+73a7i4mKnDQAAmJPHw86hQ4fUrFkzWa1WPf3001q7dq3at28vSUpMTNS7776rrVu36pVXXlFWVpZ69+4tu90uScrPz5e/v7+aN2/u1GdoaKjy8/Mv+56pqakKCgpybBEREXV3gQAAwKM8+jGWJLVr104HDhzQmTNn9OGHHyopKUk7duxQ+/btNXz4cEe7mJgYxcbGKjIyUuvXr9fQoUMv26dhGLJYLJc9P336dE2aNMmxX1xcTOABAMCkPB52/P39ddttt0mSYmNjlZWVpVdffVVvvPFGtbZhYWGKjIzUsWPHJEk2m00VFRUqLCx0mt0pKChQt27dLvueVqtVVqvVzVcCAAC8kcc/xrqUYRiOj6kudfr0aeXk5CgsLEyS1LlzZ/n5+SkzM9PRJi8vT4cPH75i2AEAAA2HR2d2ZsyYocTEREVERKikpEQZGRnavn27Nm3apNLSUs2cOVOPPPKIwsLCdPz4cc2YMUMhISEaMmSIJCkoKEjJycmaPHmyWrRooeDgYE2ZMkUdO3Z0PJ0FAAAaNo+GnW+//VaPP/648vLyFBQUpE6dOmnTpk3q27evysvLdejQIS1btkxnzpxRWFiYevXqpdWrVysgIMDRx4IFC+Tr66thw4apvLxc8fHxWrp0qXx8fDx4ZQAAwFtYDMMwPF2EpxUXFysoKEhFRUUKDAx0a99tpq13a38/dnzOQ3XWNwAA3q62f7+9bs0OAACAOxF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqRF2AACAqXk07CxZskSdOnVSYGCgAgMDFRcXp40bNzrOG4ahmTNnKjw8XI0bN1bPnj115MgRpz7sdrsmTJigkJAQNW3aVIMGDVJubu6NvhQAAOClPBp2WrdurTlz5mjv3r3au3evevfurcGDBzsCzbx58zR//nwtWrRIWVlZstls6tu3r0pKShx9pKSkaO3atcrIyNDOnTtVWlqqAQMGqKqqylOXBQAAvIjFMAzD00X8WHBwsF566SWNHj1a4eHhSklJ0XPPPSfpwixOaGio5s6dqzFjxqioqEgtW7bU8uXLNXz4cEnSqVOnFBERoQ0bNqhfv341vofdbpfdbnfsFxcXKyIiQkVFRQoMDHTr9bSZtt6t/f3Y8TkP1VnfAAB4u+LiYgUFBV3177fXrNmpqqpSRkaGysrKFBcXp+zsbOXn5yshIcHRxmq1qkePHtq1a5ckad++faqsrHRqEx4erpiYGEebmqSmpiooKMixRURE1N2FAQAAj/J42Dl06JCaNWsmq9Wqp59+WmvXrlX79u2Vn58vSQoNDXVqHxoa6jiXn58vf39/NW/e/LJtajJ9+nQVFRU5tpycHDdfFQAA8Ba+ni6gXbt2OnDggM6cOaMPP/xQSUlJ2rFjh+O8xWJxam8YRrVjl7paG6vVKqvVen2FAwCAesHjMzv+/v667bbbFBsbq9TUVN1555169dVXZbPZJKnaDE1BQYFjtsdms6miokKFhYWXbQMAABo2j4edSxmGIbvdrqioKNlsNmVmZjrOVVRUaMeOHerWrZskqXPnzvLz83Nqk5eXp8OHDzvaAACAhs2jH2PNmDFDiYmJioiIUElJiTIyMrR9+3Zt2rRJFotFKSkpmj17tqKjoxUdHa3Zs2erSZMmGjFihCQpKChIycnJmjx5slq0aKHg4GBNmTJFHTt2VJ8+fTx5aQAAwEt4NOx8++23evzxx5WXl6egoCB16tRJmzZtUt++fSVJU6dOVXl5ucaOHavCwkJ16dJFmzdvVkBAgKOPBQsWyNfXV8OGDVN5ebni4+O1dOlS+fj4eOqyAACAF/G679nxhNo+p+8KvmcHAIC6Ue++ZwcAAKAuEHYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpeTTspKam6qc//akCAgLUqlUrPfzwwzp69KhTm1GjRslisThtXbt2dWpjt9s1YcIEhYSEqGnTpho0aJByc3Nv5KUAAAAv5dGws2PHDo0bN0579uxRZmamzp07p4SEBJWVlTm1e/DBB5WXl+fYNmzY4HQ+JSVFa9euVUZGhnbu3KnS0lINGDBAVVVVN/JyAACAF/L15Jtv2rTJaT89PV2tWrXSvn379MADDziOW61W2Wy2GvsoKipSWlqali9frj59+kiSVqxYoYiICG3ZskX9+vWruwsAAABez6vW7BQVFUmSgoODnY5v375drVq1Utu2bfXUU0+poKDAcW7fvn2qrKxUQkKC41h4eLhiYmK0a9euGt/HbreruLjYaQMAAObkNWHHMAxNmjRJ3bt3V0xMjON4YmKi3n33XW3dulWvvPKKsrKy1Lt3b9ntdklSfn6+/P391bx5c6f+QkNDlZ+fX+N7paamKigoyLFFRETU3YUBAACP8ujHWD82fvx4HTx4UDt37nQ6Pnz4cMc/x8TEKDY2VpGRkVq/fr2GDh162f4Mw5DFYqnx3PTp0zVp0iTHfnFxMYEHAACT8oqZnQkTJujjjz/Wtm3b1Lp16yu2DQsLU2RkpI4dOyZJstlsqqioUGFhoVO7goIChYaG1tiH1WpVYGCg0wYAAMzJo2HHMAyNHz9ea9as0datWxUVFXXV15w+fVo5OTkKCwuTJHXu3Fl+fn7KzMx0tMnLy9Phw4fVrVu3OqsdAADUDx79GGvcuHFauXKlPvroIwUEBDjW2AQFBalx48YqLS3VzJkz9cgjjygsLEzHjx/XjBkzFBISoiFDhjjaJicna/LkyWrRooWCg4M1ZcoUdezY0fF0FgAAaLg8GnaWLFkiSerZs6fT8fT0dI0aNUo+Pj46dOiQli1bpjNnzigsLEy9evXS6tWrFRAQ4Gi/YMEC+fr6atiwYSovL1d8fLyWLl0qHx+fG3k5AADAC1kMwzA8XYSnFRcXKygoSEVFRW5fv9Nm2nq39vdjx+c8VGd9AwDg7Wr799srFigDAADUFcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNcIOAAAwNY/+NhbQ0NXVz4nwUyIA8G/M7AAAAFNzKexkZ2e7uw4AAIA64VLYue2229SrVy+tWLFCZ8+edXdNAAAAbuNS2Pn888919913a/LkybLZbBozZoz+93//1921AQAAXDeXwk5MTIzmz5+vb775Runp6crPz1f37t3VoUMHzZ8/X//617/cXScAAIBLrmuBsq+vr4YMGaL33ntPc+fO1T/+8Q9NmTJFrVu31siRI5WXl+euOgEAAFxyXWFn7969Gjt2rMLCwjR//nxNmTJF//jHP7R161Z98803Gjx4sLvqBAAAcIlL37Mzf/58paen6+jRo+rfv7+WLVum/v3766abLmSnqKgovfHGG7r99tvdWiwAAMC1cinsLFmyRKNHj9YTTzwhm81WY5tbbrlFaWlp11UcAADA9XIp7Bw7duyqbfz9/ZWUlORK9wAAAG7j0pqd9PR0vf/++9WOv//++3rnnXeuuygAAAB3cSnszJkzRyEhIdWOt2rVSrNnz77uogAAANzFpbBz4sQJRUVFVTseGRmpkydPXndRAAAA7uJS2GnVqpUOHjxY7fjnn3+uFi1aXHdRAAAA7uJS2PnZz36mZ599Vtu2bVNVVZWqqqq0detWTZw4UT/72c/cXSMAAIDLXHoa68UXX9SJEycUHx8vX98LXZw/f14jR45kzQ4AAPAqLoUdf39/rV69Wr/73e/0+eefq3HjxurYsaMiIyPdXR8AAMB1cSnsXNS2bVu1bdvWXbUAAAC4nUthp6qqSkuXLtUnn3yigoICnT9/3un81q1b3VIcAADA9XIp7EycOFFLly7VQw89pJiYGFksFnfXBQAA4BYuhZ2MjAy999576t+/v7vrAQAAcCuXHj339/fXbbfd5u5aAAAA3M6lsDN58mS9+uqrMgzD3fUAAAC4lUsfY+3cuVPbtm3Txo0b1aFDB/n5+TmdX7NmjVuKAwAAuF4uhZ2bb75ZQ4YMcXctAAAAbudS2ElPT3fLm6empmrNmjX68ssv1bhxY3Xr1k1z585Vu3btHG0Mw9CsWbP05ptvqrCwUF26dNEf//hHdejQwdHGbrdrypQpWrVqlcrLyxUfH6/FixerdevWbqkTAADUXy6t2ZGkc+fOacuWLXrjjTdUUlIiSTp16pRKS0tr3ceOHTs0btw47dmzR5mZmTp37pwSEhJUVlbmaDNv3jzNnz9fixYtUlZWlmw2m/r27et4T0lKSUnR2rVrlZGRoZ07d6q0tFQDBgxQVVWVq5cHAABMwqWZnRMnTujBBx/UyZMnZbfb1bdvXwUEBGjevHk6e/asXn/99Vr1s2nTJqf99PR0tWrVSvv27dMDDzwgwzC0cOFCPf/88xo6dKgk6Z133lFoaKhWrlypMWPGqKioSGlpaVq+fLn69OkjSVqxYoUiIiK0ZcsW9evXz5VLBAAAJuHSzM7EiRMVGxurwsJCNW7c2HF8yJAh+uSTT1wupqioSJIUHBwsScrOzlZ+fr4SEhIcbaxWq3r06KFdu3ZJkvbt26fKykqnNuHh4YqJiXG0uZTdbldxcbHTBgAAzMnlp7E+++wz+fv7Ox2PjIzUN99841IhhmFo0qRJ6t69u2JiYiRJ+fn5kqTQ0FCntqGhoTpx4oSjjb+/v5o3b16tzcXXXyo1NVWzZs1yqU4AAFC/uDSzc/78+RrXw+Tm5iogIMClQsaPH6+DBw9q1apV1c5d+nMUhmFc9ScqrtRm+vTpKioqcmw5OTku1QwAALyfS2Gnb9++WrhwoWPfYrGotLRUL7zwgks/ITFhwgR9/PHH2rZtm9MTVDabTZKqzdAUFBQ4ZntsNpsqKipUWFh42TaXslqtCgwMdNoAAIA5uRR2FixYoB07dqh9+/Y6e/asRowYoTZt2uibb77R3Llza92PYRgaP3681qxZo61btyoqKsrpfFRUlGw2mzIzMx3HKioqtGPHDnXr1k2S1LlzZ/n5+Tm1ycvL0+HDhx1tAABAw+XSmp3w8HAdOHBAq1at0t/+9jedP39eycnJ+sUvfuG0YPlqxo0bp5UrV+qjjz5SQECAYwYnKChIjRs3lsViUUpKimbPnq3o6GhFR0dr9uzZatKkiUaMGOFom5ycrMmTJ6tFixYKDg7WlClT1LFjR8fTWQAAoOFyKexIUuPGjTV69GiNHj3a5TdfsmSJJKlnz55Ox9PT0zVq1ChJ0tSpU1VeXq6xY8c6vlRw8+bNTmuDFixYIF9fXw0bNszxpYJLly6Vj4+Py7UBAABzsBgu/JrnsmXLrnh+5MiRLhfkCcXFxQoKClJRUZHb1++0mbberf392PE5D9VZ37gx6ur+4N4A0BDU9u+3SzM7EydOdNqvrKzUDz/8IH9/fzVp0qTehR0AAGBeLi1QLiwsdNpKS0t19OhRde/evcZHxwEAADzF5d/GulR0dLTmzJlTbdYHAADAk1xeoFwTHx8fnTp1yp1dAgDcjLViaGhcCjsff/yx075hGMrLy9OiRYt03333uaUwAAAAd3Ap7Dz88MNO+xaLRS1btlTv3r31yiuvuKMuAAAAt3Ap7Jw/f97ddQAAANQJty1QBgAA8EYuzexMmjSp1m3nz5/vylsAAAC4hUthZ//+/frb3/6mc+fOqV27dpKkr776Sj4+Prrnnnsc7SwWi3uqBAAAcJFLYWfgwIEKCAjQO++8o+bNm0u68EWDTzzxhO6//35NnjzZrUUCAAC4yqU1O6+88opSU1MdQUeSmjdvrhdffJGnsQAAgFdxKewUFxfr22+/rXa8oKBAJSUl110UAACAu7gUdoYMGaInnnhCH3zwgXJzc5Wbm6sPPvhAycnJGjp0qLtrBAAAcJlLa3Zef/11TZkyRY899pgqKysvdOTrq+TkZL300ktuLRAAAOB6uBR2mjRposWLF+ull17SP/7xDxmGodtuu01NmzZ1d30AAADX5bq+VDAvL095eXlq27atmjZtKsMw3FUXAACAW7gUdk6fPq34+Hi1bdtW/fv3V15eniTpySef5LFzAADgVVwKO7/61a/k5+enkydPqkmTJo7jw4cP16ZNm9xWHAAAwPVyac3O5s2b9Ze//EWtW7d2Oh4dHa0TJ064pTAAAAB3cGlmp6yszGlG56LvvvtOVqv1uosCAABwF5fCzgMPPKBly5Y59i0Wi86fP6+XXnpJvXr1cltxAAAA18ulj7Feeukl9ezZU3v37lVFRYWmTp2qI0eO6Pvvv9dnn33m7hoBAABc5tLMTvv27XXw4EHde++96tu3r8rKyjR06FDt379fP/nJT9xdIwAAgMuueWansrJSCQkJeuONNzRr1qy6qAkAAMBtrnlmx8/PT4cPH5bFYqmLegAAANzKpY+xRo4cqbS0NHfXAgAA4HYuLVCuqKjQW2+9pczMTMXGxlb7Taz58+e7pTgAAIDrdU1h55///KfatGmjw4cP65577pEkffXVV05t+HgLAAB4k2sKO9HR0crLy9O2bdskXfh5iD/84Q8KDQ2tk+IAAACu1zWt2bn0V803btyosrIytxYEAADgTi4tUL7o0vADAADgba4p7FgslmprclijAwAAvNk1rdkxDEOjRo1y/Njn2bNn9fTTT1d7GmvNmjXuqxAAAOA6XFPYSUpKctp/7LHH3FoMAACAu11T2ElPT6+rOgAAAOrEdS1Qvl6ffvqpBg4cqPDwcFksFq1bt87p/KhRoxzrhC5uXbt2dWpjt9s1YcIEhYSEqGnTpho0aJByc3Nv4FUAAABv5tGwU1ZWpjvvvFOLFi26bJsHH3xQeXl5jm3Dhg1O51NSUrR27VplZGRo586dKi0t1YABA1RVVVXX5QMAgHrApZ+LcJfExEQlJiZesY3VapXNZqvxXFFRkdLS0rR8+XL16dNHkrRixQpFRERoy5Yt6tevn9trBgAA9YtHZ3ZqY/v27WrVqpXatm2rp556SgUFBY5z+/btU2VlpRISEhzHwsPDFRMTo127dl22T7vdruLiYqcNAACYk1eHncTERL377rvaunWrXnnlFWVlZal3796y2+2SpPz8fPn7+6t58+ZOrwsNDVV+fv5l+01NTVVQUJBji4iIqNPrAAAAnuPRj7GuZvjw4Y5/jomJUWxsrCIjI7V+/XoNHTr0sq8zDOOKX3Y4ffp0TZo0ybFfXFxM4AEAwKS8embnUmFhYYqMjNSxY8ckSTabTRUVFSosLHRqV1BQcMUfJ7VarQoMDHTaAACAOdWrsHP69Gnl5OQoLCxMktS5c2f5+fkpMzPT0SYvL0+HDx9Wt27dPFUmAADwIh79GKu0tFRff/21Yz87O1sHDhxQcHCwgoODNXPmTD3yyCMKCwvT8ePHNWPGDIWEhGjIkCGSpKCgICUnJ2vy5Mlq0aKFgoODNWXKFHXs2NHxdBYAAGjYPBp29u7dq169ejn2L66jSUpK0pIlS3To0CEtW7ZMZ86cUVhYmHr16qXVq1crICDA8ZoFCxbI19dXw4YNU3l5ueLj47V06VL5+Pjc8OsBAADex6Nhp2fPnjIM47Ln//KXv1y1j0aNGum1117Ta6+95s7SAACASdSrNTsAAADXirADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMzaNh59NPP9XAgQMVHh4ui8WidevWOZ03DEMzZ85UeHi4GjdurJ49e+rIkSNObex2uyZMmKCQkBA1bdpUgwYNUm5u7g28CgAA4M08GnbKysp05513atGiRTWenzdvnubPn69FixYpKytLNptNffv2VUlJiaNNSkqK1q5dq4yMDO3cuVOlpaUaMGCAqqqqbtRlAAAAL+bryTdPTExUYmJijecMw9DChQv1/PPPa+jQoZKkd955R6GhoVq5cqXGjBmjoqIipaWlafny5erTp48kacWKFYqIiNCWLVvUr1+/G3YtAADAO3ntmp3s7Gzl5+crISHBccxqtapHjx7atWuXJGnfvn2qrKx0ahMeHq6YmBhHm5rY7XYVFxc7bQAAwJy8Nuzk5+dLkkJDQ52Oh4aGOs7l5+fL399fzZs3v2ybmqSmpiooKMixRUREuLl6AADgLbw27FxksVic9g3DqHbsUldrM336dBUVFTm2nJwct9QKAAC8j9eGHZvNJknVZmgKCgocsz02m00VFRUqLCy8bJuaWK1WBQYGOm0AAMCcvDbsREVFyWazKTMz03GsoqJCO3bsULdu3SRJnTt3lp+fn1ObvLw8HT582NEGAAA0bB59Gqu0tFRff/21Yz87O1sHDhxQcHCwbrnlFqWkpGj27NmKjo5WdHS0Zs+erSZNmmjEiBGSpKCgICUnJ2vy5Mlq0aKFgoODNWXKFHXs2NHxdBYAAGjYPBp29u7dq169ejn2J02aJElKSkrS0qVLNXXqVJWXl2vs2LEqLCxUly5dtHnzZgUEBDhes2DBAvn6+mrYsGEqLy9XfHy8li5dKh8fnxt+PQAAwPt4NOz07NlThmFc9rzFYtHMmTM1c+bMy7Zp1KiRXnvtNb322mt1UCEAAKjvvHbNDgAAgDsQdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKn5eroAwNu1mbbe0yUAAK4DMzsAAMDUCDsAAMDUCDsAAMDUCDsAAMDUCDsAAMDUeBoLgFeoj0+9HZ/zkKdLANyqrv499PS/K8zsAAAAU2Nmpx6ry/8n7OkUDu9VH2dgADRszOwAAABTI+wAAABT8+qwM3PmTFksFqfNZrM5zhuGoZkzZyo8PFyNGzdWz549deTIEQ9WDAAAvI1Xhx1J6tChg/Ly8hzboUOHHOfmzZun+fPna9GiRcrKypLNZlPfvn1VUlLiwYoBAIA38fqw4+vrK5vN5thatmwp6cKszsKFC/X8889r6NChiomJ0TvvvKMffvhBK1eu9HDVAADAW3j901jHjh1TeHi4rFarunTpotmzZ+vWW29Vdna28vPzlZCQ4GhrtVrVo0cP7dq1S2PGjLlsn3a7XXa73bFfXFxcp9eAf+MJshuDJ6YA4N+8Oux06dJFy5YtU9u2bfXtt9/qxRdfVLdu3XTkyBHl5+dLkkJDQ51eExoaqhMnTlyx39TUVM2aNavO6oZn8AceAFATr/4YKzExUY888og6duyoPn36aP36C3/M3nnnHUcbi8Xi9BrDMKodu9T06dNVVFTk2HJyctxfPAAA8ApeHXYu1bRpU3Xs2FHHjh1zPJV1cYbnooKCgmqzPZeyWq0KDAx02gAAgDnVq7Bjt9v197//XWFhYYqKipLNZlNmZqbjfEVFhXbs2KFu3bp5sEoAAOBNvHrNzpQpUzRw4EDdcsstKigo0Isvvqji4mIlJSXJYrEoJSVFs2fPVnR0tKKjozV79mw1adJEI0aM8HTpAADAS3h12MnNzdXPf/5zfffdd2rZsqW6du2qPXv2KDIyUpI0depUlZeXa+zYsSosLFSXLl20efNmBQQEeLjy+o/FvgAAs/DqsJORkXHF8xaLRTNnztTMmTNvTEEAAKDeqVdrdgAAAK4VYQcAAJgaYQcAAJgaYQcAAJiaVy9QBoCGiiciAfdhZgcAAJgaMzsA4CJmX4D6gZkdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgar6eLgAAYA5tpq2vs76Pz3mozvqG+TGzAwAATI2wAwAATI2wAwAATI2wAwAATI2wAwAATI2wAwAATI1HzwEADVZ9fFy+Lms2K8IOAMDr8Qce14OPsQAAgKkRdgAAgKkRdgAAgKmxZgcAgDrAOiPvYZqZncWLFysqKkqNGjVS586d9de//tXTJQEAAC9girCzevVqpaSk6Pnnn9f+/ft1//33KzExUSdPnvR0aQAAwMNMEXbmz5+v5ORkPfnkk7rjjju0cOFCRUREaMmSJZ4uDQAAeFi9X7NTUVGhffv2adq0aU7HExIStGvXrhpfY7fbZbfbHftFRUWSpOLiYrfXd97+g9v7BACgPqmLv68/7tcwjCu2q/dh57vvvlNVVZVCQ0OdjoeGhio/P7/G16SmpmrWrFnVjkdERNRJjQAANGRBC+u2/5KSEgUFBV32fL0POxdZLBanfcMwqh27aPr06Zo0aZJj//z58/r+++/VokWLy77GFcXFxYqIiFBOTo4CAwPd1q9ZMV61x1jVHmNVe4xV7TFWtVeXY2UYhkpKShQeHn7FdvU+7ISEhMjHx6faLE5BQUG12Z6LrFarrFar07Gbb765rkpUYGAg/zJcA8ar9hir2mOsao+xqj3GqvbqaqyuNKNzUb1foOzv76/OnTsrMzPT6XhmZqa6devmoaoAAIC3qPczO5I0adIkPf7444qNjVVcXJzefPNNnTx5Uk8//bSnSwMAAB5mirAzfPhwnT59Wr/97W+Vl5enmJgYbdiwQZGRkR6ty2q16oUXXqj2kRlqxnjVHmNVe4xV7TFWtcdY1Z43jJXFuNrzWgAAAPVYvV+zAwAAcCWEHQAAYGqEHQAAYGqEHQAAYGqEHQAAYGqEneu0ePFiRUVFqVGjRurcubP++te/XrH9jh071LlzZzVq1Ei33nqrXn/99RtUqeddy1ht375dFoul2vbll1/ewIo949NPP9XAgQMVHh4ui8WidevWXfU1DfW+utaxasj3VWpqqn76058qICBArVq10sMPP6yjR49e9XUN8d5yZawa6r21ZMkSderUyfHtyHFxcdq4ceMVX+OJe4qwcx1Wr16tlJQUPf/889q/f7/uv/9+JSYm6uTJkzW2z87OVv/+/XX//fdr//79mjFjhp599ll9+OGHN7jyG+9ax+qio0ePKi8vz7FFR0ffoIo9p6ysTHfeeacWLVpUq/YN+b661rG6qCHeVzt27NC4ceO0Z88eZWZm6ty5c0pISFBZWdllX9NQ7y1XxuqihnZvtW7dWnPmzNHevXu1d+9e9e7dW4MHD9aRI0dqbO+xe8qAy+69917j6aefdjp2++23G9OmTaux/dSpU43bb7/d6diYMWOMrl271lmN3uJax2rbtm2GJKOwsPAGVOe9JBlr1669YpuGfF/9WG3Givvq3woKCgxJxo4dOy7bhnvrgtqMFffWvzVv3tx46623ajznqXuKmR0XVVRUaN++fUpISHA6npCQoF27dtX4mt27d1dr369fP+3du1eVlZV1VqunuTJWF919990KCwtTfHy8tm3bVpdl1lsN9b66HtxXUlFRkSQpODj4sm24ty6ozVhd1JDvraqqKmVkZKisrExxcXE1tvHUPUXYcdF3332nqqqqar+sHhoaWu0X2C/Kz8+vsf25c+f03Xff1VmtnubKWIWFhenNN9/Uhx9+qDVr1qhdu3aKj4/Xp59+eiNKrlca6n3lCu6rCwzD0KRJk9S9e3fFxMRcth33Vu3HqiHfW4cOHVKzZs1ktVr19NNPa+3atWrfvn2NbT11T5nit7E8yWKxOO0bhlHt2NXa13TcjK5lrNq1a6d27do59uPi4pSTk6OXX35ZDzzwQJ3WWR815PvqWnBfXTB+/HgdPHhQO3fuvGrbhn5v1XasGvK91a5dOx04cEBnzpzRhx9+qKSkJO3YseOygccT9xQzOy4KCQmRj49PtZmJgoKCaqn1IpvNVmN7X19ftWjRos5q9TRXxqomXbt21bFjx9xdXr3XUO8rd2lo99WECRP08ccfa9u2bWrduvUV2zb0e+taxqomDeXe8vf312233abY2Filpqbqzjvv1KuvvlpjW0/dU4QdF/n7+6tz587KzMx0Op6Zmalu3brV+Jq4uLhq7Tdv3qzY2Fj5+fnVWa2e5spY1WT//v0KCwtzd3n1XkO9r9ylodxXhmFo/PjxWrNmjbZu3aqoqKirvqah3luujFVNGsq9dSnDMGS322s857F7qk6XP5tcRkaG4efnZ6SlpRlffPGFkZKSYjRt2tQ4fvy4YRiGMW3aNOPxxx93tP/nP/9pNGnSxPjVr35lfPHFF0ZaWprh5+dnfPDBB566hBvmWsdqwYIFxtq1a42vvvrKOHz4sDFt2jRDkvHhhx966hJumJKSEmP//v3G/v37DUnG/Pnzjf379xsnTpwwDIP76seudawa8n31zDPPGEFBQcb27duNvLw8x/bDDz842nBvXeDKWDXUe2v69OnGp59+amRnZxsHDx40ZsyYYdx0003G5s2bDcPwnnuKsHOd/vjHPxqRkZGGv7+/cc899zg9mpiUlGT06NHDqf327duNu+++2/D39zfatGljLFmy5AZX7DnXMlZz5841fvKTnxiNGjUymjdvbnTv3t1Yv369B6q+8S4+wnrplpSUZBgG99WPXetYNeT7qqZxkmSkp6c72nBvXeDKWDXUe2v06NGO/663bNnSiI+PdwQdw/Cee8piGP+/MggAAMCEWLMDAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABMjbADAABM7f8AgWhsU9o6RhUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################\n",
      "####################################################################\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns : \n",
    "    if df[i].dtype.name == 'object':\n",
    "        df[i].value_counts().plot(kind=\"pie\",title=f\"{i} plot\")\n",
    "        plt.show()\n",
    "        df[i].value_counts().plot(kind='bar',title=f\"{i} plot bars\")\n",
    "        \n",
    "    else :\n",
    "        df[i].plot(kind='kde' , title=f\"{i} distribution\")\n",
    "        plt.show()\n",
    "        df[i].plot(kind='hist', bins=20, title=f\"{i} hisogram\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"####################################################################\")\n",
    "    print(\"####################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100c1af",
   "metadata": {},
   "source": [
    "## Data cleaing and preparing for the Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8ed56",
   "metadata": {},
   "source": [
    "### initially I thought in Alcohol_Consump as one hot vector but I found that we can consider it as ordinal field according to \"severity\" but you should try make them one hot enconding and see the difference in the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe05fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(df):\n",
    "    copy_df = df.copy()\n",
    "    # converting the categorical data to numeric data . \n",
    "    # in H_cal_consump we don't need one hot vector it just one or zero\n",
    "    copy_df[\"H_Cal_Consump\"].replace([\"yes\",\"no\"],[1,0], inplace=True)\n",
    "    copy_df[\"Gender\"].replace(['Male',\"Female\"],[1,0], inplace=True)\n",
    "    copy_df[\"Alcohol_Consump\"].replace([\"no\",\"Sometimes\",\"Frequently\",\"Always\"],[0,1,2,3],inplace=True)\n",
    "    copy_df[\"Smoking\"].replace([\"yes\",\"no\"],[1,0], inplace=True)\n",
    "    copy_df=pd.concat([copy_df,pd.get_dummies(copy_df['Food_Between_Meals'],prefix=\"FBM_\")],axis=1)\n",
    "    copy_df.drop(['Food_Between_Meals'],axis=1,inplace=True)\n",
    "    copy_df['Fam_Hist'].replace(['yes','no'],[1,0],inplace=True)\n",
    "    copy_df['H_Cal_Burn'].replace(['yes','no'],[1,0],inplace=True)\n",
    "    copy_df = pd.concat([copy_df,pd.get_dummies(copy_df[\"Transport\"])],axis=1)\n",
    "    copy_df.drop(['Transport'],axis=1,inplace=True)\n",
    "    copy_df[\"Body_Level\"].replace([\"Body Level 1\",\"Body Level 2\",\"Body Level 3\",\"Body Level 4\"],\n",
    "                              [0,1,2,3],inplace=True)\n",
    "    \n",
    "    \n",
    "    return copy_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddcbfd",
   "metadata": {},
   "source": [
    "## dealing with imbalancies \n",
    " - oversampling \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a0dc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Body Level 4    680\n",
       "Body Level 3    406\n",
       "Body Level 2    201\n",
       "Body Level 1    190\n",
       "Name: Body_Level, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Body_Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3684cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original= cleaning_data(df).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3d1d2",
   "metadata": {},
   "source": [
    "## Applying all the models before applying any Technique that  can help in the imbalancies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf3b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train=df_original.drop([\"Body_Level\"],axis=1).to_numpy()\n",
    "Y_train=df_original[\"Body_Level\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6461df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29bd092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train_scaled\n",
    "                                                     , Y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6767cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LogisticRegression(random_state=42,max_iter=1000).fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77edc248",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred=reg_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30c89b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95       141\n",
      "           1       0.93      0.73      0.82       129\n",
      "           2       0.92      0.98      0.95       286\n",
      "           3       1.00      0.99      1.00       477\n",
      "\n",
      "    accuracy                           0.96      1033\n",
      "   macro avg       0.94      0.92      0.93      1033\n",
      "weighted avg       0.96      0.96      0.95      1033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2d29693",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred=reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7f6f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.89        49\n",
      "           1       0.86      0.71      0.78        72\n",
      "           2       0.90      0.92      0.91       120\n",
      "           3       0.99      0.99      0.99       203\n",
      "\n",
      "    accuracy                           0.92       444\n",
      "   macro avg       0.89      0.90      0.89       444\n",
      "weighted avg       0.92      0.92      0.92       444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9da4a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest_model = RandomForestClassifier(max_depth=40,random_state=42).fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73a8a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred=rand_forest_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9024a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       141\n",
      "           1       1.00      1.00      1.00       129\n",
      "           2       1.00      1.00      1.00       286\n",
      "           3       1.00      1.00      1.00       477\n",
      "\n",
      "    accuracy                           1.00      1033\n",
      "   macro avg       1.00      1.00      1.00      1033\n",
      "weighted avg       1.00      1.00      1.00      1033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7443863",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred=rand_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "041e22a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93        49\n",
      "           1       0.81      0.88      0.84        72\n",
      "           2       0.92      0.92      0.92       120\n",
      "           3       0.99      0.98      0.98       203\n",
      "\n",
      "    accuracy                           0.93       444\n",
      "   macro avg       0.92      0.92      0.92       444\n",
      "weighted avg       0.94      0.93      0.94       444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97fc1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(gamma='auto').fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c0e0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred=svm_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3c59fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       141\n",
      "           1       0.85      0.78      0.81       129\n",
      "           2       0.93      0.95      0.94       286\n",
      "           3       0.99      1.00      0.99       477\n",
      "\n",
      "    accuracy                           0.95      1033\n",
      "   macro avg       0.93      0.92      0.92      1033\n",
      "weighted avg       0.95      0.95      0.95      1033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86b48bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred=svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cd6acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        49\n",
      "           1       0.66      0.72      0.69        72\n",
      "           2       0.84      0.82      0.83       120\n",
      "           3       0.97      0.96      0.97       203\n",
      "\n",
      "    accuracy                           0.87       444\n",
      "   macro avg       0.84      0.84      0.84       444\n",
      "weighted avg       0.88      0.87      0.88       444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f068cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint('Best_Logistic_before_over_Sampling_.h5', monitor='val_accuracy', mode='max'\n",
    "                     , verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "model_NN=keras.Sequential([keras.Input(shape=(23)),\n",
    "                        layers.Dense(4,activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bd55607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN.compile(loss=SparseCategoricalFocalLoss(from_logits=False,gamma=2),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b41c5077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.9877 - accuracy: 0.3438  \n",
      "Epoch 1: val_accuracy improved from -inf to 0.57658, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 2s 14ms/step - loss: 0.8718 - accuracy: 0.4143 - val_loss: 0.5468 - val_accuracy: 0.5766\n",
      "Epoch 2/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.4695 - accuracy: 0.6341\n",
      "Epoch 2: val_accuracy improved from 0.57658 to 0.64640, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4515 - accuracy: 0.6302 - val_loss: 0.3990 - val_accuracy: 0.6464\n",
      "Epoch 3/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.3650 - accuracy: 0.6678\n",
      "Epoch 3: val_accuracy improved from 0.64640 to 0.70946, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.3517 - accuracy: 0.6805 - val_loss: 0.3437 - val_accuracy: 0.7095\n",
      "Epoch 4/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.3121 - accuracy: 0.6921\n",
      "Epoch 4: val_accuracy did not improve from 0.70946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.3072 - accuracy: 0.7018 - val_loss: 0.3145 - val_accuracy: 0.7072\n",
      "Epoch 5/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.2677 - accuracy: 0.7799\n",
      "Epoch 5: val_accuracy improved from 0.70946 to 0.77928, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2797 - accuracy: 0.7725 - val_loss: 0.2882 - val_accuracy: 0.7793\n",
      "Epoch 6/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.2615 - accuracy: 0.7928\n",
      "Epoch 6: val_accuracy did not improve from 0.77928\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2577 - accuracy: 0.7938 - val_loss: 0.2778 - val_accuracy: 0.7748\n",
      "Epoch 7/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.2430 - accuracy: 0.8034\n",
      "Epoch 7: val_accuracy improved from 0.77928 to 0.79730, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2407 - accuracy: 0.8054 - val_loss: 0.2591 - val_accuracy: 0.7973\n",
      "Epoch 8/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.2244 - accuracy: 0.8242\n",
      "Epoch 8: val_accuracy improved from 0.79730 to 0.82207, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2245 - accuracy: 0.8219 - val_loss: 0.2462 - val_accuracy: 0.8221\n",
      "Epoch 9/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.2196 - accuracy: 0.8269\n",
      "Epoch 9: val_accuracy did not improve from 0.82207\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2112 - accuracy: 0.8374 - val_loss: 0.2380 - val_accuracy: 0.8108\n",
      "Epoch 10/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.1985 - accuracy: 0.8472\n",
      "Epoch 10: val_accuracy improved from 0.82207 to 0.82883, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1986 - accuracy: 0.8509 - val_loss: 0.2244 - val_accuracy: 0.8288\n",
      "Epoch 11/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.1939 - accuracy: 0.8588\n",
      "Epoch 11: val_accuracy improved from 0.82883 to 0.83108, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1897 - accuracy: 0.8625 - val_loss: 0.2188 - val_accuracy: 0.8311\n",
      "Epoch 12/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.1845 - accuracy: 0.8625\n",
      "Epoch 12: val_accuracy improved from 0.83108 to 0.84685, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1836 - accuracy: 0.8722 - val_loss: 0.2078 - val_accuracy: 0.8468\n",
      "Epoch 13/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.1708 - accuracy: 0.8815\n",
      "Epoch 13: val_accuracy did not improve from 0.84685\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.8712 - val_loss: 0.1998 - val_accuracy: 0.8401\n",
      "Epoch 14/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.1631 - accuracy: 0.8886\n",
      "Epoch 14: val_accuracy improved from 0.84685 to 0.85135, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.8800 - val_loss: 0.1936 - val_accuracy: 0.8514\n",
      "Epoch 15/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.1714 - accuracy: 0.8642\n",
      "Epoch 15: val_accuracy did not improve from 0.85135\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1647 - accuracy: 0.8722 - val_loss: 0.1952 - val_accuracy: 0.8356\n",
      "Epoch 16/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.1562 - accuracy: 0.8854\n",
      "Epoch 16: val_accuracy improved from 0.85135 to 0.86036, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1561 - accuracy: 0.8819 - val_loss: 0.1803 - val_accuracy: 0.8604\n",
      "Epoch 17/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.1500 - accuracy: 0.8846\n",
      "Epoch 17: val_accuracy improved from 0.86036 to 0.86712, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.8819 - val_loss: 0.1763 - val_accuracy: 0.8671\n",
      "Epoch 18/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.1467 - accuracy: 0.8929\n",
      "Epoch 18: val_accuracy did not improve from 0.86712\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.8887 - val_loss: 0.1717 - val_accuracy: 0.8671\n",
      "Epoch 19/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.1412 - accuracy: 0.8963\n",
      "Epoch 19: val_accuracy improved from 0.86712 to 0.86937, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1408 - accuracy: 0.8925 - val_loss: 0.1675 - val_accuracy: 0.8694\n",
      "Epoch 20/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.1299 - accuracy: 0.9035\n",
      "Epoch 20: val_accuracy did not improve from 0.86937\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.8916 - val_loss: 0.1619 - val_accuracy: 0.8649\n",
      "Epoch 21/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.1322 - accuracy: 0.9051\n",
      "Epoch 21: val_accuracy improved from 0.86937 to 0.87838, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1327 - accuracy: 0.9042 - val_loss: 0.1569 - val_accuracy: 0.8784\n",
      "Epoch 22/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.1239 - accuracy: 0.9094\n",
      "Epoch 22: val_accuracy improved from 0.87838 to 0.88288, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1278 - accuracy: 0.9022 - val_loss: 0.1526 - val_accuracy: 0.8829\n",
      "Epoch 23/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.1227 - accuracy: 0.8988\n",
      "Epoch 23: val_accuracy did not improve from 0.88288\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1254 - accuracy: 0.8993 - val_loss: 0.1515 - val_accuracy: 0.8784\n",
      "Epoch 24/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.1232 - accuracy: 0.9050\n",
      "Epoch 24: val_accuracy improved from 0.88288 to 0.88739, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1200 - accuracy: 0.9080 - val_loss: 0.1449 - val_accuracy: 0.8874\n",
      "Epoch 25/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.1229 - accuracy: 0.9025\n",
      "Epoch 25: val_accuracy did not improve from 0.88739\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9051 - val_loss: 0.1459 - val_accuracy: 0.8761\n",
      "Epoch 26/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.1173 - accuracy: 0.9009\n",
      "Epoch 26: val_accuracy did not improve from 0.88739\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.9032 - val_loss: 0.1387 - val_accuracy: 0.8784\n",
      "Epoch 27/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/33 [===================>..........] - ETA: 0s - loss: 0.1080 - accuracy: 0.9117\n",
      "Epoch 27: val_accuracy did not improve from 0.88739\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.9090 - val_loss: 0.1349 - val_accuracy: 0.8829\n",
      "Epoch 28/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.1053 - accuracy: 0.9076\n",
      "Epoch 28: val_accuracy improved from 0.88739 to 0.89189, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1085 - accuracy: 0.9080 - val_loss: 0.1345 - val_accuracy: 0.8919\n",
      "Epoch 29/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.1072 - accuracy: 0.9141\n",
      "Epoch 29: val_accuracy did not improve from 0.89189\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1070 - accuracy: 0.9138 - val_loss: 0.1297 - val_accuracy: 0.8919\n",
      "Epoch 30/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.1085 - accuracy: 0.9149\n",
      "Epoch 30: val_accuracy improved from 0.89189 to 0.90766, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9158 - val_loss: 0.1253 - val_accuracy: 0.9077\n",
      "Epoch 31/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.1034 - accuracy: 0.9117\n",
      "Epoch 31: val_accuracy did not improve from 0.90766\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1009 - accuracy: 0.9206 - val_loss: 0.1249 - val_accuracy: 0.8964\n",
      "Epoch 32/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.1017 - accuracy: 0.9107\n",
      "Epoch 32: val_accuracy did not improve from 0.90766\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0997 - accuracy: 0.9119 - val_loss: 0.1233 - val_accuracy: 0.8806\n",
      "Epoch 33/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0898 - accuracy: 0.9261\n",
      "Epoch 33: val_accuracy did not improve from 0.90766\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0974 - accuracy: 0.9187 - val_loss: 0.1175 - val_accuracy: 0.9032\n",
      "Epoch 34/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0905 - accuracy: 0.9318\n",
      "Epoch 34: val_accuracy did not improve from 0.90766\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0961 - accuracy: 0.9284 - val_loss: 0.1147 - val_accuracy: 0.9054\n",
      "Epoch 35/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0905 - accuracy: 0.9348\n",
      "Epoch 35: val_accuracy did not improve from 0.90766\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9255 - val_loss: 0.1125 - val_accuracy: 0.9032\n",
      "Epoch 36/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0904 - accuracy: 0.9304\n",
      "Epoch 36: val_accuracy improved from 0.90766 to 0.91216, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0908 - accuracy: 0.9293 - val_loss: 0.1116 - val_accuracy: 0.9122\n",
      "Epoch 37/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0813 - accuracy: 0.9361\n",
      "Epoch 37: val_accuracy improved from 0.91216 to 0.91667, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0883 - accuracy: 0.9274 - val_loss: 0.1091 - val_accuracy: 0.9167\n",
      "Epoch 38/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0825 - accuracy: 0.9403\n",
      "Epoch 38: val_accuracy did not improve from 0.91667\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0874 - accuracy: 0.9284 - val_loss: 0.1132 - val_accuracy: 0.8941\n",
      "Epoch 39/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.0873 - accuracy: 0.9276\n",
      "Epoch 39: val_accuracy did not improve from 0.91667\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0862 - accuracy: 0.9284 - val_loss: 0.1081 - val_accuracy: 0.8964\n",
      "Epoch 40/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0844 - accuracy: 0.9321\n",
      "Epoch 40: val_accuracy did not improve from 0.91667\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9264 - val_loss: 0.1032 - val_accuracy: 0.9122\n",
      "Epoch 41/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0830 - accuracy: 0.9397\n",
      "Epoch 41: val_accuracy did not improve from 0.91667\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9419 - val_loss: 0.1034 - val_accuracy: 0.9167\n",
      "Epoch 42/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0824 - accuracy: 0.9300\n",
      "Epoch 42: val_accuracy improved from 0.91667 to 0.92342, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0802 - accuracy: 0.9322 - val_loss: 0.0999 - val_accuracy: 0.9234\n",
      "Epoch 43/200\n",
      "17/33 [==============>...............] - ETA: 0s - loss: 0.0843 - accuracy: 0.9357\n",
      "Epoch 43: val_accuracy did not improve from 0.92342\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 0.9409 - val_loss: 0.1051 - val_accuracy: 0.9009\n",
      "Epoch 44/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0787 - accuracy: 0.9362\n",
      "Epoch 44: val_accuracy did not improve from 0.92342\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.9400 - val_loss: 0.0972 - val_accuracy: 0.9144\n",
      "Epoch 45/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0752 - accuracy: 0.9413\n",
      "Epoch 45: val_accuracy did not improve from 0.92342\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0771 - accuracy: 0.9400 - val_loss: 0.0945 - val_accuracy: 0.9234\n",
      "Epoch 46/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0717 - accuracy: 0.9444\n",
      "Epoch 46: val_accuracy did not improve from 0.92342\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0748 - accuracy: 0.9400 - val_loss: 0.0970 - val_accuracy: 0.9122\n",
      "Epoch 47/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0730 - accuracy: 0.9375\n",
      "Epoch 47: val_accuracy did not improve from 0.92342\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9351 - val_loss: 0.0908 - val_accuracy: 0.9167\n",
      "Epoch 48/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0716 - accuracy: 0.9407\n",
      "Epoch 48: val_accuracy did not improve from 0.92342\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9400 - val_loss: 0.0943 - val_accuracy: 0.9122\n",
      "Epoch 49/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0715 - accuracy: 0.9365\n",
      "Epoch 49: val_accuracy improved from 0.92342 to 0.92793, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9380 - val_loss: 0.0880 - val_accuracy: 0.9279\n",
      "Epoch 50/200\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9458\n",
      "Epoch 50: val_accuracy did not improve from 0.92793\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9458 - val_loss: 0.0867 - val_accuracy: 0.9279\n",
      "Epoch 51/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0689 - accuracy: 0.9447\n",
      "Epoch 51: val_accuracy improved from 0.92793 to 0.93018, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0689 - accuracy: 0.9448 - val_loss: 0.0858 - val_accuracy: 0.9302\n",
      "Epoch 52/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0698 - accuracy: 0.9398\n",
      "Epoch 52: val_accuracy did not improve from 0.93018\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.9458 - val_loss: 0.0872 - val_accuracy: 0.9189\n",
      "Epoch 53/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0678 - accuracy: 0.9456\n",
      "Epoch 53: val_accuracy did not improve from 0.93018\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9477 - val_loss: 0.0833 - val_accuracy: 0.9257\n",
      "Epoch 54/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0630 - accuracy: 0.9560\n",
      "Epoch 54: val_accuracy did not improve from 0.93018\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.9487 - val_loss: 0.0828 - val_accuracy: 0.9279\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0657 - accuracy: 0.9491\n",
      "Epoch 55: val_accuracy did not improve from 0.93018\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.9487 - val_loss: 0.0812 - val_accuracy: 0.9257\n",
      "Epoch 56/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0649 - accuracy: 0.9398\n",
      "Epoch 56: val_accuracy improved from 0.93018 to 0.94144, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.9448 - val_loss: 0.0799 - val_accuracy: 0.9414\n",
      "Epoch 57/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9463\n",
      "Epoch 57: val_accuracy did not improve from 0.94144\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9468 - val_loss: 0.0798 - val_accuracy: 0.9414\n",
      "Epoch 58/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0620 - accuracy: 0.9461\n",
      "Epoch 58: val_accuracy did not improve from 0.94144\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9468 - val_loss: 0.0768 - val_accuracy: 0.9369\n",
      "Epoch 59/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9502\n",
      "Epoch 59: val_accuracy did not improve from 0.94144\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.9497 - val_loss: 0.0767 - val_accuracy: 0.9414\n",
      "Epoch 60/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0625 - accuracy: 0.9476\n",
      "Epoch 60: val_accuracy did not improve from 0.94144\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9487 - val_loss: 0.0795 - val_accuracy: 0.9257\n",
      "Epoch 61/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0601 - accuracy: 0.9573\n",
      "Epoch 61: val_accuracy did not improve from 0.94144\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9574 - val_loss: 0.0752 - val_accuracy: 0.9392\n",
      "Epoch 62/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0586 - accuracy: 0.9461\n",
      "Epoch 62: val_accuracy did not improve from 0.94144\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9497 - val_loss: 0.0759 - val_accuracy: 0.9302\n",
      "Epoch 63/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0570 - accuracy: 0.9587\n",
      "Epoch 63: val_accuracy improved from 0.94144 to 0.94369, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0579 - accuracy: 0.9555 - val_loss: 0.0727 - val_accuracy: 0.9437\n",
      "Epoch 64/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0568 - accuracy: 0.9609\n",
      "Epoch 64: val_accuracy did not improve from 0.94369\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0570 - accuracy: 0.9526 - val_loss: 0.0730 - val_accuracy: 0.9324\n",
      "Epoch 65/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0547 - accuracy: 0.9591\n",
      "Epoch 65: val_accuracy improved from 0.94369 to 0.95495, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0556 - accuracy: 0.9584 - val_loss: 0.0695 - val_accuracy: 0.9550\n",
      "Epoch 66/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0571 - accuracy: 0.9560\n",
      "Epoch 66: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9584 - val_loss: 0.0703 - val_accuracy: 0.9369\n",
      "Epoch 67/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0551 - accuracy: 0.9587\n",
      "Epoch 67: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9584 - val_loss: 0.0700 - val_accuracy: 0.9347\n",
      "Epoch 68/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0550 - accuracy: 0.9569\n",
      "Epoch 68: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9564 - val_loss: 0.0677 - val_accuracy: 0.9482\n",
      "Epoch 69/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0526 - accuracy: 0.9556\n",
      "Epoch 69: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0528 - accuracy: 0.9555 - val_loss: 0.0668 - val_accuracy: 0.9527\n",
      "Epoch 70/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0504 - accuracy: 0.9576\n",
      "Epoch 70: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9593 - val_loss: 0.0662 - val_accuracy: 0.9347\n",
      "Epoch 71/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0530 - accuracy: 0.9526\n",
      "Epoch 71: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.9526 - val_loss: 0.0684 - val_accuracy: 0.9392\n",
      "Epoch 72/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0519 - accuracy: 0.9595\n",
      "Epoch 72: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0520 - accuracy: 0.9564 - val_loss: 0.0653 - val_accuracy: 0.9482\n",
      "Epoch 73/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0501 - accuracy: 0.9615\n",
      "Epoch 73: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9622 - val_loss: 0.0643 - val_accuracy: 0.9505\n",
      "Epoch 74/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0506 - accuracy: 0.9567\n",
      "Epoch 74: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0496 - accuracy: 0.9613 - val_loss: 0.0630 - val_accuracy: 0.9505\n",
      "Epoch 75/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0489 - accuracy: 0.9561\n",
      "Epoch 75: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9555 - val_loss: 0.0628 - val_accuracy: 0.9527\n",
      "Epoch 76/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0490 - accuracy: 0.9563\n",
      "Epoch 76: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9555 - val_loss: 0.0619 - val_accuracy: 0.9550\n",
      "Epoch 77/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0491 - accuracy: 0.9580\n",
      "Epoch 77: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0489 - accuracy: 0.9564 - val_loss: 0.0612 - val_accuracy: 0.9550\n",
      "Epoch 78/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0477 - accuracy: 0.9625\n",
      "Epoch 78: val_accuracy did not improve from 0.95495\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.9603 - val_loss: 0.0618 - val_accuracy: 0.9505\n",
      "Epoch 79/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0474 - accuracy: 0.9700\n",
      "Epoch 79: val_accuracy improved from 0.95495 to 0.95946, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9671 - val_loss: 0.0596 - val_accuracy: 0.9595\n",
      "Epoch 80/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0466 - accuracy: 0.9597\n",
      "Epoch 80: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9603 - val_loss: 0.0603 - val_accuracy: 0.9437\n",
      "Epoch 81/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0460 - accuracy: 0.9625\n",
      "Epoch 81: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.9632 - val_loss: 0.0579 - val_accuracy: 0.9572\n",
      "Epoch 82/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9688\n",
      "Epoch 82: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9642 - val_loss: 0.0588 - val_accuracy: 0.9527\n",
      "Epoch 83/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0417 - accuracy: 0.9688\n",
      "Epoch 83: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9632 - val_loss: 0.0585 - val_accuracy: 0.9572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9613\n",
      "Epoch 84: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.9613 - val_loss: 0.0574 - val_accuracy: 0.9505\n",
      "Epoch 85/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0404 - accuracy: 0.9753\n",
      "Epoch 85: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.9642 - val_loss: 0.0564 - val_accuracy: 0.9550\n",
      "Epoch 86/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0419 - accuracy: 0.9675\n",
      "Epoch 86: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.9681 - val_loss: 0.0573 - val_accuracy: 0.9482\n",
      "Epoch 87/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0433 - accuracy: 0.9655\n",
      "Epoch 87: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0427 - accuracy: 0.9661 - val_loss: 0.0564 - val_accuracy: 0.9505\n",
      "Epoch 88/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0431 - accuracy: 0.9615\n",
      "Epoch 88: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9622 - val_loss: 0.0549 - val_accuracy: 0.9550\n",
      "Epoch 89/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0424 - accuracy: 0.9627\n",
      "Epoch 89: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0424 - accuracy: 0.9632 - val_loss: 0.0530 - val_accuracy: 0.9595\n",
      "Epoch 90/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0415 - accuracy: 0.9646\n",
      "Epoch 90: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9652 - val_loss: 0.0565 - val_accuracy: 0.9459\n",
      "Epoch 91/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0407 - accuracy: 0.9625\n",
      "Epoch 91: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0412 - accuracy: 0.9622 - val_loss: 0.0521 - val_accuracy: 0.9572\n",
      "Epoch 92/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0409 - accuracy: 0.9699\n",
      "Epoch 92: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.9690 - val_loss: 0.0528 - val_accuracy: 0.9572\n",
      "Epoch 93/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0391 - accuracy: 0.9675\n",
      "Epoch 93: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0402 - accuracy: 0.9652 - val_loss: 0.0515 - val_accuracy: 0.9572\n",
      "Epoch 94/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0395 - accuracy: 0.9725\n",
      "Epoch 94: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9700 - val_loss: 0.0533 - val_accuracy: 0.9482\n",
      "Epoch 95/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0384 - accuracy: 0.9656\n",
      "Epoch 95: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0390 - accuracy: 0.9642 - val_loss: 0.0513 - val_accuracy: 0.9595\n",
      "Epoch 96/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0387 - accuracy: 0.9707\n",
      "Epoch 96: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9710 - val_loss: 0.0509 - val_accuracy: 0.9595\n",
      "Epoch 97/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0388 - accuracy: 0.9699\n",
      "Epoch 97: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 0.9681 - val_loss: 0.0531 - val_accuracy: 0.9505\n",
      "Epoch 98/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0382 - accuracy: 0.9688\n",
      "Epoch 98: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9671 - val_loss: 0.0508 - val_accuracy: 0.9527\n",
      "Epoch 99/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 99: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9690 - val_loss: 0.0496 - val_accuracy: 0.9572\n",
      "Epoch 100/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 100: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9690 - val_loss: 0.0509 - val_accuracy: 0.9505\n",
      "Epoch 101/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 101: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.9690 - val_loss: 0.0524 - val_accuracy: 0.9505\n",
      "Epoch 102/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0360 - accuracy: 0.9676\n",
      "Epoch 102: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9632 - val_loss: 0.0488 - val_accuracy: 0.9505\n",
      "Epoch 103/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0368 - accuracy: 0.9688\n",
      "Epoch 103: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9671 - val_loss: 0.0491 - val_accuracy: 0.9595\n",
      "Epoch 104/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0352 - accuracy: 0.9675\n",
      "Epoch 104: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.9681 - val_loss: 0.0483 - val_accuracy: 0.9527\n",
      "Epoch 105/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0361 - accuracy: 0.9712\n",
      "Epoch 105: val_accuracy did not improve from 0.95946\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9710 - val_loss: 0.0505 - val_accuracy: 0.9482\n",
      "Epoch 106/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0349 - accuracy: 0.9654\n",
      "Epoch 106: val_accuracy improved from 0.95946 to 0.96622, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.9671 - val_loss: 0.0471 - val_accuracy: 0.9662\n",
      "Epoch 107/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0346 - accuracy: 0.9763\n",
      "Epoch 107: val_accuracy did not improve from 0.96622\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0346 - accuracy: 0.9739 - val_loss: 0.0498 - val_accuracy: 0.9482\n",
      "Epoch 108/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0352 - accuracy: 0.9643\n",
      "Epoch 108: val_accuracy did not improve from 0.96622\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0348 - accuracy: 0.9642 - val_loss: 0.0496 - val_accuracy: 0.9527\n",
      "Epoch 109/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0350 - accuracy: 0.9722\n",
      "Epoch 109: val_accuracy did not improve from 0.96622\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.9739 - val_loss: 0.0481 - val_accuracy: 0.9550\n",
      "Epoch 110/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0342 - accuracy: 0.9788\n",
      "Epoch 110: val_accuracy did not improve from 0.96622\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0340 - accuracy: 0.9777 - val_loss: 0.0469 - val_accuracy: 0.9595\n",
      "Epoch 111/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0335 - accuracy: 0.9734\n",
      "Epoch 111: val_accuracy did not improve from 0.96622\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9671 - val_loss: 0.0463 - val_accuracy: 0.9550\n",
      "Epoch 112/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0319 - accuracy: 0.9734\n",
      "Epoch 112: val_accuracy did not improve from 0.96622\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.9710 - val_loss: 0.0454 - val_accuracy: 0.9595\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0318 - accuracy: 0.9712\n",
      "Epoch 113: val_accuracy improved from 0.96622 to 0.96847, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9681 - val_loss: 0.0431 - val_accuracy: 0.9685\n",
      "Epoch 114/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0334 - accuracy: 0.9688\n",
      "Epoch 114: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 0.9700 - val_loss: 0.0447 - val_accuracy: 0.9595\n",
      "Epoch 115/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9688\n",
      "Epoch 115: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9681 - val_loss: 0.0438 - val_accuracy: 0.9595\n",
      "Epoch 116/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0429 - accuracy: 0.9688\n",
      "Epoch 116: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9758 - val_loss: 0.0454 - val_accuracy: 0.9640\n",
      "Epoch 117/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0303 - accuracy: 0.9688\n",
      "Epoch 117: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.9700 - val_loss: 0.0449 - val_accuracy: 0.9595\n",
      "Epoch 118/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 118: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.9739 - val_loss: 0.0436 - val_accuracy: 0.9595\n",
      "Epoch 119/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0311 - accuracy: 0.9724\n",
      "Epoch 119: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9700 - val_loss: 0.0433 - val_accuracy: 0.9595\n",
      "Epoch 120/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0313 - accuracy: 0.9825\n",
      "Epoch 120: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9797 - val_loss: 0.0454 - val_accuracy: 0.9595\n",
      "Epoch 121/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0334 - accuracy: 0.9729\n",
      "Epoch 121: val_accuracy improved from 0.96847 to 0.97072, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 0.9739 - val_loss: 0.0425 - val_accuracy: 0.9707\n",
      "Epoch 122/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0299 - accuracy: 0.9760\n",
      "Epoch 122: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.9739 - val_loss: 0.0422 - val_accuracy: 0.9662\n",
      "Epoch 123/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0315 - accuracy: 0.9708\n",
      "Epoch 123: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9719 - val_loss: 0.0411 - val_accuracy: 0.9640\n",
      "Epoch 124/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0306 - accuracy: 0.9748\n",
      "Epoch 124: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.9719 - val_loss: 0.0414 - val_accuracy: 0.9617\n",
      "Epoch 125/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0295 - accuracy: 0.9762\n",
      "Epoch 125: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.9739 - val_loss: 0.0418 - val_accuracy: 0.9595\n",
      "Epoch 126/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0306 - accuracy: 0.9777\n",
      "Epoch 126: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.9758 - val_loss: 0.0422 - val_accuracy: 0.9572\n",
      "Epoch 127/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0295 - accuracy: 0.9699\n",
      "Epoch 127: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.9700 - val_loss: 0.0411 - val_accuracy: 0.9640\n",
      "Epoch 128/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0281 - accuracy: 0.9810\n",
      "Epoch 128: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.9758 - val_loss: 0.0422 - val_accuracy: 0.9572\n",
      "Epoch 129/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0319 - accuracy: 0.9732\n",
      "Epoch 129: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0302 - accuracy: 0.9768 - val_loss: 0.0424 - val_accuracy: 0.9572\n",
      "Epoch 130/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0287 - accuracy: 0.9743\n",
      "Epoch 130: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.9748 - val_loss: 0.0408 - val_accuracy: 0.9662\n",
      "Epoch 131/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0280 - accuracy: 0.9754\n",
      "Epoch 131: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.9758 - val_loss: 0.0420 - val_accuracy: 0.9595\n",
      "Epoch 132/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0290 - accuracy: 0.9766\n",
      "Epoch 132: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.9739 - val_loss: 0.0426 - val_accuracy: 0.9617\n",
      "Epoch 133/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 133: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.9768 - val_loss: 0.0414 - val_accuracy: 0.9572\n",
      "Epoch 134/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0315 - accuracy: 0.9688\n",
      "Epoch 134: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.9729 - val_loss: 0.0394 - val_accuracy: 0.9707\n",
      "Epoch 135/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 135: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.9768 - val_loss: 0.0423 - val_accuracy: 0.9482\n",
      "Epoch 136/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0266 - accuracy: 0.9819\n",
      "Epoch 136: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.9777 - val_loss: 0.0410 - val_accuracy: 0.9595\n",
      "Epoch 137/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0280 - accuracy: 0.9745\n",
      "Epoch 137: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.9710 - val_loss: 0.0414 - val_accuracy: 0.9550\n",
      "Epoch 138/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0274 - accuracy: 0.9762\n",
      "Epoch 138: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.9768 - val_loss: 0.0389 - val_accuracy: 0.9662\n",
      "Epoch 139/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0269 - accuracy: 0.9777\n",
      "Epoch 139: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 0.9758 - val_loss: 0.0406 - val_accuracy: 0.9550\n",
      "Epoch 140/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0278 - accuracy: 0.9778\n",
      "Epoch 140: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.9777 - val_loss: 0.0388 - val_accuracy: 0.9617\n",
      "Epoch 141/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0264 - accuracy: 0.9720\n",
      "Epoch 141: val_accuracy did not improve from 0.97072\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.9739 - val_loss: 0.0382 - val_accuracy: 0.9707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0244 - accuracy: 0.9828\n",
      "Epoch 142: val_accuracy improved from 0.97072 to 0.97297, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.9797 - val_loss: 0.0375 - val_accuracy: 0.9730\n",
      "Epoch 143/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0269 - accuracy: 0.9777\n",
      "Epoch 143: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.9768 - val_loss: 0.0379 - val_accuracy: 0.9662\n",
      "Epoch 144/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0246 - accuracy: 0.9777\n",
      "Epoch 144: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.9748 - val_loss: 0.0387 - val_accuracy: 0.9640\n",
      "Epoch 145/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0247 - accuracy: 0.9817\n",
      "Epoch 145: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9797 - val_loss: 0.0382 - val_accuracy: 0.9662\n",
      "Epoch 146/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0259 - accuracy: 0.9763\n",
      "Epoch 146: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.9758 - val_loss: 0.0392 - val_accuracy: 0.9640\n",
      "Epoch 147/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0254 - accuracy: 0.9754\n",
      "Epoch 147: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.9777 - val_loss: 0.0372 - val_accuracy: 0.9685\n",
      "Epoch 148/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0250 - accuracy: 0.9781\n",
      "Epoch 148: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.9797 - val_loss: 0.0376 - val_accuracy: 0.9640\n",
      "Epoch 149/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0245 - accuracy: 0.9798\n",
      "Epoch 149: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9787 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 150/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0344 - accuracy: 0.9375\n",
      "Epoch 150: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.9729 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 151/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 151: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.9787 - val_loss: 0.0359 - val_accuracy: 0.9707\n",
      "Epoch 152/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 152: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.9768 - val_loss: 0.0376 - val_accuracy: 0.9640\n",
      "Epoch 153/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0234 - accuracy: 0.9808\n",
      "Epoch 153: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.9748 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 154/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0246 - accuracy: 0.9787\n",
      "Epoch 154: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.9768 - val_loss: 0.0439 - val_accuracy: 0.9505\n",
      "Epoch 155/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0261 - accuracy: 0.9743\n",
      "Epoch 155: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0251 - accuracy: 0.9768 - val_loss: 0.0341 - val_accuracy: 0.9730\n",
      "Epoch 156/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0231 - accuracy: 0.9798\n",
      "Epoch 156: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.9777 - val_loss: 0.0353 - val_accuracy: 0.9707\n",
      "Epoch 157/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0237 - accuracy: 0.9798\n",
      "Epoch 157: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9806 - val_loss: 0.0377 - val_accuracy: 0.9550\n",
      "Epoch 158/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0215 - accuracy: 0.9833\n",
      "Epoch 158: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.9806 - val_loss: 0.0366 - val_accuracy: 0.9685\n",
      "Epoch 159/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0238 - accuracy: 0.9812\n",
      "Epoch 159: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.9826 - val_loss: 0.0376 - val_accuracy: 0.9572\n",
      "Epoch 160/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0237 - accuracy: 0.9775\n",
      "Epoch 160: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.9797 - val_loss: 0.0358 - val_accuracy: 0.9640\n",
      "Epoch 161/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0242 - accuracy: 0.9763\n",
      "Epoch 161: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.9787 - val_loss: 0.0366 - val_accuracy: 0.9707\n",
      "Epoch 162/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0219 - accuracy: 0.9788\n",
      "Epoch 162: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.9748 - val_loss: 0.0352 - val_accuracy: 0.9685\n",
      "Epoch 163/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0220 - accuracy: 0.9833\n",
      "Epoch 163: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.9806 - val_loss: 0.0354 - val_accuracy: 0.9685\n",
      "Epoch 164/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0222 - accuracy: 0.9828\n",
      "Epoch 164: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.9806 - val_loss: 0.0370 - val_accuracy: 0.9685\n",
      "Epoch 165/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0226 - accuracy: 0.9810\n",
      "Epoch 165: val_accuracy improved from 0.97297 to 0.97523, saving model to Best_Logistic_before_over_Sampling_.h5\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 0.9826 - val_loss: 0.0343 - val_accuracy: 0.9752\n",
      "Epoch 166/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0224 - accuracy: 0.9792\n",
      "Epoch 166: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9777 - val_loss: 0.0348 - val_accuracy: 0.9685\n",
      "Epoch 167/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9814\n",
      "Epoch 167: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.9816 - val_loss: 0.0353 - val_accuracy: 0.9685\n",
      "Epoch 168/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 168: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.9777 - val_loss: 0.0364 - val_accuracy: 0.9640\n",
      "Epoch 169/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 169: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.9826 - val_loss: 0.0341 - val_accuracy: 0.9707\n",
      "Epoch 170/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0219 - accuracy: 0.9784\n",
      "Epoch 170: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9787 - val_loss: 0.0346 - val_accuracy: 0.9707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0225 - accuracy: 0.9775\n",
      "Epoch 171: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9806 - val_loss: 0.0391 - val_accuracy: 0.9595\n",
      "Epoch 172/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0191 - accuracy: 0.9875\n",
      "Epoch 172: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 0.9835 - val_loss: 0.0350 - val_accuracy: 0.9707\n",
      "Epoch 173/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0210 - accuracy: 0.9784\n",
      "Epoch 173: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9787 - val_loss: 0.0331 - val_accuracy: 0.9707\n",
      "Epoch 174/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0215 - accuracy: 0.9781\n",
      "Epoch 174: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.9787 - val_loss: 0.0345 - val_accuracy: 0.9640\n",
      "Epoch 175/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0207 - accuracy: 0.9781\n",
      "Epoch 175: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.9768 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 176/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0221 - accuracy: 0.9829\n",
      "Epoch 176: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.9835 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 177/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0220 - accuracy: 0.9772\n",
      "Epoch 177: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9787 - val_loss: 0.0338 - val_accuracy: 0.9730\n",
      "Epoch 178/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0220 - accuracy: 0.9750\n",
      "Epoch 178: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.9787 - val_loss: 0.0349 - val_accuracy: 0.9685\n",
      "Epoch 179/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0205 - accuracy: 0.9832\n",
      "Epoch 179: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.9826 - val_loss: 0.0346 - val_accuracy: 0.9707\n",
      "Epoch 180/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0199 - accuracy: 0.9838\n",
      "Epoch 180: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.9826 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 181/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0206 - accuracy: 0.9795\n",
      "Epoch 181: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9787 - val_loss: 0.0330 - val_accuracy: 0.9685\n",
      "Epoch 182/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0195 - accuracy: 0.9792\n",
      "Epoch 182: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9787 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 183/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0190 - accuracy: 0.9850\n",
      "Epoch 183: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.9806 - val_loss: 0.0354 - val_accuracy: 0.9595\n",
      "Epoch 184/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0206 - accuracy: 0.9826\n",
      "Epoch 184: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9835 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 185/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0212 - accuracy: 0.9750\n",
      "Epoch 185: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9768 - val_loss: 0.0341 - val_accuracy: 0.9617\n",
      "Epoch 186/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0203 - accuracy: 0.9798\n",
      "Epoch 186: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.9797 - val_loss: 0.0352 - val_accuracy: 0.9707\n",
      "Epoch 187/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0201 - accuracy: 0.9838\n",
      "Epoch 187: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.9835 - val_loss: 0.0359 - val_accuracy: 0.9572\n",
      "Epoch 188/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0191 - accuracy: 0.9798\n",
      "Epoch 188: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.9777 - val_loss: 0.0332 - val_accuracy: 0.9707\n",
      "Epoch 189/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0196 - accuracy: 0.9829\n",
      "Epoch 189: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9816 - val_loss: 0.0326 - val_accuracy: 0.9662\n",
      "Epoch 190/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0198 - accuracy: 0.9826\n",
      "Epoch 190: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.9787 - val_loss: 0.0350 - val_accuracy: 0.9595\n",
      "Epoch 191/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0182 - accuracy: 0.9820\n",
      "Epoch 191: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9777 - val_loss: 0.0377 - val_accuracy: 0.9550\n",
      "Epoch 192/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0184 - accuracy: 0.9837\n",
      "Epoch 192: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 0.9826 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 193/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0180 - accuracy: 0.9833\n",
      "Epoch 193: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.9806 - val_loss: 0.0333 - val_accuracy: 0.9685\n",
      "Epoch 194/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0195 - accuracy: 0.9795\n",
      "Epoch 194: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.9816 - val_loss: 0.0351 - val_accuracy: 0.9685\n",
      "Epoch 195/200\n",
      "30/33 [==========================>...] - ETA: 0s - loss: 0.0188 - accuracy: 0.9823\n",
      "Epoch 195: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.9835 - val_loss: 0.0331 - val_accuracy: 0.9730\n",
      "Epoch 196/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0188 - accuracy: 0.9815\n",
      "Epoch 196: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 0.9826 - val_loss: 0.0328 - val_accuracy: 0.9730\n",
      "Epoch 197/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0174 - accuracy: 0.9844\n",
      "Epoch 197: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.9855 - val_loss: 0.0372 - val_accuracy: 0.9572\n",
      "Epoch 198/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0183 - accuracy: 0.9860\n",
      "Epoch 198: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9864 - val_loss: 0.0370 - val_accuracy: 0.9527\n",
      "Epoch 199/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0182 - accuracy: 0.9795\n",
      "Epoch 199: val_accuracy did not improve from 0.97523\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9816 - val_loss: 0.0345 - val_accuracy: 0.9707\n",
      "Epoch 200/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 200: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.9826 - val_loss: 0.0337 - val_accuracy: 0.9617\n"
     ]
    }
   ],
   "source": [
    "history = model_NN.fit(X_train,Y_train,validation_data=[X_test,Y_test] \n",
    "             ,batch_size=32, epochs=200 , verbose =1, callbacks=[mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32b22583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       141\n",
      "           1       0.97      0.91      0.94       129\n",
      "           2       0.98      0.99      0.99       286\n",
      "           3       1.00      1.00      1.00       477\n",
      "\n",
      "    accuracy                           0.98      1033\n",
      "   macro avg       0.98      0.97      0.97      1033\n",
      "weighted avg       0.98      0.98      0.98      1033\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'          precision    recall  f1-score   support\\n\\n           0       0.95      0.99      0.97       141\\n           1       0.96      0.87      0.91       129\\n           2       0.96      0.98      0.97       286\\n           3       1.00      0.99      1.00       477\\n\\n    accuracy                           0.97      1033\\n   macro avg       0.97      0.96      0.96      1033\\nweighted avg       0.97      0.97      0.97      1033\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_pred= model_NN.predict(X_train)\n",
    "Y_train_pred = np.argmax(Y_train_pred,axis=1)\n",
    "print(classification_report(Y_train, Y_train_pred))\n",
    "\n",
    "'''          precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.99      0.97       141\n",
    "           1       0.96      0.87      0.91       129\n",
    "           2       0.96      0.98      0.97       286\n",
    "           3       1.00      0.99      1.00       477\n",
    "\n",
    "    accuracy                           0.97      1033\n",
    "   macro avg       0.97      0.96      0.96      1033\n",
    "weighted avg       0.97      0.97      0.97      1033\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb24d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        49\n",
      "           1       0.94      0.86      0.90        72\n",
      "           2       0.95      0.96      0.95       120\n",
      "           3       1.00      0.99      0.99       203\n",
      "\n",
      "    accuracy                           0.96       444\n",
      "   macro avg       0.94      0.95      0.95       444\n",
      "weighted avg       0.96      0.96      0.96       444\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n      precision    recall  f1-score   support\\n\\n           0       0.89      1.00      0.94        49\\n           1       0.92      0.83      0.88        72\\n           2       0.93      0.95      0.94       120\\n           3       1.00      0.99      0.99       203\\n\\n    accuracy                           0.95       444\\n   macro avg       0.94      0.94      0.94       444\\nweighted avg       0.96      0.95      0.95       444\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred= model_NN.predict(X_test)\n",
    "Y_test_pred = np.argmax(Y_test_pred,axis=1)\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "\n",
    "'''\n",
    "      precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      1.00      0.94        49\n",
    "           1       0.92      0.83      0.88        72\n",
    "           2       0.93      0.95      0.94       120\n",
    "           3       1.00      0.99      0.99       203\n",
    "\n",
    "    accuracy                           0.95       444\n",
    "   macro avg       0.94      0.94      0.94       444\n",
    "weighted avg       0.96      0.95      0.95       444\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfdeca5",
   "metadata": {},
   "source": [
    "**I have saved the report, you should see that it is more accurate that the Random Forest which is the **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a99c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy=history.history[\"val_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "119cbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1,201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f266ee2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66222a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(epochs , accuracy , color='#ff0000' , label='training_acc')\n",
    "plt.plot(epochs , val_accuracy , color='#0000FF',label='val_acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('trainig accuracy versus val_acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuarcy ')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a1e83037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1031/1033 [============================>.] - ETA: 0s - loss: 0.4228 - accuracy: 0.6984\n",
      "Epoch 1: val_accuracy improved from -inf to 0.79505, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 3s 2ms/step - loss: 0.4223 - accuracy: 0.6989 - val_loss: 0.2678 - val_accuracy: 0.7950\n",
      "Epoch 2/200\n",
      "1013/1033 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.8342\n",
      "Epoch 2: val_accuracy improved from 0.79505 to 0.86937, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.2124 - accuracy: 0.8316 - val_loss: 0.1610 - val_accuracy: 0.8694\n",
      "Epoch 3/200\n",
      "1020/1033 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.8686\n",
      "Epoch 3: val_accuracy did not improve from 0.86937\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.1512 - accuracy: 0.8693 - val_loss: 0.1831 - val_accuracy: 0.8356\n",
      "Epoch 4/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.1414 - accuracy: 0.8737\n",
      "Epoch 4: val_accuracy did not improve from 0.86937\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.1412 - accuracy: 0.8742 - val_loss: 0.1525 - val_accuracy: 0.8401\n",
      "Epoch 5/200\n",
      "1028/1033 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.8804\n",
      "Epoch 5: val_accuracy improved from 0.86937 to 0.87613, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.1122 - accuracy: 0.8800 - val_loss: 0.1430 - val_accuracy: 0.8761\n",
      "Epoch 6/200\n",
      "1032/1033 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.8963\n",
      "Epoch 6: val_accuracy improved from 0.87613 to 0.88288, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0986 - accuracy: 0.8964 - val_loss: 0.1141 - val_accuracy: 0.8829\n",
      "Epoch 7/200\n",
      "1011/1033 [============================>.] - ETA: 0s - loss: 0.0946 - accuracy: 0.9139\n",
      "Epoch 7: val_accuracy improved from 0.88288 to 0.89865, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0958 - accuracy: 0.9129 - val_loss: 0.0953 - val_accuracy: 0.8986\n",
      "Epoch 8/200\n",
      "1017/1033 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9056\n",
      "Epoch 8: val_accuracy did not improve from 0.89865\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0913 - accuracy: 0.9042 - val_loss: 0.0963 - val_accuracy: 0.8986\n",
      "Epoch 9/200\n",
      "1013/1033 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9082\n",
      "Epoch 9: val_accuracy did not improve from 0.89865\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0803 - accuracy: 0.9090 - val_loss: 0.1089 - val_accuracy: 0.8784\n",
      "Epoch 10/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9252\n",
      "Epoch 10: val_accuracy improved from 0.89865 to 0.93243, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0738 - accuracy: 0.9235 - val_loss: 0.0692 - val_accuracy: 0.9324\n",
      "Epoch 11/200\n",
      "1007/1033 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9196\n",
      "Epoch 11: val_accuracy did not improve from 0.93243\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0775 - accuracy: 0.9197 - val_loss: 0.0833 - val_accuracy: 0.8964\n",
      "Epoch 12/200\n",
      "1030/1033 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9252\n",
      "Epoch 12: val_accuracy did not improve from 0.93243\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0662 - accuracy: 0.9255 - val_loss: 0.0822 - val_accuracy: 0.9212\n",
      "Epoch 13/200\n",
      "1011/1033 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9238\n",
      "Epoch 13: val_accuracy improved from 0.93243 to 0.93468, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0784 - accuracy: 0.9235 - val_loss: 0.0713 - val_accuracy: 0.9347\n",
      "Epoch 14/200\n",
      "1026/1033 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9337\n",
      "Epoch 14: val_accuracy did not improve from 0.93468\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0573 - accuracy: 0.9342 - val_loss: 0.0695 - val_accuracy: 0.9324\n",
      "Epoch 15/200\n",
      "1009/1033 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9177\n",
      "Epoch 15: val_accuracy did not improve from 0.93468\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0649 - accuracy: 0.9187 - val_loss: 0.0912 - val_accuracy: 0.9324\n",
      "Epoch 16/200\n",
      "1011/1033 [============================>.] - ETA: 0s - loss: 0.0547 - accuracy: 0.9426\n",
      "Epoch 16: val_accuracy did not improve from 0.93468\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0537 - accuracy: 0.9439 - val_loss: 0.1087 - val_accuracy: 0.9009\n",
      "Epoch 17/200\n",
      "1000/1033 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9280\n",
      "Epoch 17: val_accuracy did not improve from 0.93468\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0575 - accuracy: 0.9303 - val_loss: 0.0694 - val_accuracy: 0.9189\n",
      "Epoch 18/200\n",
      "1026/1033 [============================>.] - ETA: 0s - loss: 0.0464 - accuracy: 0.9425\n",
      "Epoch 18: val_accuracy improved from 0.93468 to 0.94369, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0472 - accuracy: 0.9400 - val_loss: 0.0676 - val_accuracy: 0.9437\n",
      "Epoch 19/200\n",
      "1015/1033 [============================>.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9448\n",
      "Epoch 19: val_accuracy did not improve from 0.94369\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0446 - accuracy: 0.9458 - val_loss: 0.1000 - val_accuracy: 0.9077\n",
      "Epoch 20/200\n",
      " 999/1033 [============================>.] - ETA: 0s - loss: 0.0603 - accuracy: 0.9299\n",
      "Epoch 20: val_accuracy did not improve from 0.94369\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0591 - accuracy: 0.9303 - val_loss: 0.0619 - val_accuracy: 0.9279\n",
      "Epoch 21/200\n",
      "1006/1033 [============================>.] - ETA: 0s - loss: 0.0370 - accuracy: 0.9523\n",
      "Epoch 21: val_accuracy did not improve from 0.94369\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0365 - accuracy: 0.9535 - val_loss: 0.0743 - val_accuracy: 0.9369\n",
      "Epoch 22/200\n",
      "1006/1033 [============================>.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9463\n",
      "Epoch 22: val_accuracy improved from 0.94369 to 0.94820, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0521 - accuracy: 0.9458 - val_loss: 0.0783 - val_accuracy: 0.9482\n",
      "Epoch 23/200\n",
      "1009/1033 [============================>.] - ETA: 0s - loss: 0.0497 - accuracy: 0.9485\n",
      "Epoch 23: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0495 - accuracy: 0.9487 - val_loss: 0.0469 - val_accuracy: 0.9392\n",
      "Epoch 24/200\n",
      "1011/1033 [============================>.] - ETA: 0s - loss: 0.0488 - accuracy: 0.9456\n",
      "Epoch 24: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0485 - accuracy: 0.9458 - val_loss: 0.0673 - val_accuracy: 0.9144\n",
      "Epoch 25/200\n",
      "1014/1033 [============================>.] - ETA: 0s - loss: 0.0408 - accuracy: 0.9527\n",
      "Epoch 25: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0403 - accuracy: 0.9535 - val_loss: 0.0568 - val_accuracy: 0.9459\n",
      "Epoch 26/200\n",
      "1005/1033 [============================>.] - ETA: 0s - loss: 0.0440 - accuracy: 0.9542\n",
      "Epoch 26: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0446 - accuracy: 0.9535 - val_loss: 0.0453 - val_accuracy: 0.9459\n",
      "Epoch 27/200\n",
      "1018/1033 [============================>.] - ETA: 0s - loss: 0.0476 - accuracy: 0.9470\n",
      "Epoch 27: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0474 - accuracy: 0.9468 - val_loss: 0.0525 - val_accuracy: 0.9369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.0366 - accuracy: 0.9602\n",
      "Epoch 28: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0364 - accuracy: 0.9603 - val_loss: 0.1084 - val_accuracy: 0.9167\n",
      "Epoch 29/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9452\n",
      "Epoch 29: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0501 - accuracy: 0.9458 - val_loss: 0.0611 - val_accuracy: 0.9212\n",
      "Epoch 30/200\n",
      "1008/1033 [============================>.] - ETA: 0s - loss: 0.0442 - accuracy: 0.9544\n",
      "Epoch 30: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0435 - accuracy: 0.9545 - val_loss: 0.0520 - val_accuracy: 0.9414\n",
      "Epoch 31/200\n",
      "1017/1033 [============================>.] - ETA: 0s - loss: 0.0465 - accuracy: 0.9479\n",
      "Epoch 31: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0475 - accuracy: 0.9477 - val_loss: 0.0824 - val_accuracy: 0.9279\n",
      "Epoch 32/200\n",
      "1022/1033 [============================>.] - ETA: 0s - loss: 0.0483 - accuracy: 0.9511\n",
      "Epoch 32: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0485 - accuracy: 0.9497 - val_loss: 0.0634 - val_accuracy: 0.9369\n",
      "Epoch 33/200\n",
      "1027/1033 [============================>.] - ETA: 0s - loss: 0.0365 - accuracy: 0.9494\n",
      "Epoch 33: val_accuracy did not improve from 0.94820\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0366 - accuracy: 0.9497 - val_loss: 0.0476 - val_accuracy: 0.9437\n",
      "Epoch 34/200\n",
      "1013/1033 [============================>.] - ETA: 0s - loss: 0.0377 - accuracy: 0.9615\n",
      "Epoch 34: val_accuracy improved from 0.94820 to 0.95495, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0378 - accuracy: 0.9603 - val_loss: 0.0514 - val_accuracy: 0.9550\n",
      "Epoch 35/200\n",
      "1008/1033 [============================>.] - ETA: 0s - loss: 0.0437 - accuracy: 0.9613\n",
      "Epoch 35: val_accuracy did not improve from 0.95495\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0430 - accuracy: 0.9613 - val_loss: 0.0515 - val_accuracy: 0.9550\n",
      "Epoch 36/200\n",
      "1004/1033 [============================>.] - ETA: 0s - loss: 0.0474 - accuracy: 0.9602\n",
      "Epoch 36: val_accuracy did not improve from 0.95495\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0488 - accuracy: 0.9584 - val_loss: 0.0656 - val_accuracy: 0.9369\n",
      "Epoch 37/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9571\n",
      "Epoch 37: val_accuracy improved from 0.95495 to 0.96622, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0341 - accuracy: 0.9545 - val_loss: 0.0594 - val_accuracy: 0.9662\n",
      "Epoch 38/200\n",
      "1019/1033 [============================>.] - ETA: 0s - loss: 0.0479 - accuracy: 0.9480\n",
      "Epoch 38: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0474 - accuracy: 0.9487 - val_loss: 0.0732 - val_accuracy: 0.9459\n",
      "Epoch 39/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.0358 - accuracy: 0.9446\n",
      "Epoch 39: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0358 - accuracy: 0.9448 - val_loss: 0.0519 - val_accuracy: 0.9505\n",
      "Epoch 40/200\n",
      "1018/1033 [============================>.] - ETA: 0s - loss: 0.0362 - accuracy: 0.9568\n",
      "Epoch 40: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0360 - accuracy: 0.9574 - val_loss: 0.0718 - val_accuracy: 0.9414\n",
      "Epoch 41/200\n",
      "1020/1033 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 0.9520\n",
      "Epoch 41: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0433 - accuracy: 0.9516 - val_loss: 0.0560 - val_accuracy: 0.9459\n",
      "Epoch 42/200\n",
      "1021/1033 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9549\n",
      "Epoch 42: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0407 - accuracy: 0.9555 - val_loss: 0.0586 - val_accuracy: 0.9459\n",
      "Epoch 43/200\n",
      "1027/1033 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.9562\n",
      "Epoch 43: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0365 - accuracy: 0.9564 - val_loss: 0.0556 - val_accuracy: 0.9437\n",
      "Epoch 44/200\n",
      "1018/1033 [============================>.] - ETA: 0s - loss: 0.0305 - accuracy: 0.9607\n",
      "Epoch 44: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0310 - accuracy: 0.9603 - val_loss: 0.0811 - val_accuracy: 0.9369\n",
      "Epoch 45/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9432\n",
      "Epoch 45: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0489 - accuracy: 0.9429 - val_loss: 0.0614 - val_accuracy: 0.9505\n",
      "Epoch 46/200\n",
      "1010/1033 [============================>.] - ETA: 0s - loss: 0.0334 - accuracy: 0.9614\n",
      "Epoch 46: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0340 - accuracy: 0.9593 - val_loss: 0.0556 - val_accuracy: 0.9459\n",
      "Epoch 47/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0361 - accuracy: 0.9551\n",
      "Epoch 47: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0359 - accuracy: 0.9555 - val_loss: 0.0692 - val_accuracy: 0.9324\n",
      "Epoch 48/200\n",
      "1000/1033 [============================>.] - ETA: 0s - loss: 0.0331 - accuracy: 0.9580\n",
      "Epoch 48: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0325 - accuracy: 0.9584 - val_loss: 0.0771 - val_accuracy: 0.9167\n",
      "Epoch 49/200\n",
      "1001/1033 [============================>.] - ETA: 0s - loss: 0.0346 - accuracy: 0.9570\n",
      "Epoch 49: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0347 - accuracy: 0.9564 - val_loss: 0.0696 - val_accuracy: 0.9302\n",
      "Epoch 50/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0452 - accuracy: 0.9502\n",
      "Epoch 50: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0450 - accuracy: 0.9506 - val_loss: 0.0488 - val_accuracy: 0.9505\n",
      "Epoch 51/200\n",
      "1032/1033 [============================>.] - ETA: 0s - loss: 0.0373 - accuracy: 0.9603\n",
      "Epoch 51: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0372 - accuracy: 0.9603 - val_loss: 0.0724 - val_accuracy: 0.9234\n",
      "Epoch 52/200\n",
      "1019/1033 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9578\n",
      "Epoch 52: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0315 - accuracy: 0.9584 - val_loss: 0.0559 - val_accuracy: 0.9617\n",
      "Epoch 53/200\n",
      "1002/1033 [============================>.] - ETA: 0s - loss: 0.0413 - accuracy: 0.9561\n",
      "Epoch 53: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0422 - accuracy: 0.9555 - val_loss: 0.0643 - val_accuracy: 0.9392\n",
      "Epoch 54/200\n",
      "1013/1033 [============================>.] - ETA: 0s - loss: 0.0264 - accuracy: 0.9674\n",
      "Epoch 54: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0262 - accuracy: 0.9681 - val_loss: 0.0762 - val_accuracy: 0.9347\n",
      "Epoch 55/200\n",
      "1001/1033 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9640\n",
      "Epoch 55: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0320 - accuracy: 0.9642 - val_loss: 0.0818 - val_accuracy: 0.9302\n",
      "Epoch 56/200\n",
      "1015/1033 [============================>.] - ETA: 0s - loss: 0.0452 - accuracy: 0.9507\n",
      "Epoch 56: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0448 - accuracy: 0.9506 - val_loss: 0.0831 - val_accuracy: 0.9527\n",
      "Epoch 57/200\n",
      "1013/1033 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9664\n",
      "Epoch 57: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0269 - accuracy: 0.9671 - val_loss: 0.0630 - val_accuracy: 0.9527\n",
      "Epoch 58/200\n",
      "1033/1033 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9584\n",
      "Epoch 58: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0403 - accuracy: 0.9584 - val_loss: 0.0555 - val_accuracy: 0.9482\n",
      "Epoch 59/200\n",
      "1015/1033 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9655\n",
      "Epoch 59: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0347 - accuracy: 0.9661 - val_loss: 0.0560 - val_accuracy: 0.9482\n",
      "Epoch 60/200\n",
      "1007/1033 [============================>.] - ETA: 0s - loss: 0.0280 - accuracy: 0.9662\n",
      "Epoch 60: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0299 - accuracy: 0.9652 - val_loss: 0.0785 - val_accuracy: 0.9459\n",
      "Epoch 61/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0429 - accuracy: 0.9570\n",
      "Epoch 61: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0425 - accuracy: 0.9574 - val_loss: 0.0630 - val_accuracy: 0.9392\n",
      "Epoch 62/200\n",
      "1032/1033 [============================>.] - ETA: 0s - loss: 0.0334 - accuracy: 0.9593\n",
      "Epoch 62: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0334 - accuracy: 0.9593 - val_loss: 0.0583 - val_accuracy: 0.9505\n",
      "Epoch 63/200\n",
      "1020/1033 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9706\n",
      "Epoch 63: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0224 - accuracy: 0.9710 - val_loss: 0.0577 - val_accuracy: 0.9459\n",
      "Epoch 64/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0415 - accuracy: 0.9601\n",
      "Epoch 64: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0424 - accuracy: 0.9593 - val_loss: 0.0665 - val_accuracy: 0.9324\n",
      "Epoch 65/200\n",
      "1027/1033 [============================>.] - ETA: 0s - loss: 0.0288 - accuracy: 0.9640\n",
      "Epoch 65: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0288 - accuracy: 0.9642 - val_loss: 0.0481 - val_accuracy: 0.9505\n",
      "Epoch 66/200\n",
      "1014/1033 [============================>.] - ETA: 0s - loss: 0.0334 - accuracy: 0.9566\n",
      "Epoch 66: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0331 - accuracy: 0.9574 - val_loss: 0.0682 - val_accuracy: 0.9527\n",
      "Epoch 67/200\n",
      "1006/1033 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9692\n",
      "Epoch 67: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0290 - accuracy: 0.9681 - val_loss: 0.0639 - val_accuracy: 0.9572\n",
      "Epoch 68/200\n",
      " 993/1033 [===========================>..] - ETA: 0s - loss: 0.0349 - accuracy: 0.9637\n",
      "Epoch 68: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0342 - accuracy: 0.9642 - val_loss: 0.0537 - val_accuracy: 0.9482\n",
      "Epoch 69/200\n",
      "1023/1033 [============================>.] - ETA: 0s - loss: 0.0312 - accuracy: 0.9677\n",
      "Epoch 69: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0310 - accuracy: 0.9681 - val_loss: 0.0709 - val_accuracy: 0.9347\n",
      "Epoch 70/200\n",
      "1010/1033 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 0.9564\n",
      "Epoch 70: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0338 - accuracy: 0.9555 - val_loss: 0.0561 - val_accuracy: 0.9527\n",
      "Epoch 71/200\n",
      "1010/1033 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9653\n",
      "Epoch 71: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0308 - accuracy: 0.9661 - val_loss: 0.1037 - val_accuracy: 0.9189\n",
      "Epoch 72/200\n",
      "1009/1033 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9693\n",
      "Epoch 72: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0269 - accuracy: 0.9700 - val_loss: 0.0643 - val_accuracy: 0.9595\n",
      "Epoch 73/200\n",
      "1033/1033 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9642\n",
      "Epoch 73: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0371 - accuracy: 0.9642 - val_loss: 0.0822 - val_accuracy: 0.9234\n",
      "Epoch 74/200\n",
      " 988/1033 [===========================>..] - ETA: 0s - loss: 0.0266 - accuracy: 0.9626\n",
      "Epoch 74: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0260 - accuracy: 0.9632 - val_loss: 0.0762 - val_accuracy: 0.9550\n",
      "Epoch 75/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9708\n",
      "Epoch 75: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0322 - accuracy: 0.9710 - val_loss: 0.0697 - val_accuracy: 0.9414\n",
      "Epoch 76/200\n",
      "1017/1033 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9626\n",
      "Epoch 76: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0330 - accuracy: 0.9632 - val_loss: 0.0709 - val_accuracy: 0.9437\n",
      "Epoch 77/200\n",
      "1002/1033 [============================>.] - ETA: 0s - loss: 0.0357 - accuracy: 0.9581\n",
      "Epoch 77: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0350 - accuracy: 0.9593 - val_loss: 0.0883 - val_accuracy: 0.9414\n",
      "Epoch 78/200\n",
      "1032/1033 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9719\n",
      "Epoch 78: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0208 - accuracy: 0.9719 - val_loss: 0.1063 - val_accuracy: 0.9257\n",
      "Epoch 79/200\n",
      "1017/1033 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9685\n",
      "Epoch 79: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0330 - accuracy: 0.9681 - val_loss: 0.0712 - val_accuracy: 0.9414\n",
      "Epoch 80/200\n",
      "1028/1033 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9601\n",
      "Epoch 80: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0369 - accuracy: 0.9603 - val_loss: 0.0622 - val_accuracy: 0.9617\n",
      "Epoch 81/200\n",
      "1013/1033 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9733\n",
      "Epoch 81: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0252 - accuracy: 0.9729 - val_loss: 0.0748 - val_accuracy: 0.9482\n",
      "Epoch 82/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0354 - accuracy: 0.9580\n",
      "Epoch 82: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0351 - accuracy: 0.9584 - val_loss: 0.0522 - val_accuracy: 0.9527\n",
      "Epoch 83/200\n",
      "1020/1033 [============================>.] - ETA: 0s - loss: 0.0258 - accuracy: 0.9696\n",
      "Epoch 83: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0255 - accuracy: 0.9700 - val_loss: 0.0718 - val_accuracy: 0.9482\n",
      "Epoch 84/200\n",
      "1028/1033 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9669\n",
      "Epoch 84: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0273 - accuracy: 0.9671 - val_loss: 0.0737 - val_accuracy: 0.9459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/200\n",
      "1025/1033 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9590\n",
      "Epoch 85: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0362 - accuracy: 0.9593 - val_loss: 0.0525 - val_accuracy: 0.9617\n",
      "Epoch 86/200\n",
      "1005/1033 [============================>.] - ETA: 0s - loss: 0.0230 - accuracy: 0.9662\n",
      "Epoch 86: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0225 - accuracy: 0.9671 - val_loss: 0.0658 - val_accuracy: 0.9617\n",
      "Epoch 87/200\n",
      "1033/1033 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9642\n",
      "Epoch 87: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0294 - accuracy: 0.9642 - val_loss: 0.0628 - val_accuracy: 0.9505\n",
      "Epoch 88/200\n",
      "1002/1033 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9711\n",
      "Epoch 88: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0360 - accuracy: 0.9700 - val_loss: 0.0727 - val_accuracy: 0.9505\n",
      "Epoch 89/200\n",
      "1028/1033 [============================>.] - ETA: 0s - loss: 0.0382 - accuracy: 0.9562\n",
      "Epoch 89: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0380 - accuracy: 0.9564 - val_loss: 0.0602 - val_accuracy: 0.9617\n",
      "Epoch 90/200\n",
      "1002/1033 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9681\n",
      "Epoch 90: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.9671 - val_loss: 0.0575 - val_accuracy: 0.9550\n",
      "Epoch 91/200\n",
      "1008/1033 [============================>.] - ETA: 0s - loss: 0.0350 - accuracy: 0.9712\n",
      "Epoch 91: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0360 - accuracy: 0.9690 - val_loss: 0.0694 - val_accuracy: 0.9527\n",
      "Epoch 92/200\n",
      "1026/1033 [============================>.] - ETA: 0s - loss: 0.0299 - accuracy: 0.9620\n",
      "Epoch 92: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0300 - accuracy: 0.9613 - val_loss: 0.0583 - val_accuracy: 0.9617\n",
      "Epoch 93/200\n",
      "1031/1033 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9777\n",
      "Epoch 93: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0222 - accuracy: 0.9777 - val_loss: 0.0567 - val_accuracy: 0.9550\n",
      "Epoch 94/200\n",
      "1015/1033 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9626\n",
      "Epoch 94: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0296 - accuracy: 0.9613 - val_loss: 0.0569 - val_accuracy: 0.9595\n",
      "Epoch 95/200\n",
      "1028/1033 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9621\n",
      "Epoch 95: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0277 - accuracy: 0.9622 - val_loss: 0.0841 - val_accuracy: 0.9459\n",
      "Epoch 96/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9711\n",
      "Epoch 96: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0278 - accuracy: 0.9710 - val_loss: 0.0679 - val_accuracy: 0.9617\n",
      "Epoch 97/200\n",
      "1009/1033 [============================>.] - ETA: 0s - loss: 0.0276 - accuracy: 0.9693\n",
      "Epoch 97: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0271 - accuracy: 0.9700 - val_loss: 0.0465 - val_accuracy: 0.9572\n",
      "Epoch 98/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0276 - accuracy: 0.9717\n",
      "Epoch 98: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0275 - accuracy: 0.9719 - val_loss: 0.0777 - val_accuracy: 0.9482\n",
      "Epoch 99/200\n",
      " 988/1033 [===========================>..] - ETA: 0s - loss: 0.0238 - accuracy: 0.9727\n",
      "Epoch 99: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0231 - accuracy: 0.9729 - val_loss: 0.0659 - val_accuracy: 0.9527\n",
      "Epoch 100/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.0287 - accuracy: 0.9689\n",
      "Epoch 100: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0286 - accuracy: 0.9690 - val_loss: 0.0854 - val_accuracy: 0.9459\n",
      "Epoch 101/200\n",
      "1032/1033 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9661\n",
      "Epoch 101: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9661 - val_loss: 0.0661 - val_accuracy: 0.9527\n",
      "Epoch 102/200\n",
      "1022/1033 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9677\n",
      "Epoch 102: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0275 - accuracy: 0.9681 - val_loss: 0.0788 - val_accuracy: 0.9392\n",
      "Epoch 103/200\n",
      "1020/1033 [============================>.] - ETA: 0s - loss: 0.0250 - accuracy: 0.9706\n",
      "Epoch 103: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0251 - accuracy: 0.9700 - val_loss: 0.0495 - val_accuracy: 0.9617\n",
      "Epoch 104/200\n",
      "1007/1033 [============================>.] - ETA: 0s - loss: 0.0276 - accuracy: 0.9682\n",
      "Epoch 104: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0276 - accuracy: 0.9671 - val_loss: 0.1326 - val_accuracy: 0.9279\n",
      "Epoch 105/200\n",
      "1004/1033 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9681\n",
      "Epoch 105: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0277 - accuracy: 0.9671 - val_loss: 0.0626 - val_accuracy: 0.9662\n",
      "Epoch 106/200\n",
      "1033/1033 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9613\n",
      "Epoch 106: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0281 - accuracy: 0.9613 - val_loss: 0.0541 - val_accuracy: 0.9527\n",
      "Epoch 107/200\n",
      "1020/1033 [============================>.] - ETA: 0s - loss: 0.0233 - accuracy: 0.9657\n",
      "Epoch 107: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0232 - accuracy: 0.9661 - val_loss: 0.0584 - val_accuracy: 0.9595\n",
      "Epoch 108/200\n",
      "1025/1033 [============================>.] - ETA: 0s - loss: 0.0283 - accuracy: 0.9766\n",
      "Epoch 108: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0298 - accuracy: 0.9758 - val_loss: 0.0685 - val_accuracy: 0.9482\n",
      "Epoch 109/200\n",
      "1021/1033 [============================>.] - ETA: 0s - loss: 0.0258 - accuracy: 0.9667\n",
      "Epoch 109: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0256 - accuracy: 0.9671 - val_loss: 0.0527 - val_accuracy: 0.9595\n",
      "Epoch 110/200\n",
      "1031/1033 [============================>.] - ETA: 0s - loss: 0.0325 - accuracy: 0.9670\n",
      "Epoch 110: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0325 - accuracy: 0.9671 - val_loss: 0.0759 - val_accuracy: 0.9572\n",
      "Epoch 111/200\n",
      "1014/1033 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9773\n",
      "Epoch 111: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0197 - accuracy: 0.9777 - val_loss: 0.0744 - val_accuracy: 0.9437\n",
      "Epoch 112/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.0262 - accuracy: 0.9640\n",
      "Epoch 112: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0261 - accuracy: 0.9642 - val_loss: 0.0744 - val_accuracy: 0.9414\n",
      "Epoch 113/200\n",
      "1026/1033 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9669\n",
      "Epoch 113: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0369 - accuracy: 0.9671 - val_loss: 0.0947 - val_accuracy: 0.9279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "1008/1033 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9762\n",
      "Epoch 114: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0242 - accuracy: 0.9739 - val_loss: 0.0734 - val_accuracy: 0.9459\n",
      "Epoch 115/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0213 - accuracy: 0.9751\n",
      "Epoch 115: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0209 - accuracy: 0.9758 - val_loss: 0.0903 - val_accuracy: 0.9437\n",
      "Epoch 116/200\n",
      "1011/1033 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9802\n",
      "Epoch 116: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0202 - accuracy: 0.9787 - val_loss: 0.0594 - val_accuracy: 0.9640\n",
      "Epoch 117/200\n",
      "1031/1033 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9690\n",
      "Epoch 117: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0260 - accuracy: 0.9690 - val_loss: 0.0612 - val_accuracy: 0.9482\n",
      "Epoch 118/200\n",
      "1004/1033 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9681\n",
      "Epoch 118: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0217 - accuracy: 0.9661 - val_loss: 0.0531 - val_accuracy: 0.9662\n",
      "Epoch 119/200\n",
      "1007/1033 [============================>.] - ETA: 0s - loss: 0.0302 - accuracy: 0.9682\n",
      "Epoch 119: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0299 - accuracy: 0.9671 - val_loss: 0.0646 - val_accuracy: 0.9595\n",
      "Epoch 120/200\n",
      "1018/1033 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9695\n",
      "Epoch 120: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0225 - accuracy: 0.9690 - val_loss: 0.0777 - val_accuracy: 0.9505\n",
      "Epoch 121/200\n",
      "1014/1033 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9724\n",
      "Epoch 121: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0252 - accuracy: 0.9729 - val_loss: 0.0975 - val_accuracy: 0.9482\n",
      "Epoch 122/200\n",
      "1022/1033 [============================>.] - ETA: 0s - loss: 0.0273 - accuracy: 0.9736\n",
      "Epoch 122: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0270 - accuracy: 0.9739 - val_loss: 0.0707 - val_accuracy: 0.9527\n",
      "Epoch 123/200\n",
      "1010/1033 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9762\n",
      "Epoch 123: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0224 - accuracy: 0.9768 - val_loss: 0.1293 - val_accuracy: 0.9369\n",
      "Epoch 124/200\n",
      "1010/1033 [============================>.] - ETA: 0s - loss: 0.0326 - accuracy: 0.9644\n",
      "Epoch 124: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9652 - val_loss: 0.0817 - val_accuracy: 0.9437\n",
      "Epoch 125/200\n",
      "1033/1033 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9690\n",
      "Epoch 125: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0261 - accuracy: 0.9690 - val_loss: 0.0843 - val_accuracy: 0.9527\n",
      "Epoch 126/200\n",
      "1025/1033 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9766\n",
      "Epoch 126: val_accuracy did not improve from 0.96622\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0228 - accuracy: 0.9768 - val_loss: 0.0676 - val_accuracy: 0.9459\n",
      "Epoch 127/200\n",
      "1012/1033 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9713\n",
      "Epoch 127: val_accuracy improved from 0.96622 to 0.96847, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0261 - accuracy: 0.9719 - val_loss: 0.0644 - val_accuracy: 0.9685\n",
      "Epoch 128/200\n",
      "1010/1033 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9713\n",
      "Epoch 128: val_accuracy did not improve from 0.96847\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0262 - accuracy: 0.9710 - val_loss: 0.0965 - val_accuracy: 0.9347\n",
      "Epoch 129/200\n",
      "1026/1033 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9659\n",
      "Epoch 129: val_accuracy did not improve from 0.96847\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0350 - accuracy: 0.9661 - val_loss: 0.1043 - val_accuracy: 0.9414\n",
      "Epoch 130/200\n",
      "1017/1033 [============================>.] - ETA: 0s - loss: 0.0230 - accuracy: 0.9744\n",
      "Epoch 130: val_accuracy improved from 0.96847 to 0.97297, saving model to Best_Logistic_before_over_Sampling_Stochastic.h5\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.9748 - val_loss: 0.0569 - val_accuracy: 0.9730\n",
      "Epoch 131/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9795\n",
      "Epoch 131: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0147 - accuracy: 0.9797 - val_loss: 0.0802 - val_accuracy: 0.9505\n",
      "Epoch 132/200\n",
      "1015/1033 [============================>.] - ETA: 0s - loss: 0.0217 - accuracy: 0.9783\n",
      "Epoch 132: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0214 - accuracy: 0.9787 - val_loss: 0.0981 - val_accuracy: 0.9505\n",
      "Epoch 133/200\n",
      "1012/1033 [============================>.] - ETA: 0s - loss: 0.0362 - accuracy: 0.9575\n",
      "Epoch 133: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0356 - accuracy: 0.9584 - val_loss: 0.0866 - val_accuracy: 0.9437\n",
      "Epoch 134/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0216 - accuracy: 0.9731\n",
      "Epoch 134: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0243 - accuracy: 0.9729 - val_loss: 0.0719 - val_accuracy: 0.9527\n",
      "Epoch 135/200\n",
      "1032/1033 [============================>.] - ETA: 0s - loss: 0.0255 - accuracy: 0.9748\n",
      "Epoch 135: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0255 - accuracy: 0.9748 - val_loss: 0.0817 - val_accuracy: 0.9482\n",
      "Epoch 136/200\n",
      "1015/1033 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9842\n",
      "Epoch 136: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0146 - accuracy: 0.9845 - val_loss: 0.1114 - val_accuracy: 0.9482\n",
      "Epoch 137/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0314 - accuracy: 0.9688\n",
      "Epoch 137: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0311 - accuracy: 0.9690 - val_loss: 0.0773 - val_accuracy: 0.9662\n",
      "Epoch 138/200\n",
      "1031/1033 [============================>.] - ETA: 0s - loss: 0.0289 - accuracy: 0.9728\n",
      "Epoch 138: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0288 - accuracy: 0.9729 - val_loss: 0.0641 - val_accuracy: 0.9662\n",
      "Epoch 139/200\n",
      "1017/1033 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9744\n",
      "Epoch 139: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0303 - accuracy: 0.9729 - val_loss: 0.0648 - val_accuracy: 0.9617\n",
      "Epoch 140/200\n",
      "1023/1033 [============================>.] - ETA: 0s - loss: 0.0260 - accuracy: 0.9785\n",
      "Epoch 140: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0258 - accuracy: 0.9787 - val_loss: 0.0591 - val_accuracy: 0.9617\n",
      "Epoch 141/200\n",
      "1022/1033 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9755\n",
      "Epoch 141: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0173 - accuracy: 0.9758 - val_loss: 0.0679 - val_accuracy: 0.9617\n",
      "Epoch 142/200\n",
      "1018/1033 [============================>.] - ETA: 0s - loss: 0.0242 - accuracy: 0.9725\n",
      "Epoch 142: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0238 - accuracy: 0.9729 - val_loss: 0.0897 - val_accuracy: 0.9414\n",
      "Epoch 143/200\n",
      "1008/1033 [============================>.] - ETA: 0s - loss: 0.0296 - accuracy: 0.9663\n",
      "Epoch 143: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0289 - accuracy: 0.9671 - val_loss: 0.0795 - val_accuracy: 0.9505\n",
      "Epoch 144/200\n",
      "1030/1033 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9757\n",
      "Epoch 144: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0180 - accuracy: 0.9758 - val_loss: 0.0840 - val_accuracy: 0.9459\n",
      "Epoch 145/200\n",
      "1017/1033 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9794\n",
      "Epoch 145: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0272 - accuracy: 0.9797 - val_loss: 0.0545 - val_accuracy: 0.9617\n",
      "Epoch 146/200\n",
      " 992/1033 [===========================>..] - ETA: 0s - loss: 0.0324 - accuracy: 0.9708\n",
      "Epoch 146: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0339 - accuracy: 0.9700 - val_loss: 0.0727 - val_accuracy: 0.9369\n",
      "Epoch 147/200\n",
      " 999/1033 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9770\n",
      "Epoch 147: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0195 - accuracy: 0.9777 - val_loss: 0.0959 - val_accuracy: 0.9437\n",
      "Epoch 148/200\n",
      "1001/1033 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9780\n",
      "Epoch 148: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0285 - accuracy: 0.9768 - val_loss: 0.0782 - val_accuracy: 0.9550\n",
      "Epoch 149/200\n",
      "1006/1033 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9841\n",
      "Epoch 149: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0149 - accuracy: 0.9835 - val_loss: 0.0669 - val_accuracy: 0.9527\n",
      "Epoch 150/200\n",
      "1011/1033 [============================>.] - ETA: 0s - loss: 0.0262 - accuracy: 0.9743\n",
      "Epoch 150: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0260 - accuracy: 0.9739 - val_loss: 0.0763 - val_accuracy: 0.9595\n",
      "Epoch 151/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0240 - accuracy: 0.9691\n",
      "Epoch 151: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.9700 - val_loss: 0.0890 - val_accuracy: 0.9550\n",
      "Epoch 152/200\n",
      "1006/1033 [============================>.] - ETA: 0s - loss: 0.0190 - accuracy: 0.9811\n",
      "Epoch 152: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0196 - accuracy: 0.9797 - val_loss: 0.0826 - val_accuracy: 0.9550\n",
      "Epoch 153/200\n",
      "1030/1033 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9748\n",
      "Epoch 153: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0280 - accuracy: 0.9748 - val_loss: 0.0788 - val_accuracy: 0.9505\n",
      "Epoch 154/200\n",
      "1009/1033 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9782\n",
      "Epoch 154: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0211 - accuracy: 0.9768 - val_loss: 0.0734 - val_accuracy: 0.9550\n",
      "Epoch 155/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9731\n",
      "Epoch 155: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0311 - accuracy: 0.9729 - val_loss: 0.0856 - val_accuracy: 0.9482\n",
      "Epoch 156/200\n",
      "1025/1033 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9805\n",
      "Epoch 156: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0200 - accuracy: 0.9806 - val_loss: 0.0726 - val_accuracy: 0.9640\n",
      "Epoch 157/200\n",
      " 988/1033 [===========================>..] - ETA: 0s - loss: 0.0250 - accuracy: 0.9767\n",
      "Epoch 157: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0283 - accuracy: 0.9748 - val_loss: 0.1247 - val_accuracy: 0.9279\n",
      "Epoch 158/200\n",
      "1011/1033 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9693\n",
      "Epoch 158: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0218 - accuracy: 0.9681 - val_loss: 0.0789 - val_accuracy: 0.9572\n",
      "Epoch 159/200\n",
      "1008/1033 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9732\n",
      "Epoch 159: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0185 - accuracy: 0.9739 - val_loss: 0.0727 - val_accuracy: 0.9617\n",
      "Epoch 160/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0219 - accuracy: 0.9761\n",
      "Epoch 160: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0213 - accuracy: 0.9768 - val_loss: 0.1027 - val_accuracy: 0.9437\n",
      "Epoch 161/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9746\n",
      "Epoch 161: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9748 - val_loss: 0.0983 - val_accuracy: 0.9550\n",
      "Epoch 162/200\n",
      "1020/1033 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9716\n",
      "Epoch 162: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0237 - accuracy: 0.9719 - val_loss: 0.0720 - val_accuracy: 0.9595\n",
      "Epoch 163/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9670\n",
      "Epoch 163: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0274 - accuracy: 0.9671 - val_loss: 0.0816 - val_accuracy: 0.9437\n",
      "Epoch 164/200\n",
      "1005/1033 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.9721\n",
      "Epoch 164: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0244 - accuracy: 0.9719 - val_loss: 0.0599 - val_accuracy: 0.9730\n",
      "Epoch 165/200\n",
      "1015/1033 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9744\n",
      "Epoch 165: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0267 - accuracy: 0.9739 - val_loss: 0.0954 - val_accuracy: 0.9392\n",
      "Epoch 166/200\n",
      " 997/1033 [===========================>..] - ETA: 0s - loss: 0.0269 - accuracy: 0.9769\n",
      "Epoch 166: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0274 - accuracy: 0.9768 - val_loss: 0.0897 - val_accuracy: 0.9459\n",
      "Epoch 167/200\n",
      "1028/1033 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9737\n",
      "Epoch 167: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0251 - accuracy: 0.9739 - val_loss: 0.0857 - val_accuracy: 0.9685\n",
      "Epoch 168/200\n",
      "1012/1033 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9713\n",
      "Epoch 168: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0258 - accuracy: 0.9719 - val_loss: 0.0898 - val_accuracy: 0.9482\n",
      "Epoch 169/200\n",
      "1012/1033 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9802\n",
      "Epoch 169: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0191 - accuracy: 0.9787 - val_loss: 0.0710 - val_accuracy: 0.9617\n",
      "Epoch 170/200\n",
      "1017/1033 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.9715\n",
      "Epoch 170: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0240 - accuracy: 0.9719 - val_loss: 0.0743 - val_accuracy: 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200\n",
      "1006/1033 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9732\n",
      "Epoch 171: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0196 - accuracy: 0.9739 - val_loss: 0.0691 - val_accuracy: 0.9662\n",
      "Epoch 172/200\n",
      "1020/1033 [============================>.] - ETA: 0s - loss: 0.0382 - accuracy: 0.9676\n",
      "Epoch 172: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0378 - accuracy: 0.9681 - val_loss: 0.0613 - val_accuracy: 0.9662\n",
      "Epoch 173/200\n",
      " 999/1033 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9790\n",
      "Epoch 173: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0148 - accuracy: 0.9777 - val_loss: 0.0738 - val_accuracy: 0.9595\n",
      "Epoch 174/200\n",
      "1027/1033 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9805\n",
      "Epoch 174: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0188 - accuracy: 0.9806 - val_loss: 0.0987 - val_accuracy: 0.9550\n",
      "Epoch 175/200\n",
      "1031/1033 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9748\n",
      "Epoch 175: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0170 - accuracy: 0.9748 - val_loss: 0.1070 - val_accuracy: 0.9459\n",
      "Epoch 176/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9689\n",
      "Epoch 176: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0237 - accuracy: 0.9690 - val_loss: 0.0657 - val_accuracy: 0.9617\n",
      "Epoch 177/200\n",
      "1029/1033 [============================>.] - ETA: 0s - loss: 0.0178 - accuracy: 0.9767\n",
      "Epoch 177: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0177 - accuracy: 0.9768 - val_loss: 0.0871 - val_accuracy: 0.9482\n",
      "Epoch 178/200\n",
      "1028/1033 [============================>.] - ETA: 0s - loss: 0.0280 - accuracy: 0.9728\n",
      "Epoch 178: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0279 - accuracy: 0.9729 - val_loss: 0.0921 - val_accuracy: 0.9527\n",
      "Epoch 179/200\n",
      "1001/1033 [============================>.] - ETA: 0s - loss: 0.0300 - accuracy: 0.9750\n",
      "Epoch 179: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0300 - accuracy: 0.9748 - val_loss: 0.0585 - val_accuracy: 0.9617\n",
      "Epoch 180/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9811\n",
      "Epoch 180: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0123 - accuracy: 0.9816 - val_loss: 0.0626 - val_accuracy: 0.9707\n",
      "Epoch 181/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9756\n",
      "Epoch 181: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0220 - accuracy: 0.9758 - val_loss: 0.1433 - val_accuracy: 0.9347\n",
      "Epoch 182/200\n",
      "1003/1033 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9671\n",
      "Epoch 182: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0311 - accuracy: 0.9671 - val_loss: 0.0817 - val_accuracy: 0.9595\n",
      "Epoch 183/200\n",
      "1005/1033 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9811\n",
      "Epoch 183: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.9787 - val_loss: 0.0894 - val_accuracy: 0.9459\n",
      "Epoch 184/200\n",
      "1000/1033 [============================>.] - ETA: 0s - loss: 0.0233 - accuracy: 0.9710\n",
      "Epoch 184: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.9719 - val_loss: 0.0866 - val_accuracy: 0.9572\n",
      "Epoch 185/200\n",
      "1013/1033 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9753\n",
      "Epoch 185: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0183 - accuracy: 0.9758 - val_loss: 0.0721 - val_accuracy: 0.9527\n",
      "Epoch 186/200\n",
      "1008/1033 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9841\n",
      "Epoch 186: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0130 - accuracy: 0.9835 - val_loss: 0.0876 - val_accuracy: 0.9662\n",
      "Epoch 187/200\n",
      "1016/1033 [============================>.] - ETA: 0s - loss: 0.0333 - accuracy: 0.9665\n",
      "Epoch 187: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0328 - accuracy: 0.9671 - val_loss: 0.0801 - val_accuracy: 0.9505\n",
      "Epoch 188/200\n",
      "1024/1033 [============================>.] - ETA: 0s - loss: 0.0245 - accuracy: 0.9668\n",
      "Epoch 188: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0243 - accuracy: 0.9671 - val_loss: 0.0662 - val_accuracy: 0.9550\n",
      "Epoch 189/200\n",
      "1032/1033 [============================>.] - ETA: 0s - loss: 0.0234 - accuracy: 0.9758\n",
      "Epoch 189: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0234 - accuracy: 0.9758 - val_loss: 0.0724 - val_accuracy: 0.9527\n",
      "Epoch 190/200\n",
      "1022/1033 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9755\n",
      "Epoch 190: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0178 - accuracy: 0.9758 - val_loss: 0.0854 - val_accuracy: 0.9459\n",
      "Epoch 191/200\n",
      "1004/1033 [============================>.] - ETA: 0s - loss: 0.0282 - accuracy: 0.9701\n",
      "Epoch 191: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0290 - accuracy: 0.9690 - val_loss: 0.0916 - val_accuracy: 0.9505\n",
      "Epoch 192/200\n",
      "1000/1033 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9810\n",
      "Epoch 192: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0200 - accuracy: 0.9806 - val_loss: 0.1124 - val_accuracy: 0.9550\n",
      "Epoch 193/200\n",
      "1028/1033 [============================>.] - ETA: 0s - loss: 0.0196 - accuracy: 0.9786\n",
      "Epoch 193: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0195 - accuracy: 0.9787 - val_loss: 0.0719 - val_accuracy: 0.9707\n",
      "Epoch 194/200\n",
      "1027/1033 [============================>.] - ETA: 0s - loss: 0.0246 - accuracy: 0.9718\n",
      "Epoch 194: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0246 - accuracy: 0.9719 - val_loss: 0.1067 - val_accuracy: 0.9347\n",
      "Epoch 195/200\n",
      "1005/1033 [============================>.] - ETA: 0s - loss: 0.0190 - accuracy: 0.9761\n",
      "Epoch 195: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0213 - accuracy: 0.9748 - val_loss: 0.0950 - val_accuracy: 0.9595\n",
      "Epoch 196/200\n",
      "1021/1033 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9824\n",
      "Epoch 196: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0218 - accuracy: 0.9826 - val_loss: 0.0890 - val_accuracy: 0.9595\n",
      "Epoch 197/200\n",
      "1030/1033 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9796\n",
      "Epoch 197: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0171 - accuracy: 0.9797 - val_loss: 0.0624 - val_accuracy: 0.9662\n",
      "Epoch 198/200\n",
      "1012/1033 [============================>.] - ETA: 0s - loss: 0.0296 - accuracy: 0.9694\n",
      "Epoch 198: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0291 - accuracy: 0.9700 - val_loss: 0.0576 - val_accuracy: 0.9595\n",
      "Epoch 199/200\n",
      "1007/1033 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9831\n",
      "Epoch 199: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0122 - accuracy: 0.9835 - val_loss: 0.0660 - val_accuracy: 0.9640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200\n",
      "1006/1033 [============================>.] - ETA: 0s - loss: 0.0246 - accuracy: 0.9761\n",
      "Epoch 200: val_accuracy did not improve from 0.97297\n",
      "1033/1033 [==============================] - 2s 2ms/step - loss: 0.0251 - accuracy: 0.9748 - val_loss: 0.1138 - val_accuracy: 0.9437\n"
     ]
    }
   ],
   "source": [
    "mc = ModelCheckpoint('Best_Logistic_before_over_Sampling_Stochastic.h5', \n",
    "                     monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "model_NN=keras.Sequential([keras.Input(shape=(23)),\n",
    "                        layers.Dense(4,activation='softmax')])\n",
    "\n",
    "\n",
    "model_NN.compile(loss=SparseCategoricalFocalLoss(from_logits=False,gamma=2),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_NN.fit(X_train,Y_train,validation_data=[X_test,Y_test] \n",
    "          ,  batch_size=1 , epochs=200 , verbose =1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "84f8a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy=history.history[\"val_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0339f813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxMAAAMTCAYAAADaWkpQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAxOAAAMTgF/d4wjAADyGklEQVR4nOydd3gU9dbHv5uekA4pJBASCB1DFcWCBhVUsHexcm1cbNiu116uqJf3ggW7AoqoNFFBEEGCjYgIoQkJJCSkk957su8fh192dnZmd2ZLloTzeZ48kM3s7OzU8z3VkJKSYgTDMAzDMAzDMIxOPNy9AQzDMAzDMAzDdE9YTDAMwzAMwzAMYxcsJhiGYRiGYRiGsQsWEwzDMAzDMAzD2AWLCYZhGIZhGIZh7ILFBMMwDMMwDMMwdsFigmEYhmEYhmEYu2AxwTCMW/ntt9+wcuVKl6z7tddew8MPP6z7fcXFxUhOTsaePXucvk0M40oefvhhvPbaa+7eDIZhTiG83L0BDMOc2vz222/YtWsXrr/+eqev+9Zbb0Vra6vu94WHh+Odd97BgAEDnL5NDMMwDNOTYDHBMEy3obm5Gb6+vpqXj42NtetzfHx8MGLECLve251pa2uDwWCAp6enuzelS9B7PjEMwzCWsJhgGMZtvPbaa9i0aRMAIDk5GQAQFRWFr776Cnv27MHcuXPxwgsv4I8//sD27dsREBCAL7/8EtnZ2fj888/x999/o7KyEn369MHEiRPxj3/8A4GBgWbrLy4uxhtvvAEAnet8+eWXsWPHDmzbtg3e3t44++yzMWfOHPj5+QGgNKebbroJCxcuxJgxYwAA7e3tWLp0KTZs2ID6+nqMGDECDz30EO644w7861//wsUXX6z6PbVur9jGzz//HOnp6Whvb0dMTAyuvvpqTJ8+vXOZ9evX47vvvsOxY8fg4+ODgQMH4u6778aoUaM6v+OXX36J6Ojozvf88MMPeP3115GSktL5WnJyMmbOnAk/Pz+sX78epaWlWL58OYKCgvDxxx9j9+7dKCkpQa9evTBs2DDce++96N+/v9n2FhUVYcmSJfjrr79QV1eHPn36YNKkSXjggQewYsUKfPLJJ1i5ciVCQ0M732M0GjFz5kyMGjUKTz31lOI+u+OOOxAXF4eXXnrJ7PWDBw9izpw5mDdvHiZNmgQAyMzMxJIlS7Bv3z60tLRg8ODBuOeee5CUlGR2LuzatQvPP/883n33XWRmZuLmm2/GHXfcgS1btmDFihXIz8+Hp6cnIiMjceWVV+Lyyy8HQKlD0dHRePLJJ8225cYbb8TFF1+MO+64AwCQl5eHDz/8EAcOHEB9fT3CwsIwfPhwPPvss4oCraWlBddeey0uvvhi/POf/zT729atW/Hyyy9j8eLFSEhIwL59+7By5UpkZGSgpqYGUVFROP/883HLLbfAx8dHcR9qoaSkBEuXLsXevXtRVlaGsLAwJCUl4d5770Xv3r3Nls3MzMSnn36Kffv2oampCVFRUZg2bRpmzpzZucyvv/6KFStWICsrCx4eHoiLi8Mtt9yCs88+2+5tZBjm5IbFBMMwbuPWW29FVVUVMjIy8MorrwAAvL29zZZZtGgRJk2ahGeeeaYzZamkpAR9+/ZFcnIygoKCUFJSglWrVuHJJ5/EokWLbH7uokWLcNZZZ+G5555DXl4ePvjgA4SEhOCuu+5Sfc+nn36K5cuX44YbbsD48eNx5MgRPPPMM5q+p9bt/e233/D8888jKSkJjzzyCEJCQpCdnY3jx493LvPee+9h5cqVmD59Ou68804AwKFDh1BSUqJpW+Rs2rQJ/fr1w/333w+DwYDAwEA0NDSgra0Nt99+O8LDw1FfX48ff/wRc+bMwdKlSxEeHg6ARNfs2bPh7++PWbNmITY2FsePH8dff/0FALjkkkuwePFi/PDDD7jxxhs7P/Ovv/5CUVER/v3vf6tu10UXXYRPP/0UtbW1CAoK6nx9y5YtCA0NxcSJEwEAhw8fxkMPPYTExEQ89thj8PX1xbp16/DYY4/h7bffxtChQzvfW19fj1deeQU33ngj7r77bgQEBGD//v2YN28errrqKtx3333o6OhAbm4u6uvrde/Lf//73wgKCsLDDz+MkJAQlJWVYceOHejo6FAUEz4+PjjvvPPw008/4d577zVbZsuWLUhMTERCQgIAEm3Dhg3DpZdeioCAABQUFOCLL75AYWGh5vNQicrKSvTq1Qv33HMPQkNDUVVVhW+//RYPPPAAli5d2ilU0tPT8fDDDyM2Nhb//Oc/ERERgfz8fBw9erRzXWvXrsVbb72Fc889F9dddx38/f1x5MgRFBcX2719DMOc/LCYYBjGbcTGxiI0NBReXl6qaUUjRozAI488YvbaGWecgTPOOKPz9/b2diQlJeHGG29EZmYmEhMTrX7u6NGj8eCDDwIATj/9dOTl5WHbtm2qYqK2tharVq3CZZddhnvvvRcAMGHCBHh6euK9996z+T21bK/RaMSiRYswZMgQLFiwAAaDAQAwfvz4zvcVFBRg9erVuP766zF79uzO14WH3l5ef/11M+92YGAgHn30UbPtPeOMM3DnnXdi69atuPbaawEAS5YsQUtLCz755BMzL7aI0gQHByM5ORnr16/HDTfc0Pmd1q1bh/j4eJx22mmq23ThhRfik08+QUpKSmeEoK2tDVu3bsWUKVM6De/3338fkZGRWLBgQacQPf300zFr1ix89tlnnSIVABobG/Hss8+a7a8VK1YgMDAQDzzwQOdrp59+us49CFRXV6OgoAD/+c9/zLzwF154odX3TZ06FevXr8euXbs6BVJVVRX+/PNP3HPPPZ3LTZs2zex9p512Gvr374+5c+figQceQEhIiO5tBoChQ4eaCS5xrK+66ir8+eefOOeccwCQiA0ODsa7777bmRo2bty4zvfV19fjo48+wuTJk/Hiiy92vi6+E8MwPRcWEwzDnNQopUe0tbVh5cqV+PHHH3H8+HE0NTV1/i03N9emmDjzzDPNfk9ISMD333+vuvzRo0fR1NSE888/3+z18847T5OY0LK9eXl5OH78OG6++eZOo1vOrl270NHRYZby5Cinn366YprMzz//jFWrViE3Nxe1tbVm2yv466+/MGnSJIt0GClXXHEFNm3ahN27d2P8+PEoLy/H9u3bLdJ65ERFRWH06NHYvHlzp5j4888/UV1d3WlYNzc3Y+/evZg5cyY8PDzQ3t7e+f7x48djy5YtZuv08vIyE3UAGdO1tbV45ZVXcOGFF2LkyJEWqWdaCA4ORkxMDD766CNUVVVhzJgxmmp2Ro0ahb59+2Lz5s2dhvfWrVthNBrNhEhDQwOWL1+Obdu2obS01KyxQEFBgd1iAgC+++47rFu3DoWFhWhoaOh8XRzrpqYmHDhwADfeeKNqjcnff/+NxsZGp56bDMN0D1hMMAxzUiNSaqR89NFH+O6773D77bdjyJAhCAgIQEdHB+bMmYOWlhab6wwODjb73cfHx2rXp/LycgAwy/sHgLCwMA3fQNv2VldXAwAiIiJU11NTU2NzGb0o7d/U1FS88MILuPLKK3HrrbciODgYHh4emD9/vtn+ra6uRp8+fayuf/jw4RgyZAjWrVuH8ePHY+PGjfD29sbUqVNtbttFF12E+fPno6ioCH379sWPP/6IuLi4Tk96TU0NOjo6sGzZMixbtkxxHR0dHfDwoC7ooaGhnf8XjBkzBi+88AJWr16Np59+uvO12bNnY9CgQTa3UWAwGDB//nwsXboUH3zwAWpraxETE4MbbrihUwypve/CCy/EqlWr0NjYCH9/f2zevBkTJkwwOzavv/469u/fj1tvvRUJCQnw9/dHSUkJnnvuOU3nvBpr167FokWLMHPmTCQlJSEwMBAGgwFPPvlk53pra2vR0dFh9VhrOX8ZhumZsJhgGOakRslLv3XrVtx4441mefgFBQUu2wbhea+qqjJ7vbKyUtP7tWyv8CyXlpaqrkeIoNLSUsTFxSkuI6IMcgNTGHty1PbvmDFj8NBDD5m9LsSMdJvLyspUt1dw+eWX44033kB5eTnWr1+P888/X5P3/7zzzsNbb72FzZs345prrkFqaipuvfXWzr8HBgbCw8MDV1xxhUUakEAuHtQ+57zzzkNTUxN2796NDz74AP/617+wcuVKeHh4qIpNacQGAGJiYvDUU0/BaDQiKysLa9aswcKFCxEVFWUREZEydepULFu2DL/++iuGDx+O9PT0TmED0LH87bff8Pjjj5sV+tfV1dn8brbYunUrpk6dilmzZnW+1traanasg4KC4OHhYfVYS89fUefBMMypAQ+tYxjGrXh7e+v2rDY3N1sUaltLU3KUgQMHws/PD9u2bTN7/eeff9b0fi3b279/f0RHR2Pjxo0wGo2K6xk/fjw8PDysfteoqCgA1EFKyh9//KFpW9W2988//7QQOhMmTEBqaioqKiqsru/CCy+Ev78/XnnlFRw/ftyqp15Kr169cNZZZ2Hz5s3Ytm0bWltbcdFFF3X+3d/fH6eddhqysrIwePDgzvx/6Y8e/Pz8cNZZZ+Hyyy9HeXl5p0EdFRVlsT/37NljlhIkxWAwIDExEffffz8Ay2Mhp1+/fhg+fDg2b96MzZs3IyAgoLNWASDjvqOjwyIdzRnnvNKx3rhxIzo6Ojp/9/Pzw6hRo7BlyxY0NzcrrmfUqFHw9/d36XXIMMzJCUcmGIZxKwMGDEBNTQ2+/fZbDB06tLPVqTUmTpyIFStWICQkBFFRUdixY4cuY1kvQUFBuO666/D5558jICCgs5vThg0bACh79/Vur8FgwJw5c/D888/jkUcewWWXXYbQ0FAcO3YMVVVVuPPOOxEbG4trr70Wq1atQn19Pc4++2x4eHggPT0d/fv3x5QpU9C7d2+MHj0a7733Hjo6OuDv748ff/wRRUVFmr/vxIkTsWDBAixduhRJSUnIysrCF198YZHmcuedd+KPP/7AnDlzMHPmTMTGxqKsrAx//vmnmWfd19cX06ZNw+rVq5GYmIjhw4dr3paLLroIKSkpWLp0KUaPHt0plgT//Oc/8dBDD+GJJ57ApZdeivDwcFRXV+Pw4cMwGAxWO3QBwOLFi1FZWYmxY8eid+/eKC0txddff43ExMTOtLYpU6Zg/fr1eOutt3D22WcjPz8fa9asQa9evTrXk5WVhUWLFiE5ORmxsbFob2/Hpk2b4OnpibFjx2r6nosWLcLRo0cxefLkzjbFAImqkSNH4sMPP+z8/aeffkJGRobW3ajKxIkTsXLlSsTFxSEhIQEHDhzAunXrLCJHs2fPxsMPP4w5c+bg+uuvR0REBAoLC5GVlYUHH3wQAQEBuPvuu/HWW2/hueeew4UXXoiAgABkZmbCx8cHV199tcPbyjDMyQmLCYZh3Mr06dNx6NAhfPzxx6irq+ucM2GNBx98EG+//TY+/PBDtLW1YezYsZg/fz5uuukml23n7bffjo6ODmzYsAFff/01hg8fjn/961944IEHzIxKR7b3nHPOwfz587Fs2TLMnz8fAKXOiO5JABl1sbGx+Pbbb/Hjjz/Cz88PAwcOxIQJEzqXefrpp7Fw4UL873//g4+PDy655BKMHz8e//d//6fpu06fPh1lZWXYsGEDvvzySwwcOBDPP/88li5darZcdHQ03n33XSxevBgfffQRGhsb0adPH8Wi+fPOOw+rV6/GZZddpmkbBBMnTkRoaCjKyso6W+FKGTJkCN5//318+umnePvtt1FfX4+QkBAMGTIEV1xxhc31Dx8+HF9//TXeeecd1NbWIjQ0FBMmTDBL+xk7dizmzp2LlStX4vvvv8fgwYPx9NNP4/nnn+9cJjw8HJGRkVi1ahVKS0vh4+ODhIQEzJs3T1OEZMqUKXj33XdRUVFhFn0RPPPMM3jrrbewYMECeHp64swzz8Rzzz2H++67z+a6rXHbbbehoaEBX3zxBRobGzF8+HC8/vrrZmIQAIYNG4a3334bS5YswVtvvYXW1lZERUXhkksu6VzmqquuQnh4OL766iu88sor8PLyQlxcHG677TaHtpFhmJMbQ0pKinI83YX88ssv+Oabb3D48GHU19djy5YtVieuNjY24q233sIvv/wCLy8vTJ06Fffdd5/Ze1JTU/HBBx+gsLAQ8fHxePjhh0/JCbYMw3QdKSkpeOmll/DVV19ZeMwZcz766CN88803WL16Nfz9/d29OQzDMIyTcEtkorm5GePGjcP48ePx8ccf21z+jTfeQHp6OubPn4+mpibMmzevc0gSQO3rnn/+edx6660499xz8d133+HJJ5/EsmXLHGqXxzAMIzh48CD++OMPDB8+HD4+Pjh8+DCWL1+O8847j4WEFY4ePYq8vDysXbsWl19+OQsJhmGYHoZbxIQI4e7Zs8fmsrW1tdiyZQtef/31zkjDrFmz8MEHH+D222+Hp6cn1q1bhyFDhnR2+XjggQewfft2bN682Sw9gGEYxl78/f2xd+9erF27Fo2NjejduzcuueQSmzn5pzpPPfUUqqqqMHHiRNx+++3u3pxTCuncDSU8PDxs1vswDMPY4qSvmTh8+DAA6vstGDduHGpqalBQUIC4uDikp6ebTeI0GAwYO3YsDh061NWbyzBMDyUhIQFvvvmmuzej22Gr/oVxHbamby9cuNDs2cowDGMPJ72YqKysRGBgILy8TJsqOmxUVVUhLi4OVVVVFsOkQkJCOoWInI6ODpSXl8Pf35+9MgzDMEyPZOHChVb/Hhsbi/r6+i7aGoZh3I3RaOyMrGuZwaOVk15MKPVblwsAtZ7sapSXl+P66693aLsYhmEYhmEYpruxcuVKp06rP+nFRHh4OOrq6tDW1tYZnRBTZ0U0IiwszGIybXV1NcLCwhTXKQoA8/LyOifKMkx34qmnnsK8efPcvRkMYxd8/jLdGT5/me5KTU0N+vfv7/RGGCe9mBg8eDAAYO/evRg/fjwAIC0tDcHBwYiNjQVA/a/T0tLM3peWlqZafC0iG8HBwSwmmG6Jj48Pn7tMt4XPX6Y7w+cv091xdoq/8xKmdFBTU4PMzEwUFBQAADIzM5GZmYnGxkaUlpbitttu6yyeDg4OxgUXXIC3334bhw4dQlpaGhYvXowrrriic87EZZddhoyMDCxfvhzHjh3DokWL0NDQoDj4h2EYhmEYhmEY5+CWyMT27dvx+uuvd/4uJnguXLgQ0dHRyMvLQ3Nzc+ff586dizfffBOPPfYYPD09MXXqVLMWg3FxcXjxxRc7p6DGx8fjtdde4xkTTI9l2rRp7t4EhrEbPn+Z7gyfvwxjjlsmYLub+vp6zJgxA9XV1RyqZBiGYRiGYXo8NTU1CAkJwfr169GrVy+nrdctaU4MwzAMwzAMw3R/WEwwDMMwDMMwDGMXLCYYhmEYhmEYhrELFhMMwzAMwzAMw9gFiwmGYRiGYRiGYeyCxQTDMAzDMAzDMHbBYoJhGIZhGIZhGLtgMcEwDMMwDMMwjF2wmGAYhmEYhmEYxi5YTDAMwzAMwzAMYxcsJhiGYRiGYRiGsQsWEwzDMAzDMAzD2AWLCYZhGIZhGIZh7ILFBMMwDMMwDMMwdsFigmEYhmEYhmEYu2AxwTAMwzAMwzCMXbCYYBiGYRiGYRjGLlhMMAzDMAzDMAxjFywmGIZhGIZhGIaxCxYTDMMwDMMwDMPYBYsJhmEYhmEYhmHsgsUEwzAMwzAMwzB2wWKCYRiGYRiGYRi7YDHBMAzDMAzDMIxdsJhgGIZhGIZhGMYuWEwwDMMwDON6tm8HDhxw91YwDONkWEwwDMMwDON6XnkF+OQTd28FwzBOhsUEwzAMwzCuZ/9+ID/f3VuhnxUrgOPH3b0VjLvJyAA2b3b3Vujn55/p2nMhLCYYhmEYhnEtlZVAXh79dCfa24F77wW++cbdW8K4m/feA/77X3dvhX7+7/+Ad9916UewmGAYhmEYxrWIWonuFplISwOqq13u2WW6AampQEGBu7eCqKsDPv4YMBptL5udDeza5dLN8XLp2hmGYRiGYfbtA0aOBA4dAtraAK9uYn6kpAA+PrT9zKlLUxMJy4AAd28JsXEjcPfdQGQkcPnl6ssZjUBODl1zra0u2xyOTDAMwzAM41r27QOmTqX/FxW5d1v0sHUrcOONFJnQ4gVmTg5SUoDff3fe+nbtIlFZXQ3U1ztvvfaSmgr06wc8/DDQ2Ki+XFkZba+nJ3DwoMs2h8UEwzAMwzCuZf9+YNw4IDq6+9RNtLYCv/4KzJkD1NZ2vxStU5lXXgH+8x/nrS81FbjgAsDX9+RIdUpNpe/Xuzcwf776cjk5QEQEMGGCS1OdWEwwDMMwDOM6OjpITCQlkTe1uxjlf/0F+PmRITZkiOtSnRoagNdeO3UjH19+CRw+7Lz1NTdTVGLrVqotcAapqcBZZwExMc4VE8ePA2+/re/YNzcDu3fT9ixaBLz+OokGJbKzgfh4YPx4eo+LYDHBMAzDMIzrOHaMcs6HDQP69+8+kYmUFOD88wEPD+C001xXhP3LL8C//w2kp7tm/SczBw8Ct90GvPmm89a5YwcQEgIMGAD89JPj6zMaSUyceSYQG+tcMbF+PfDgg8CLL2p/z+7dQHAwkJgInHEGpeE9+qjysjk5QEICRQU5MsEwDMMwjFsoKQGeesp+I2rfPhISPj7ujUy8+ipQUaF9+ZQUIDmZ/p+UZDsyUVpKbTj1kppK/65fr/+93RmjkQzp0aPpuzsrMiOO24wZztmnubl0DUyYQGKisNDxdQoOHwamTCEx9dFH2t4jhI3BQL+/+iqJpq1bLZeVRib27qVCbBfAYoJhGIZhGHU++QR4/31g6FDyoOotQBUpToD7IhM7dpAgSknRtrxIlRFiQktkYvNm4PHHKRKjh9RU2j+nmphYs4b26fffU6GwaB/sKFIx8f33lGbnCKmpJHh69XJ+ZOLwYdrO774D5s7Vdg6kpgKTJpl+j4wEbr4Z2LDBclkRmRgyhCJszkwnk8BigmEYhmGcRV0dFX+2t7t7S5yD0QgsXw689RYZyz/8QIbJ3r3a17FvHxnjgH2RiV9/Bb79Vt975CxcSMaU1lSlHTsolWT4cPo9KYnSkFpa1N8j1v3FF9q3q6ODPuvll0m86ImcuJsjR2iQmz3U1wOPPEJD4KKigIsuco6YamwkYzs5GTjnHPqctDT15Y1G4LnnrHdEkhrvzq6ZOHyYrqdzzwWWLgVuugn4+2/r75GLCYDOT6VzOzubxISnJzBmDLBnj5M23BwWEwzDMAzjLF55BXjmGeDoUXdviXPYt48MkiuvJANm+3Zg7Fhg0yZ963AkMvHKK8Att9jfUvbYMWDtWuC++7QXUYt6CZFKMmAA4O9vva5h3z5KP1m+XHvKzsGDJDwvvZTmcOjZr+5m2TKq9bBHOL/6KgnLW2+l352VkpSaSt2LEhMprW7aNOvrLSoiIffLL9bXKYx3Z0Ym2tuBzEwSEwBw7bXAZZcBX32l/p68PNrm0083f10pDa+jgyIT8fH0+/jxLCYYhmEYF9PYCDz99KnbVeazz6x7MW1x+DDlPkdF9ZwhZ8uXA1dcAQQG0u8GAzBwINUHaKGxkTzYQkz060fGkNYBWnV1ZNgnJQFPPKF/+wHqlnPZZfQ91CITr79O0Q9x7kvrJQD63qedZv247t9PQjI7W/vxT00lw9DLy3kGdVeRkkJzF/ReM7m5wIIF1InI44QZeumlFKEpK3N8m5KTTSLQ1j4VaT9q6W+NjfT9XCEmcnPJ4BfGPkDtZ62l4omUOHE9CkaNAoqLza/L48cpXW/AAPqdxQTDMAzjctLTgXnzgJoad2+Je1i4kHKX7cFoBB56CJg1i7yhrur805V0dFDbzltuMX89IoIKUrVw8CClC8XG0u99+5KhpzXKsGULGVtffUXRhd9+07z5AOhc/ugjSqlJSgKysizbhTY1Ac8+C9x5Jxmiv/9ORtuUKebLWRMTlZXkNZ40iaI4y5dr2z6p13vGDJps7KIiWafS0EDG/6hR2utQBL/8Qik348aZXouJodc2bnRsu+Qi8JJLSAyonW+HD1MKkNp32LULCA+nVCGAzuOiIsfrMMRnDxwIeHubXpsyBfjzT/W6JKUUJ4Cusfh48/tOdjZdb35+9Pu4cS5zcrCYYBiGYQjhcTsZhjJ1Ne3tJKays+17/7p1NJfgpZe0df5xhNRUSjHRwn/+o96D3ha//EKG9kUXmb8eGaldTIjia+Ep9vamwXVa6ybWrycju39/iprdf7++tJrFi6nuYdIkihj16WOZk757NxAaSvvprLPIOyxSZaSo5aUDVDwcG0uG5y23UN2EdDszMyklSB71E515AGDiRDJsRXcnW9TXk4DVGiXSQkcHbaetVLTffydDddYs/WJi3z4qaJYjCqbVqKmhImU1Q76+ngxxqZiIiKD2qUrFyYCpAHrXLmUnijDexfkbE0Nizxn7/MgRU4qTICGBPkNtereamAAsxa4ovhYMG+ayqDOLCYZhGIY4lcWEmIVgj+Hd2Ag8/DDlgYeH206HcYSmJsoz19KXv6mJxM0HH9j3WcuXAzfcYO45BUhMaDWmpMXXAq11Ex0dZFzOmEG/P/IIGYxav09bG+2nRx4hY1CkKskFgTDQgoMpMpeRAaxebTIgBdZEorQu5KKLqFBb5OEfP07RqtdfNze8Kyros4SY8PSkdB+tqU7z5lGXrRkz9HfYUuPTT2mA3sMPW19ORACmTKECea1pa4B5dy8pM2ZQgb/aurZtA954Q11w/P47GeJSA1qsV22fitasCQn0PeTIjXc/P5o67Yx7pCi+lpOcrCzQmppI+KqJCbnYFW1hBV5eFElyASwmGIZhGEL0T3dmH3V3c+yYtoFQhw7Rv/ZEJl56iTzes2bR70lJVIDtrOm7Uv73P1rv4cO2vYy7d5Nh9sUX+tMympqAVauAmTMt/6YnzUlqZAu0dnTavZvSac45h3739aWuUk8/rU3MrF9P3/vqq02vKQkCucE4YIDJwJcyahRdG+Xlln/bv98kmry8SIQtX07HasYM8o7/+99UKyDYsYPSXCIjTa9prZvIzKS0vG3bqDD8xhsdT4+qqgL+9S/gnXdobsHmzerLCjFx2ml0XPQMRFMSmACl4fj7W/fK9+plvg+lbN1qHpUQzJhB36W52fJvwqBXMuDb26nhgNx419LR6fHHKcVK/Lzyivpny1ETE7t30zC+QYOUP9NWZAKg5gkugMUEwzAMQ/TEyMSPP1LEwJbn9OBBKoTNz9fnZV28mNpjLlliKiaNiiLvpa0Wj3rJzaXvsnw5UFtLHm9rpKYCF19MXms1A02NDRvoOygZ1SLNSUvKhJIXWmtkYv162n5pZOSSS4DJk0lQ2CIlhYquvbxMr8kjE2K6sZq3V0poKBAXp5zqJBdNM2dSdOO66yjisWQJ8MADVAMiOkIpfe7UqWRk2hK1Dz8M3H47vf+bb2j5f/7TsTSWF16g9KPZs0kgP/igcivc2lpg504yej08gPPO057qVF5OgkxJTHh4WI/MpKZSG9c//yTDWo68XkIwahSdQ/I5Fm1tVEOjJibWr6dokfwasFWE3dREEZQLLyQhe/bZFJWSiz1rYuKvv2g/S5GnXMlJSqJ7jkivE21hpYwZo77dDsBigmEYhiEKCsjz1ZPExL595JEUkQc1Dh2ih7+Hh/bWpRs2kIH4zTfU1lNgMFjPr7eXRx+l9pEXXEAGua0BVKmpZHhff732gmDBV1/RICwlwyUykvap3NiRk55OokO6bwDtkQlRLyFn4ULg88/JoLXG7t3UwUaKiEwIozsvj0TZhAm2twdQTpPq6CBDVWogn3EGRasKC4GvvybvfXQ0zRF44w1aRklMhITQMbNWO7B+Pb33P/+h30NDKT1o40b7JnAD9J0++IAiPwYDCRMvL/pdzq+/UvQmLo5+T05Wnr6s9jlxcfQ9lVCLzLS10fGePh244w46B6Tk5lJ0RElMGAwU9ZALkJwc+tuAAdQGeM8e8zkfCxaQoPLxMX+fLTFx4AAQFETpdXffTREpT0/zz29ups8fPNjy/f37U3qSPO3q55+ti97Bg+lczMoyfT9pmhOgXKviBFhMMAzDMERBAXnne5KYEIafrTSMQ4fIGIyL05bqtHMnpbIsWUKGiBxnF2Fv2UJRltdeo9+HDLEuJqQe95kzgZUrrQ9ck/Pbb+QlVyIkhDy91lKNSkuByy8ngyooyPxvWiIThYVk3F1yieXfBg4EHnuMirHV0rfa26mLj7RjEACMGEGdl0R3H+l0Yy0oHddjx6huZtgw02sGAxXl//STueE8dy61ID5+nNKclIzDqVPpeCvR1ERF1/PmUeRI0K8fHeMXXtBf92M0kii+/37TkD4vL2rd+uKLlmmP8giA6ICllEYkRyntTcqFF9L1d+SI5fu8vWn7HnqIvqu4T1VW0nlyzz10bikxfrzlPeDwYSqy9/SkYvIhQ0x1Lrt20c8991iuKzbWeirorl30eUKIe3paRm+ysiilKyZGeR3ySMn27STY5J3VpHh50fm9fz+d/7m5lpEJ6TnqRFhMMAzDMERPExNGIxkh555rXUwYjZTmNHw4PXxtGWOVleQhffll8voroeTBtpc//gDuvZcMu+hoes2WmBAe99NPpw5FISHa224WFVHPerWUCIPBet1EQwPNdRg7Fpg/3/LvWiITGzZQd6M+fZT//uSTtI2ffqr898OHSWgI41gQEEAeXCEItKY4CZKSLD3c+/eTkSb3YA8fbrn9SUmU9vLgg7R9Suk+ycnkhVbqWvX22xSJuOsuy79NmkQC99FHtX8fgFL1Dh+m9rhSzjuPIgVz5piLNrmYGDGCBOOff5pe++03y/UB5rUlSgQFkTiXR2ZSUyna4+FB5/7FF5PYaWqiVLbERNo3aqiJCWmakdSAX7iQWgWHhVmuy1ZkYtcuSxErFweHD9N56KFihkuXb28noffUU+piSSDEbmEhvU++vLyZgpNgMcEwDMPQQ7miggy4nlKAXVBAQ7VuvdW6mCgqopSdoUMpLcBWZOKnnyjVx1rHG3k6jT3k5FBazIUX0ne4/37T32yJCTHcqlcvMv5vvll7qtPu3bR+eURBilp72LY2Kgb28yNDX8lY6t/f9uA6tRQnQUAAGXz/+hcVDit9h9GjzeslBNJCVb1iYupUSt+SCgpb3nY5jzxCnnUxrE7OuHFkCO7da/m3L7+k6Ianp/K6X32Vzs8ff9S2LZs3k7D57DOq7ZDz5pv0/f71L/q9qooiPvKBfuefbzJ+DxwgMfnqq+ZpQ4C2faWU6iQ/To88QmlZN91E59GXX6rvE8A0Y0F6zqmJifx8aj7w0EPK69IiJuTpdcnJJLDE56vVS0iXT0sjx8VHH9H9SYtIFOd2djaJdheJBzksJhiGYVzN779TF56TmcJCevCMGUMeX2cNzvrpJ+Caa2wXC7uC/fvJ+3f22WSYqX2ngwcpIuHvry0ykZJiOdBMjjydRi9ff03r8POj9qEvvGBueGoRE1Lja+ZMSrvRMpBw1y7c1/6O9TT4iAjlNKeXXqJOVt98YxqWJUdpcN2yZbS94mfjRutiAgCuuorO13nzFL+DhXdYIOpZmprMpxtrITycPNbSnH21VqdqTJtGUYuzzlL+u5cX1U3IC4ILCshQVEr9EkRFUQRLrXj6BOedBxRv3k/X5vvvk2BVIjKS6jGWLiVh8csvFAWQp+dIDfFLLiGhfdpp9F6BUm2JEtOn0+dIz1X5+Tx5Ml2rBw/SeR0QYH2dgwbR+ShtiiA36M8/n7bv+efpO8jnjAisdXNqaaHzQS4mRo2i+4uo87ElJvr2pXvX2rXUbODNN6nuxhbi3Faql3AhLCYYhmFczQ8/UHvOk5mCAnqAxcTQQ99R4z8jg3Lmr76aHtDvvOOc7dSD8IIOHUoectFFR86hQ6Z0GC2RCbWuMVLk6TR66OgAnnmGDNYlS0zTo6UMGULtQdUGuMmNrxEjKBXn669tf/7u3djZNMqi+Y0ZapGJX36heobQUPX3ennRuSbqJpqayOsqaiweeQT49lvbRqfBQMuuWGEZAVLyDguE93bXLkpjkeeV20Kes6/W6lQNDw8ygK15mpW6C33/PR1Taa2EEnPmkGNAZRZJezsdprybHidD9dZbra9v8GD67GeeIaGidO4nJ9M5d8kllIL03HOWEYajR0nQWzOiAaqJGTzYFF0pKaFrcuJE0zIGAxnav/6qngonxcOD0u6kEUq5QR8RQc0CFi+m80qN2FhyFDQ2Wv7twAG69gcOtPx8afTGlpgAaJ8+8AA5Qy691PqygtNOo3qMAwf0n9cOwGKCYRjGHoxGSh3R4nnOyqJiuJOZggISEj4+ZCg6UjfxxhvkMY6LI4N30SJqn6r08HUlIj/b05O2Ry3V6dAhMrYB25GJ4mISJeedZ/vz7a2b2LSJ0kPuuEN9mfh4OgePHbP8m5rH/ZZbqIbBlsDZtQs1xiDFcQqdqImJ3FzqjmMLad3EF1+QIffkk9RK9brryCBVa4Ep5fzzgbIy87afHR30/dXERFISHfNffrHealONwYPJaBY5+4cP64tMAOQpDw9X/3tyMm2fNJpmK/VL4OVF9QMvvUT7RkZNFdU/1J47HXjiCW3bO3Eidfjau1c5KjdkCBn1AwbQtW4w0LZu3Gj6Dvv2kWjXknojFSKpqSSE5fULcXHmMzpsIa2baGggMSs36KdMoeXOPVd9PX360HdQukeKiJjSOSUViFrExAUXkPKTd66yRlQUXUvr17OYYBiGOekpLaU83UWLbC+blUUP9YYG12+XvRQUmDzgWoYyqbFsGaUJ/PIL7ZuICEqhiI6mdp5diTQ/W6k1pEAUXwP0AC4sJCNRiW3bKBffmiEosLej08KF5F22ltbg5UUGqVKq065dFBmQe0f/+U/ycJ5xBhXwKgnhkhIgPx81LX5KdqgJJTHR3k4CQYuYEB2djEZqwSmmVOvFz48mTks94JmZlG4iBKKchAQSzZ9+qi/FScrcuZQetHMn1RooRY8cQdR7iHO2sZE6PGkREwCJrNNOowiKjOofdwAAam+ZrW+fT59O4viaayz/ZjBQ0fjq1aZ0vNNPJ6N7+3b6XU862IwZVITf3q6/rkWN8eNN+zMzk2qCoqLMl3nuOYreWdsvHh50j1SqLdu9Wz29TnS9Ki2lyK8tMXH11bTP1IbUKSGmvB86xGlODMP0QB58kIy2k53PPqMwty2yssjj/f77NBTM1rLAyR2dKCw0GUS2Wh8CtJ/uvttcdGzeDNx3HxkUp59uel2koyxcqH8Ss720tFAEQaSfKHVzEUgjE1FRZMSrHSst9RICe8TE/v1UqHnffbaXVaubUBtu5e9PkYkDB6gwfcgQ8w48ABlDgwejptbDuphQqpkQtTb9+tnedhGZ2LyZRInSpG2tyNNpdu+mfa/mAffwoBz2jAz7jdTJk0msPf44fZY9Qsga8naiKSkk4NQEkhIzZyoW3Vev3AQAqG1UKP62xfDh6oXOoi5B4OFBAkQcGz2F6pMmmWZL2CEmCgsVMjXHjTPVTonIgPy49e5tmp9hDbUibGvpdcOGkchftow+R8EhkZEh8Tl5eCjPobCF2MccmWAYpkdhNFLut+jhfTKzbBkZyrbIyqLJqHFx1pevrqapr5GRJ7eYkEYmbHUrASgq8+uvVI/wwgtkAItizosuslz+5pspdWfTJqdvuiIZGSQKhHdu/HhKfZHXGJSXkzEr+q97eFivm9BSLyEQHkI9E7UXLqQc9ogI28sOHmxdTKgxaBB1q7njDjpeUnbtQsvo09HUBP1pTrm5VAshb5GqhBATCxZQxEStWFsLl15KMxuE+rFWfC1ISiIPutZhdXKEQN6xQ1+9hB6kaTHff0+iSY9ouf56Msal53JTE6o3k4C0NXPQKcyYYWrzqqe2xMuLUsnWrqXvoFNMPPGEQs+LIUNovQcPakszsobSPbK1lb6jmpgQXa8++ED1s2fOBL77zv7NAmDaxxyZYBimR3H8OFBXZ3tir6soLqb+67aMuvZ2Mg527rTdzSgri4wyWx73rCzK9U1KUhYTf/1FLT8daSEK0NPzt9/sf78eMWE0ksG2ZIlpmNq551ov5vT1pdSdBQu0bU9VFXDllTS4zB727SPvs2hNOmwYbbf8HDx0iL6vdLCYmpgoKKAiUmv51FJEOk1Ghrbli4upfsBay1kpQ4ZYDveSDquzxW23URRJWsuyaxdqR54JQDHd3oSSmDh2TJtXF6A0p99+o9SY2bO1vUeNmBiqiRFzNKx5hwVjxlBBrq0uQNa4/no6d9TmcTiKaCfa0qK9XkJKRAS1spU2f9iwAdVBFDnqEjFx0UV0jh44QPdCPbUlM2aQ2PXxsZwXYoPdu8mPY4aHBx2r3bsdFxNKqaAHD9K2qnWBAuiYWvns3FyF7dbLmDF0v3V26p0VWEwwDON6hAEnN3y6ikWLKHf499+tL3fwIIkCb2/b6SlCTNxwAwkl+ZAl+XJxccrFsj/+SJ2Ovv1W23dRoqGBCi4difyIAmzAtpgoKCC39ejRFJ35/XdKH7BVzHnffbSsraLk5mYSEtu3k6FpT2qUPD/by4u2V57qJO3kJFArwk5JIY+3VHhYQ6TTaC3CfvddSqHSajgppTnl5pKRr8XjPmEC1bLIUoRqEsmrbzUyERFBakN6bHJztYuJfv3IyTBzpmXeuj2IVCejkYxFW2Lizju1dbayhqgHsDaV2BFGjqTUtMWLaV8rTVq3hUh1Es6K5ctRfeY0AF0kJoKDKSXs9dcprUcMXdTCtGmUQiqG1WmkoYH0e12dwh9FuqMrIhO7dpFAtbatIqqp8NlNTZQ5qLjdehg7lsSbtbkbTobFBMMwrufwYfKUuCMy0dBA3UVGjrQchCQnNZW6lpxxBv3fGkIk+PhQ+z61jhtiuQEDlCMThw9TqtDcufZ3O0pJoSeRrfkIAH2vq682j4QYjfpqJnbtIoNXeHUNBm154xERFLl47TX1ZTo6yGPe2Eg94QsLtaWdyVFKqRg3zlJMSIuvBWqRCT0pToLRo22LWICKkd95x3pLSjlDhpBAlRaL//wzfWavXrbfbzCY59WXlwPHjqFmAO23sjIrAbPISIreSQfGae3kBNA+NhjovHcGM2ZQC2aRdD5ypPXl/f211XbYIi7OsRQta4h2os8/Tx5+ez7n8svpuOzZQ8fq++9RnXQOgC4SEwAdmy+/1F9bEh4OnHMOtUbVwb59dBtxuZiQ3yO1pNclJtI5o+AwEKtzWEwYDNajIy6AxQTDMK7n8GFqc3f0qL78cWfw2Wd0837pJW1iQgzN0iomAODeeyk9Ki1NfTm1yMThw8Czz5Kh/d//avtOctavpwevrfkIGRn0YP/mGzoWgooKigZo7eakxfOrxtNP0+eLDi9yHnuM1r9+Pe2TBQvUpxxbQ6lzjLSbi0BafC1Qi0xs3apfTMyeTZ5la0K6spLyw6+9lq4TrcTEkFEsCvyNRhIkKqlmGRlUf23GzJmUHlRRQcbQwIGoQTACA0mjqDYg69WLPlua6qQnzSkmhrbbltGvlXHjSNy+9RaJSC0DvroDU6bQPtab4iTo1YuG+y1fDqxZA5x2Gqp9KRKkRUykpTlhRMyMGZRCak9tyYoV2iY/SxC3YUWjfNw4Si0tL7evuFmgFJnQcl80GCh17bLLLP4kOiU7LCbcAIsJhjlV+PhjCjW7AyEmPD21ec+lzJ1LA56UeOQR9b8B5J5auJCWu+giMratpVppFRN1dZSiIcREeDhw++3KT10tkYlhwygV67//tS0I5BiNZHjfdZf1fVtURL37772Xoi/S71dQQKk7wpsdG0vTZ9Wealo8cGrExQFPPUX1E/Ji6AULqH3sDz+YCpCvvpqMkBde0P4ZlZX0ZJYbL6IIW5qaozUykZND6zznHO3bAVD+8qxZNOhMyc3f1EQpXQMH0vmjx3NrMJinOqWmUgerWbMUF9++XSHIk5hI27h6dWdby5oaOkweHlZSnQwGy45OeiITgHO7zYjOQZ98Yr/QPRkR902tQ8uUuOUWigwsWwbMnNmZk69FTPz8s32BQTMSE+kaGztW/3ujorRF2STs2UO3XMXvN2wYpTxGRWlPV1RCRCbENd3WRqmeWs69/v3Np9mfQIiJLosYOREWEwxzqrBlCzBvHhmJXY0wmNX64ltj7VoyJOWGWE4OTXi1ViewYQPdmW+4gXqKn3++em1DRQVt25lnUppTdrbyUC6AvPpBQeaTVy+/nGYQyJFGJvLzzQ3o8nKTh2ziRODGG/WluQAU06+sJDFz7JjyROTaWjK0Jk8GXnnFUixJi68BEke+vurRCS0FrtZ49FHapg8/NL321VeUzrFhg3lfdYOBvM0ffKC99mD/fkphkQ+5GjGCilmFoKyro/QipchEaal5y9+UFGp3Gxio/XsKXnqJvKFy4dveTlGE5mb6/goGhk2kYmLBAmrXGxysuGh5uYqhIlKdThzXmhradWFhOouw9dRMuIIZM+j42it0T0aGDKF7kahnsocLLiBj99dfgRtvRHU13bq0GK2i2ZnDpKQ41v5XB2lp1CNB0RciBlg6kuIE0PFoaTFdIIcOkaB1YL0cmWAY5uQnM5O8oLZmKBiN9PApLnbO57a302cPGaLeF1+N+noykDMy6EEo5e23yfjKzFR//4IF1ClJtKqU96OX8scf5EHr04d6gQ8frh6dEAJB6kU+4wwSGdInb3MzGauDBpFx29pqvl+PHKGCRGH8vfoqPXTVPve++8jYlrJ+PXVsSUykfa00iGzuXOpr/tFHtM1KYkJqrBgM6kXYRUUUlXGkg42fHwnBp5+mh3FKCvCPf1CRvFLh8IgRFMmYNIlaj/btS/tz507l9av1s/f2Ju+oWE9CAhnE8jasffqQN1Qa6fnuO+3zJeSEh9OxffhhU13MkSMUkdi7l46hvV2FxDV19CiJlQceUF20rEzFgLzhBgpbbN3aKSaCg2k3aG4PW1NDqWjuFBMXXkipVxMnum8bXEH//o6938uLohNTpwLR0aiqostHi5goK3OSmIiK0jb52kHa2siXoComALpX65nXoURAAN03Royge8nkyeRgcaDoOT+fbo0sJhimG9HeTo6FU4asLPIIv/mm9ban1dVkVKjltOvl2DESKPHx+sVEejq5R+UtRWtqyDB+9FFTvrictDQSCPfea3pt+nSKZChFZ1JTKSohOPNM22JCSlgYRV/++MP0Wk4OCZmYGPL09+1rnuokLwKMjKQH/tatlp/Z0gIsXUqCQprILlpG+viQAFBKk9q+nQSFEFWTJpHBLTzv8sgEoF6EvWsXFYzb46GXMn06FVbefjvldC9aRHUDarz+OkV+vv0W+PZbNJ1+LoxfrVBe1lo/+7VrKY3q228pSrVjh+UyBgOdr0JM/Pwz8NNPNA/BXmbNAvr0QeOTL1L0KSmJzovffjOPcOlFXFNvvUX70UqakWpkIiqKDPGKis40p+Bg0p82B9cJSzM3l6J1oaGaN72khG4P4kcpqKaLwEC6NntSZMJZvPYazRYB3eL79dMWpC4vp9uNrbmccoxGMo7Fsc3Ndbz7tRbS08meHzvWilH+n/8oFA/ZxsJm2LGD7iHffkvzcxSmjeshP59urfaIieZmJ1w/DsBigjll+eQT1dTinkdFBXkNH3uM8sW/+UZ9WeGNVpoWbDRSqtBTT2lP7Dx8mAxvLy/9YkIUxz7wABWJivSUTz6hws2ZM8kjq/SU+uAD6grUu7fptYEDKaXoxx8tl5f35rdWN6EkJpTek5VFnylaBcbFWRcT1j53zx7ylsfGkpcboFScnTtN+dQJCZZiorWV9pu0LqB/f4qICM++tJOTQK0I25HiazlvvEFG+mOPUatOa3h6UtRi4kRg4kScs/9d/LxKxWW6a5d6P/vo6M51YOJE9cFOYl+2tVF067nnHEs18fBA/fx3EfXWU8jfdZz2/QcfkIB0hCFDqA3kJ5/YTJErK6PToblZ4Y+33UbndO/e+iITomZCpDhprPkoKyNtPXAg/cTHWw2qaKdvXyespAfi7d3pABBiQmtkArAcdm6LTZvoNiOO74ABVLLhatLSqJlZSIgVo7xXLxK+OvngA5rz2ElCgvm9xMEWx/n55I+yR0zccAP5Cd0FiwnmlKWgQH8tcLclK4ssg/BwKgS1NjhMGJDyrjcA7bDffydvamIi5bzbGu4mNZjtERPDh9OT77rrTFGVN98kw2ngQHKZHT9u+d60NOW+7EqpTmJYnVxM7Nyp3H1Kj5iQLifv6KQmJv74w1IgicjJokU02jUzkwTWuHGm3u1Sb7ogM5MMCbnHWrqtapEJJTHhaL2ElEGD6DOeflr3W483heBgYahlQX1mJhnXF1/s2LaJIux336Vz7sEHHVsfgKLYCahFMLb+YznNn3AGgweTo2D0aJvpPcIwVDQib7yx83zQHJmQpjnp6eQEyv4LC6NLr72dTqtly7pnikd3o7qaDH09YkJvqtO+fdScTBzfZ54BNm/Wv616SUujDMzAQCvC2U4OHnTtqCQhJvQWYLe2Uknkpk2u2S4tsJhgTlmqq52UC9odkBq1//gHGVtqXvfCQrIkdu1SNmjHjqW0j/feo+5DtsI7cjGRn689Zn7woCm3de5cmrj8ySf0+1VXUX60aDEpxWhUbvkJkJjYsME8JnzwIL1HauANH05pQUrD66yJCen07Kws837f8o5OSmJi7Fh6msifWiJyMn48eZHnzrWciqsUmTh4kJ5Q8kFK0jQuec0EYF1MODONpHdvfR2MTlBX74HsfudaFtR/8QUJifBwx7YrIQH480+KSLz1lilFzAFEOYtSFpvdhIfTsdPQPlNEGRSNFdGdCTCLTOhKc9LRyamoyHx+2dix5BtYvVrzKhg70ROZEOeM3mdlRgal7AiSk+m8d3WqU1oanUsiC9OZ4jQ721Qk7WxEOd3w4fq3eedOSr8Sw9LdAYsJ5pSlulp/6LbbIjV+Q0KojajakLWCAsqfLi+3vHMKg9ZgoJady5eTYW7tCSE1mCMjyUqxVjQtRTqdePx46qZz//0UXRGdbwYNshQTBQUkWJQ6a0yaRMa+tHhXDKuTdtPx8FAeXtfaSl5YJTExfDitQwgQpciEEBMdHSQY5Nvo60vfVf650jSsV16hOoi1a83FhFJkQk1UiciE0ag9MnH8OIlNe1o8Opn6eiAnbIx5lMlopHPSGV1j4uPp6XzhhdRW2AkUF9PpkZLiZKPqwAES1zawGpmQYFeak87IRHGxeUaSwUB+CVv9IRjHEWKipcW68Wk00jkTF2efmBg2zPT7pEm0Lld69o1GygYdO5b8TAaDc8VETg7dAl1hsBcVmWbN6d3mlBRqJtirF/k/3AGLCeaUpbqasgNOiSJsuVH74IPA118rWwoFBXRHGzHCMtVJXlcgqtysPSGkYkLeF19w2WWWdQwtLSQ6pIbw44+TGPrHP0yvKYmJgwfJzak0MdbLiz7v4YdN043k30ugVL+Qm0tCQ6nDilyAyPf7gAGmNKfCQuquNXCg7c8tLCRhJ9JYevemuol+/cwNe7XIhMK0VYwbRxfBoUNkEGopwN69m9JqVFqPdhUtLaTpstvizAvqd+2ip7LCQCjdDBtGHVv+9z9db+vooL78SmKhqIgy74qK9I8TsYq8Ba7KdlVU0Clqq/DWrjQnByMTAGnAHTv0d4/u7rS2moaQu5r2drpli+Hf1oRlfT1dayNG6He8paebRyb8/em2lpKif5u1kpND323UKHrUBAY6T0wYjbR+o1G5YZ6j5OeTuA4NtU9MTJlC0R+l/fvrr3Q8XAmLCeaURQzuOSWiE3KjNj6e3I5KIkB4qceNMy/CbmigNpZSo9vHR9mLLmhsJCND6n2Xi4kjR8i7/O235u89coTEgNRonz6dkq2lxqySmJBGNJR4+2268559NhX+/vyzdjGRlUX7T60FoHhPRwcVh6tFJg4fpvUoTeo96yzzz01NpSektGjwnntIKEhThBIS6KkkrWNR2xd+fnSMv/6avou8eDA2lp6a0gFvzk5xshORJZdT5GteUP/558A115Dl4igjRtD31zOEDXR4b79d2QgvLibteMYZTk510kB1NRmSsbFOjExERNAC7e26Z0zIIxMAfd7ll1PTslOJ/ftp3Ij0UnMVQkj27Uu3DmvnQlkZ3RoGD9YXmRDjc6RiAiCD15XnfVoaXbbilhoU5LwBcCUl9Ajs3ds1qU75+STwAgNtR4ykNDdTGWNysrKYMBopEcHhwYM2YDHBnLIIMXFK1E0o5fgrpcQAJjExfry5mPjrLzIe5AaDra5HAQHmVoNcTHzxBeV9y++CItdfnk8vNxTVxIS1PuLBweTZP3iQogOFheZtYQVnnEFG0tGj5t9JKcVJIPZHQQG5HKXdguLiKBxWU6NcLyFdx4EDpiehWuREvi9iY2l/5eXR7+3t5JJS2xeTJgFr1pCLWC6O+vYlUSK9QJzZyckBhOeuvByovehqqptoa6PBb84cjGVHBEY4J5QMjqIi2q1qHkRXUlZGdfgxMdrFhKbWsEYjKYOCAl1iQikyAVCq06ef2u7r0JPIz6fdKEaQuJLqarrUAwNtG9tlZXRrjorS95zMyDAfnyNITqbuzq6qmxD1EgJnRiZycmg/DB7sejEBaN/uP/6gaMawYbR/t2+nR5pg+3Z61LjaacpigjllOWXERGMjPejlBrBSSgxAhnVMDBmN0jQnab2EFGtiQhjM0vdIxYTIcZ83j4xe6UA3W4JAoJbmZC0yIYiPB778kvI/lHr9h4aSp3vRItNrtsSEmJ6dmkpRFWnhblgYPS1yc62LiZgY86FsamJCjqcnGXRCJB47RlaZ2vZOmkRJxkotT/38yJqU1k2cRJGJgAAyVrLHXEliYssWyuFJTnbrton7iZLBUVxMRtaUKS6om7BBeTmd4lq8tZoLsP39aYV79tA1rqN1rlJkAqAxKwaDcvfmnkpXTj6urqZjazDYPhfEOSMfdG4LefG1YOJE+ryDB/VvtxZcKSays+mR2a+fa8VEQIC+Wo+UFLrliQzi3r3NH8eLF9NxdrWdw2KCOWWprqaCpR6f5pSdTXcouRswIcEyMtHWRhVmsbHUalIU3AKWQ90Eci+6FCWDWSom/vqLPuO22+jztm0zLadVEAwaRAdR+vm20pzkWJs+PHcu8PHHJvVpS0yI4XXLl1suZzCYUp2siQnAJNJaWsiI1yImAFNLU4D2w5Ah6pNnxTrl9RICUYRdXQ088QQ9kU4CMVFXR4ZCfDyQEzKGztvHH6f2pg5MoHUG1sSEiEyceSbp166sDSgr0y8mevcmX4R0RqIFkZF0HcfGmjcwsIFaZMLTk9LETqVC7K4WEyEh9H8tkYneve0TE9Lia4GvL3DOOa6LyrlaTMTHu15M6K31EGICoPdKo551dcCKFcDs2SwmGMZxVNx/1dVUZ9yTIhNvvmnqnNqJGJwmjyhIjU5BcTHtr7596Y42bJipRayad1x40ZXaSCgZzIMHkyVVXk4Gt8hxF+5agdbIRHg4PR1FdKK0lJ6CesSENc48k+oVxI7NzLQuJgBg0iRcsm428qImWP5NFGFrEBN/bKhA3xggqiUXUecORt++VExnlYQEZO6uwbRpgPFvG4Ksf388H7QAy+quVP57bCzw/vt0oezaRefAiQnH77xDQ6ldyZVXmmeYCerryRGQkABk53pSK9gDB5yb4mQnWiITfn5UFtOVdRPl5WQYBgdbNyBFgW5wMOlig0FD3cRff+mqLRGZUUpiAqAypnXrKK1E6SchgW4hepkx4+ScLSTOFS35/a++SpekvegRE9LIhJLT7bffSPjJUYtMAKYWsVp55hkq67JFSQn5vcaMMb3mzJqJnJyuiUwA9OjVst0NDZTmJA3GSsXEqlV06542jcUEw9jP77+TIXjNNRZ/amoih6/ewrKTnbQ0846nANQ96UqRiYICenIIT/a4cZTqlJ1NT2+1fHnpzAIpSgZzcDBZEQcPmue4S++C7e30RNIiCAwG81SnQ4formzHhFNVHnmElFprK1m30tkRCrSfcRY2Gadir8/pln+Mi6NtPXrUppjYs9eAxOBSbDj7FWzYYMDo0ZQDa5X4eOzY44sffwSyd9oWVWs9r8WKwsnKfxw6lLZ18WJKI5I8qX//nWYfKs30cxZbtlhmsAGmyETnKXzNNcBpp50UURM1MSHKT0Rqj1w7uxqtkQnhEQ0OpkBDWJiG9rA7d+qql6itJUNIbVh1YiJlPW7YoPxTXW0++1ELbW3ADz+cnJ2itEYmWlromvvjD/s/S29kQprmJPfL7dhBWaLyyJUtMfHzz9qLzVNSSLTYIi2NHgPSOo3umOYEaN/u7dvp2Egf78nJdFzq6+m2PWsWCXCumWAYvRw9Clx/PcnxqVPpziWbeCwyVlwuJroiKVryGfX1CjnOamIiPp6eyNK7unzegCjCFsPq1LrkqNVNKM1RAOi199+nHHcxpfrcc+nYFRTQndtoVG6bqoRUTEgH3TmLK6+kbX33XXpyJiRYXbx82NkwwgPZngr7PS6OzkkvL+X2soIxY5Df1AfDa/7A+IsjO8dsZGTY2NaEBGQco5a4KbuCre6LpibgYG0//JIXr1zw+r//kVV32WUWka38fLp2NmywsT120thI57NSG9O6OopMdAbXrr6azlM7ht85m9JSOm3lBkdpKZ3SommWKEbtig4+gCkyYcuAFPtbFIJqag97/LjuTk6+viajVomEBLr9KP1ER+u/bx8/Tj6Kykp97+sKtIqJ9evpWDjSmrSqSl9kondvCj61tpqem4LsbHr9999Nr7W1UfBWTUxMmEDvUZoFqkR+vrY2yvIUJ6D7pDm1t1NURYiJoCBt2y1awsob+sXE0HzXP/8kX11EBB1nVxb4s5hg3IdoJehMg7u5mbyTgYFkyL70Eg34evhhsxYH1dWUatCvnwsVu9FIHvuNG130AaCncr9+nYZ8XZ2CF1EtLScuju7q0idTYaG6mLCWsz9pErnLpMeyqoqe+IMHWy4/ZAi5tG66yZTjHhJCxy4lxZTrrzUHWx6ZcFaKk8DLiwblvfAC7R8brUdLQklA5XQopH4MGED7dPBgy6nUUnx8kN87Cf3KTe14hw7VICbi45FR3huhoUak5Cda3RcHDgDBwQZ4eRksRooAoO1TMdDz80kHuiq33dq05vp6U2QiOxu0jWp1IV2MKCuRGxxFRZSRJ9pWnn46fY+//+6a7dIamaipoWXEqalpCjage8aEaE1qD9LB21oRx8Oe9ChXYjTStgUH2zYgFy+m80bap0Iv9kQmgoLovJXv85wc6i8hjbBlZ9NxlTaxk+LlRX4jLalOwsjWkprmSjHR0UF+NxGZKCpybrcxIXRF/wKt2y2tlxCIuomnngKuuILEYO/e9LoroxMsJhj3cfAgtbB05hl++DBdlZ98Yoqh33033aklw6fEDdWeh5JmfvqJXANq1l9Hh+NCatcuejpedhmQkYG6Oh2RCV9funtJ3T4FBeYdWcaMobv5unXWxcTYsWQZSedWHDlCO1hpoNaQIfTd5TnuIqFWb3Rh0CDTVG2V9zrcQWTWLDpmtuolAJSU0a01uzrc8o9CQFtLcTpBvt9g9DMUdg6r0yQmEhKQ0RiH26+swdbWc2EcouIihOkBfN55yg/3wkJlQ7Kjg06Vp58mreyIcaOGNTEhIhMizUl+GR05Yt4eUUpuro2CYgcpKaF9Ktp9CuQ1Aj4+ri1GlSOKabWICWmqSO/eGtKcAN2RCbV6CS3oLQgGnCcmWlrIX+EsKivJYzx0qHUDsrCQOlw9+aT2623/fsvX7CnANhiU6yaysyloKz2HMzIoTc1aHwStrZFLSshotzcyofb9ysrIgNdKYSFtR1wcmRUdHfreb4v8fIpYisZ/ajUThw/TOKZvv6U6kp07lZvXJSfT+2fNot89PUkUujILg8UE4z7EHUIM8XIGogOQ1OXl6UmtPV99tfOzxA3VnoeSZhYsIINdPkVYcM01VMXqCPv2kfvh7ruBiy9GfVWrufHX3k7WlpoBLK+bkKc5BQeT0Zuba11MKA2vW7YMSEpSXn7cOPqR3/3FU0ZvdMFGZCIvjzalvV37Ki0IDgbuv9+8wk8F8dBVfAgKo0uLmOjoi37DgzpzToYOJcPOmqe4IzIahzEEt/f9ERWG3jicpx5FEQ9gtfz9229XLrIuKyOj6uyzgcmT6VA7G/EdrUUm4uPJ+JWnrkydSuMzlLj+euCGG1yXXiQiEw0NFJwTCG+8lMmTzVNEXInW1rByMWEzMmGHmFDaF3pQKwi2hrPExMqVVDyvJlb1kp9Pz6K+fa0fl88+o9vjxIn03W15xjMz6Z4nrzmypwAbsHxWionQs2aRUSvWo9bJScq0aeS8sFVknJ9Pjfaqq62np9XW0vfVGpn473/JEaKV7Gx6LPr40E9UlHNTnQoKTClOgPp2z55NAfIXXwT+8x8SckoBwYsvpvvcRReZXnOprQMWE4w7kfbCdxZqRuhZZwHXXgs8+igAU96oyy6wgwfJOrvnHvUE14wMYP585adCW5u2qMX+/fTEmDcPOPdc1B0pRHm50fTWvDyymtRSEOSzJuRiAiCR0LevbWNBWjexcCEVV3/wgfKyF1xAHWDkeQ7nnEN36c2b9YuJvDyyegoKLN4rpv+Kycl285//AG+8YXOxkhLa5Yrh+dhYyiGxISaMRiC/MhD9Vi3sfC04mA6FtehEXoEHWuCDkXu/wFnh6VY9gEJMJCdTkaN06mpODgXXlAqg8/PJY+nvT8bEkiXOLw/SEpkICqLtkJ7C2dm07UreY6ORLs1ffyWDwtkYjWToiUJQqcGh5I0fN46OQVegJ81JV2RCpDmdIpGJrVvp+fHNN46tRyAKb63lyRuNdI2JYlqj0fb3F1PExfxKgT1pToBlFL+0lATz5Ml0rxNd5qwVXwtOO40Ex8qV1pfLz6d1hYVZT3Xau5f2i/ycUjPKS0uV72tqiE5OAmfXTUiLrwH1c6GsDHj7beqJsns3dWtSIjKSWsJKo0P2CHA9sJhg3IerIhNq6TGvvQZs2gT88YdZZKKhwQlGppw33qB6gDFj1MVEQQFd3WvXmr/e0UF5J7featt9um8f3ZkNBuCTT1CPXmhtNZgeEFlZ9JBXyyeXT8FWEhPnn0+uDlsJzkJMrFgBPPccDRKzlhKktL7AQEoKLizUl+Yketz/8AM99WQD6MTxdbhNoMGgKdG7pIQ8iFVV5t5pALSdIjJjhepq2m7pQwawneqUkQEMDCiGT8omJA8rVs1Nbm+n02fsWGDkSDLOpd19P/2UHmpK0RXpw++qq+hw7dhh9evoRktkArA8hcX3VdpHxcX0kF6/Hnj5ZfOxJs6gpoYEWWSkpcGhNKRt7FjyqDqrfaU19BRg64pMDBpE4l1H9zRnRCbsEROxsY4XYG/dSr4QZ9UKSScfq4mJ7dvpUXHFFXQr79PHehF2ezuJCX9/S6NXq5gwGk3nDGC5z7OzyYCXd/XWIiYAav9rax+KfaM2X1WwZ49lVAJQ36eVldpSpwSi+FrgajFhbbvDFTJnteDSlG6wmGDcSU4O5ec7U0xYS4+JjgbuugtYsKDzhhoWRurdpmJvbta+DaWllPcxdy49MZXSnOrq6Kn96KOUDiVl6VK6u6SmUoKstW1KTzelEnl7o86H7jTlP5zoD2trwJr8Li0vwAYouqLlySmG182aRYLidIW2qFpITtbkuTfD05O+y7p1isdfiImuGAoFmOrOw8JUHlo7d9KQPivk59PDXmrYAdrExNA+FUBjI6ZMbsO2bcpRg8OH6fWhQy2HHXV0kCd07lxlj6D04efvD9x8s/MLsbVEJgDLUzglhXZterrl+9LTSVufcw5ddjfe6FhnHDklJZQGERxsaXAoDWmLjiaDbO9e522DEkaj/ZGJPn1sRCYGDtRdkORoZMLeAuykJMciE9nZ5G95/30Sos4IqmsRE4sXU3mZHzVpQ9++1usmNm+mYz5jhm0xodQtDSAnW1OTepqTaJUKmM+OSE/XJiZuvplK/pSuU4HYN3KHgRyleglA/VyvqKB1ay2iln5XoGvEhNp2K5UgaoHTnJieS3Y2eeCdlebU1kaWlDWP9oMPAt9+i+pjVQgJIZtVU2HS6NGU86GF996jdhWnnUZ3fSVrpaCA6ikef5zauYj0oMpKEhBvvEFe9iVLgLfeUv6c9HSyqE6kFxiNQH0DXdJlz7xB7qmsLOszEaSD62pr6UdagK2HmBjqXrVoEXDppfatA6Bi8kmTOtvevPWWxlSQxETaZwrHXxTcdpWYKC2lm7fSKA+tyB8wAk1iYgD1ADz90gjVjkFpaWRciVC41CBISaGi0DlzlKMr8m2bNYuy2pzZerCsjLywtiIT0n1sNNK2z55NRdjyGhmp1/Seeyin+M47nbfNJSVk6BoM2iITABlBrk51Eml+9hZgW41M2IG7aiYcFRMpKcAZZ9Ct5uKLKXrnKFIxoXRcGhvJNyM9T6OjrYvgxYup3ik+3v7IRFkZPR9PzKi02Oc5OSZvfXIyRQeys+ka0CImwsMp53/JEvVltEYm1MSEmkCrqKDrQZ4CBtAxlteAdXWak9J2NzfTfc/eyASLCaZnItoOnXee8yITR4/S3c/aDID4eOCKK1D9677OG6rNi6y5mdy48nQkJZqaqKj6kUfo95gYssTkVpZIJwoOpuJpEZ14/nm6K155Jbm216+nSrHVqy0/S5ridGIz29uBPn2MKK/3o3oFLZGJvDwSYgUF5Pqy1/UBUDWpo9bZmWd2Timqrgb+9S/1YlozBg0iS+gkiUxERCgPGdeKmpgYNsy6Ny89HRg6jG7tPqcNVe0YJH8AJyeTpm1qImPkttvoO4SGWgoi+baNH0/az5ke9rIyysW2FZmQ7uPDh8mLfvPNdC3Iby1SMWEwUDZeSorzirGFiAS0RSaArhET5eWUXRccbMrHVqtx0Z3mZAddXTMhuo85Kia2bqWUHsBUK+TouSPSr9QM36IiurdLr9XoaPXIRFkZdfu5805lo1ermCgvJ8NVtAiWR4Ok3vq+fSmQ/PHHdGy0PkJmzSJBpjb4UouYaGmhgLheMSG+g5wVK4AnnjCPWnR1mpNSzYRIz3MkMsE1E0zPIyeHrIFx45wnJg4eJEvBWk86AHjkEVTvz0WoH7XjsPlgEgPU1q+3XmW6fTvVF/TrR+0qAHoSe3lZupEKC00RgAcfBL77jp4AH31ErniRl3/GGcDnn5ObSd7Pcv9+EhMnEAbzgAEGlM18kESIGAuqhriD5eebBM5JMPhL8NVXZNxqMrbE91SITDitZkIjJSWmyISzxYQYSq32AM7IAIaeG0miLCQEU6Yot32Vi4khQ8gLvWEDtR0UmlBJEMm3zWBwvlFcXk6frSUyIbYvJYWCWkFBdDrIIzjyfG6lUSuOII47YG5wGI3ujUyIFCeDgfaN0aheJ6a7AFsnra1k1DgamdBT61ZaSp972mn2iwkR9RKtOKdPp21wtO7GVgG2MP6lt2W1gDcAfPEFPTYGD7YtJoKDrUcmpKVn1tKcANovH3+sLSohuOACSgv84Qflv2tJczp4kFItlXyI1sREv37K68zIoGtVbFNrK/nbXBWZEHNGbEUmKirIZBJzavTCNRNMz0TEDQcMMLWFcBSt7UTPPBPVwf0QcpBSi2wq9qwsygsuLVXOF8nOpl6TU6fSE+bXX013fg8PSoqW3/mlhc4DBlAk4tprgQcesLwbX345uaLkFuG+fWatV+vq6GP79wfKYkZT1OfoUetiQkxhzslRLr52MyJXWJeYOEkiE5GRtnN9raEmJgYMIL2sJFLq6090QJk6oDN1LjmZBm5LPahGo6WYEHUTc+dS3wChyZQEkdK2OdsoLiujz1aLTMjTnISxJzzHSulg8raVYtSKvcdIjpqYqK2lW5xaZOLvv807aTkbaSGtqJNWMyKVIhP19c5rhVpSYppbYC+hoXQNaDWO8vPpe/TtS15+e9Lxjhyh/Sg6ZHt7U48MR2uFbNVMSI1/gVpkwmikEUtivoDc6G1vp88QqUu2IhPinAEsxYQ0zQmge4fWFCeBpydwxx3K+1BqZIt7kJIvLy2N7ldK8z+VonCNjXQOjB+vfA/NyKDvIrYpP5/WLX009utHj0tnRDRFm23p+pXOBUeKrwFOc2J6KiJuGBFBqTVKyYt6OSEmqqvJdhc/Sh7c6pgRCPnte6CtzbZiz8qidjcXXkjRCSktLdQbz9eXciyefZYaY0uJibEuJgDK5Rk7lt4vx2CgSjr5Z4s0pxPU15PnIiLihCdx4UIyrG0NWRN3aqXiazdy4AB9xVdfpd1nc0jQ6NH0hFCo+bAlJtS8/PbQ0kIGgKsiE56elLOtVDdx+DAZClJDbdw4CtlLU5Dy8mgbR40yf39yMgUKhTECWNZ9KHnSANdEJtTEhDjXARJXjY10fkg9x0OHmqeDNTfT95AbO44cIzkivQ0wN+SKi+kWIYw4+ef7+7t2ErbUy+zlRbdctcJbuZgQBoyzohNFRbQtjgws9/DQ52kV56vY/1qiE/Li3K1bqcO4KIIGKHq3Zg115BLPG2t+MblgrKmh89tazYSSmFCLTKSlkf/o2mvp93796LoQnyuOuTTNSS3lTSkyUV5OgkQ6EVpw/vn0rx4xAZCY+P57y/u71MiOj6f9quT0U6uXAGiftrWZ908Rx37MGMvrvqaG9utrr9HjtqSElunfn64bQWysKcImMBrtm4otbbMt3W75ueBI8TVgEhPObuEtYDHBuAcRmTAYKNdAqQhbbw+/gwfRlDgKffuSPSl+5syxXLTaJwIhHrXA2rW2FbuoO1Ay6FeupLvA0qXqhctKHZ3kYmLcOOrLqdZeUXy2uBOUldFdTyImRB55Z8FkQgLFgIXVpYZwn8unX7uZJUtorl///mQ82zRU+/VTnl0B62KitpZutM5KgRIPmD59rHvUbKEmJgD1ImyRxiPdBV5eVGgsnY+YlkYeeukDDKDl4uIo0CaQpzmJib1y3Tl2LGXeOUuYlZWppzlJIxP+/uSpXb+ejvOJYeEW+ygzk5aVb7czxYS8ZqKmxmSgREcrZxB6eJBh48pUJzHJWGDNIy0XE15eZIQ7q27C0XoJgZ4ccHEteXmRIW1LTGzYQGl/0u8sFaqCkSMpADx4sOl5M3y48jWwfTtdW1KDUwxlCw1Vj0yImUhS1CITq1dTq2ZxbURFkfNBPH6qq+l38UgICiJhoCSA5OdMRAQtW1FB62ttNR8tEhFBfjW9TfwGDqS0LPncjvx8Mp579aJ9FBmpHEG0JSYA8/1aUUHn9+DBlus7fJjE88SJNJBz+XLLdC6ABGWfPuZRn/feo0QBNZGuxoEDlutXS3NyNDLR3Oy66DyLCcY9SCua4uIs6yZE21itgqKjA0hPxxH/JHh4kEejtZVqkJUMheoaA0KungJ8+KF2MXHppcAff5ieMEYjFU7PnascYxUouZH0phRNnkxPAuFe3r+f9pvkKSPyyG22cpQjrKmTKM2ptZU6aggPuaNeb2s1E+Xl9MB2pnc6PJwMlwED6EFtjyFmTUyoFWGrtWVcsID6B4guJWoPYKHrpcak3NgWE3vlunfwYPrO1jpNaUU89ERkQi7GpJEJsY2LF5MB4ONDrw0bZr4t6elkIMoNekdS0eRI05xCQ8kIKihQr5cQuLpuQjrJGNAnJgDnFmE7U0zojUwAdG3aEhM7d9J1cMst9GiR10tI2biR7letrXTeeniQp13Ohx+S9116nMV2iVoWrWlO4pEivy7+/tt8fI2nJz1GhdFbVUXHVlwD1lLe5OeMvz89X0pK6HoRE6Gl/PwziSu9nHUWtYmVIr//KYn+jg7qIjVmjPJ6/fzoeMjFRHi48vqkNVWzZlHKmJKYACxrot55h8yVu+7S5zz67DOaVi1F6VxwVEyEhFA00FVF2CwmGHXKy+1zqZaV0R1T/CgVWEuvUCUx8eeflKT7yy/aPjMvD2hqQkZzPIYOpYvGy4tueEoenOpqIOSSs4Gff0ZkYIM2MREbS6k0ojLr55/piXPbbda3TSnNSVqArQVfX6rJEJERMflagkVkQivSyMRJIia+/56+iwidO0tMKD2shSfJmWJCGJS9etH/9a67tpbOUXsjE3IGDKA6/tmzyeCw5s2TI04PcStQEzkeHnR5OMMoFmJ4wABKq5Dn60sjE2IbU1NN9RIA7YfCQpOhpLZvnJ3mJI69tD2sWicnwdixZBS5CnnKil4x4cwibEfbwgrsSXMCtImJjAzgsccomvXKK3TNSKNeUgwGetZ4eZFxrTSMraaGphUPHWreWU26XXprJpqaLL3gSue41OiVr8vHh36UzgV5ZAIwCTg1A9telO7vSmJCLvqzsshxqNYNXkmkCaM8Pp7uD9J7i7Sm6ppryCz5+mvz2hCBdL/u3EkmyB9/UARq0SINXxr0fX7+mWpvpLgiMmEw0DXj7M5sAhYTjDrTptGVpJdrryX3xEUXUbuG006zjPtKq7cGDLBMc9q9m/5V6mmpxKFDwKBByDjqbXYzVevHXV0NhIzsByQmIuLoDnW13tFBd05RdyBNdVqwALj3XttpRPI0p44O2ii9hrv0s2XF14B5ZELXDeMkjEwsXkwPZRHwcYaYMBisiwlXeKcB+9rDFhSQJ1AtR1avmACASy6hINo119DDT4+YqK83nVOilaUSzvKwl5WZhkoC5saOyIGWRyYAc89xnz708D18mH7vKjEhaiYAk8GhJTKxZ4/zWtTKkRfTnsqRibAwbWJi4kRKG3r9deCFF2jQodwTr8Ttt5O/SfrcWbmS0p9mz7YuJurrLc8BJTERHEz3B+lntLaScS1tMABYFxOA+rkgj0wA5mJCycC2F6UUSbmYULqPpqVR3Ze1+ht5/YEoZI6OJh+d1I8pjewGBAA33USmha3IxOLFtOyAAdRa9sknSVjY4tNP6b4svx4CA02RLvl2O4Ir28OymGDUKSykzkR6aGkBduwga6WszDR56q+/TMuIKVjWIhO7dlHBs1JPSyUOHgRGjEB6uvnNtG9fuilKC9+amuj3kBAAM2YgcvcP6oVJBQV0VYs754wZ9KT4+29g0ybg/vttb5s8zamkhCwivfUJl15K+/H4cYvia8AUmdCd5hQfj/T8QLxUeBdeWjMCL71EM/Nc2V3GGgUFtItvv9302tix5CW0lo/a2qo+j6KhgYyprohMSPPmAWWP2vffW29rKQx2tS69Q4fS50izAI1GMpytFUC+8AI9BIuK1FMD5AQGkoEs9o+19Cs1MbF8uXotxcqVlgPmhSHj60seX6kxIPabNDKRkEBG0fjx5uuRFmHLOzkJ4uNNo1YcoaODbnfSY681MiHy7LOy1Jf56itt12RhIbBli/lrWiMTRqPjkYnt2+laVcNZkQm9NRNCAIeHW8+eNRpN58qYMdSpe80a5RQnJQYMoIiqdPDZ4sWUNpOcTI9UcS3IxYTRaFm/UF1tWbhvMFjWTRw9SmlN0joGwH4xIT9nANM+lw9xcxSRIilN3dSS5qQlwir38otCZg8PS4EidziINFs1MVFQQMfryy9Ny559NvDyy5S6ZO086+igukBpswvpNgPKERVHiIzsgZGJL774Atdeey0uvvhiPP3006iw4irIzc3Fk08+icsuuwxXXHEF3nrrLbRKnkx79uxBcnKy2c+MGTO64mv0XIxGenqIycxa2buXJP2QIfS7hwfdWaXumJwcuqOJO6RcTBiNFJl45BEy2rU8MU50cpLfDCIj6cYr7RRRXU3/doqJn1ehtdX0uhlZWVQBLFxS48dTIuYtt1CVqhZBIE9zKiigp7O0LYgWoqLo89evp/2iEpkQaU6aM9RiYvCpx534yng9ssuCkZ0NzJ+vPCfP1bS3U0TiuuvooSyIiiIDxNpQtD17aFiZEvX1tA6lh6Yr0pyk3mn5Q7CwkLr9fvSR+jqsGeyAqWOTNDpRUECF0dYGnnt6Uh/6N9/U1xlEWldQUGBdTOzZY37upaXR5aLklygvB268kbIapcjnIkhFpBAT0sjE1KkUKJR2XAFMERxhICoJLemoFUeoqCADwZ7IhLc3eVjVojoZGeT5lOeVK7FsGc3BlCKPTKjNF2hooO/gSGTi3/+mETdqdHVkQt59zFaak/w6mjWL0lbkqSjWmDWLBITRSI+mtDQ6fqNGUURh505aTi4mAEuHh5IAACx9VBkZZJTLxyw5EpmQpzmJ1DJnpzkppUjaSnMyGmk8k0iFVUNJTAijXComOjqo/a/0HjFxIjBvnrLjRezXr7+m/0tT4ObOpfPz22/Vtyslhc6zSy+1/Ju/v3KthyPdnIAeGJnYuHEjli1bhgcffBCLFi1CfX09XnzxRcVlGxsb8cQTTyAkJATvvPMOXnnlFezZswfvvfeexbIrV67EmjVrsGbNGiyTz0Nn9FFfT26wtDR9DcZTU2lYltSlmpxsLibkd6IBA8g1KOK7x46R9ZCcTO0yfv7Z9ucePAjjMEsx4eVFN0CpB6e6mux4Hx8AZ52FoLZK+Pp0KD+Y5BOkPTxolsSePXTH0IIIjwj3qyPpRDNmAG+/TW7UwYPN/iTyyPv0oUOndaATPDxQFhiPa/w3YslnnliyBHj4YfKadDUvv0wPDIXL22ZeeXExfW8l7219PRkwapEJD4+uS3P67DMyhIWxoYQtMQFYFhhnZNBn2dKokZE0J1EPUkFkbdtGjqR9LN2X4jxSCjL+/DPtA7mQk+Zry42dujr6jlKjKS6OCh/lCDFRWkrBUOHjkCJGrTgqJktK6LhKRU5sLF3utiITgPUUsaVL6d+CAtvbkZZG+1/6fbRGJoRokxfXa63DEgWx33yjbrB3dc1ERQU9wqSRCWtiQlxH0uFgc+bou2VfeSU5sFJT6fy/6iqTN1zqW5NeS6J+QauYkEcmrIllZ0YmXJHmBFie/0ppTjk5JjNh505a5uqrra9XrWYCMBcoeXn07JA+7g0GEsfybu+Aab+KqJPU5DEYKEvcWmLF4sVUbqmUomUwWBdB9hIR0cPExNq1a3HNNddg8uTJSExMxBNPPIF9+/YhUyE2euDAAVRUVODRRx9FXFwcRo0ahbvvvhvr169HvcxaCg8P7/wJc1TCneqUl9OdLyxMmztMkJpqmuojSE4Gfv/dZEzLY6T9+lHcV9wZd+0iq8TPz1KIKHHC/XM8Kgk1NRY2tkXdhFmrPS8vGC69BJF+NcoXmVxMAOQ6v/tu7UnnkZG0L8X3c2Sew4wZ5J4fMcLiLiTSnEJC6OP0hDPL/WLQO8yUrHvrrWToOcvA1sKmTcD//R+lFMg9o4DtfHxxjNXmEkRFqYuJoUPtb+EqRy4mpA8so5EeIm++SZ+ndmlpERPyOQpqnZycgVYx4etLl644Tk1NVPg9a5byZSxekxvy0nxtJTFhq0xJIMRERgZts9r7lFLR9CKPSAHaIxOA+vnd1ka51fJWlGqkpdFxEPvWaNQnJvz9LQ0cramT2dnkbR09miJgcsQk8K6MTMi7j9mqmbBWd6QVPz8atvnhh+Q8kKayTJmiLCYA5cJbPZEJZ4mJhgY6jkoF2IWFtD5nRiYA8/NfaZaNfFq9qFNQMvSlyGsm5GJC3HsyMuh3LXUxAG1bdjbw228UeZUjTBelZ0plJUU07rzTvu22lx6V5tTS0oKsrCyMlRhiMTExiI6OxsGDBy2Wb21thaenJ7wk8WtfX1+0trbisKisO8Ett9yC6667Ds888wxylToIMdopL6e77qRJ+lKdlMTE8OFkHYpcBrlbw9eX7oyiCHv3blPysxYxUVICVFYiwzgE/ftbGgx9+1pGJsxuqDNmIKK1UPnBlJlpKSbOOYeeElrx9DSfgu3IPIcxY+i9snoJwJTm5OGhv/tKmUcU+kSYXCvR0cDFF5MR0xXk5dHD9513FL8aANtiQhxje8REUhK9T8swK1uo1Ux0dJCmLi+ndKwbb1SfnmstlUggL8J2hhGkhjTNyZbQkR6n774jQ/SZZ+jyl+//lBQ63nJDXhqZkKfkiPNcC8OGUR3JwYPW940zirDlIhKg/ZSTQ99Ha2RCbnxs2kTX9A032BYTdXWUqnHLLabbZm0tCRItBdhK9RKA9jSntDQSk/fdp3xuV1eTwHRmzYQtB4D8fNUSmXDGdTRrFt0/fX3Nu4wJ31pFBRmVcjEhPy56IhNKNUGiRqqtTbuYKC8n77jcJxsZSY9ng8H5vTqkKZJVVSRopPtGOq1eXqdgDVtpTuLeo/e4x8bSPp0+nZ4tcs46i6JTSnVQX31Fj3K1LlS2ttteepSYqKmpQUdHh0XkIDQ0FFVVVRbLDx8+HAaDAUuXLkVraysqKyvx+eefA0BnnUV4eDgee+wxvPzyy3j2xAThBx54AJV6h54xJkTCpB4xUVREVqG8f57BYB7bVarektZN7NplapZ93nl0lSv1dxUcPAgMGICMXH/Fm6n8pmtxQ734YkQ25qIkw3S+rFtHJRuKkQl76NsX6X/W4PLLAWO+A2lOBgP+m7QMK3vfZ/EnqcdWb3vYcq9I9D7DPNl+1iwK0dvbYeaXX4B//EPbsvffD1xxhXnRtZyxY6lURK0IVUtkQs2Aio2lfeaMSIzcQy31qC1eTKLJ15f27xdfkAdQjpbIxIgRVD7Tvz/9fPyx9YeTIwhjWwxh0yomRApAQgK957ffTMsdP0755Hfc4brIxKBBdL789JOyoSVwxqwJuYgE6DtXV5OBpGRwSElKou8tF8yLF9M+GjDAtpjYu5e24eabTV7RsjLyZ0jveXrFRHQ0GXriXOvfn4SiHFEQe911JOLk36W4mCIfarM59RAZCYtaN6OR6mfkc1HkYsKaaeAsMTF2LP3MmmU+hmjoUKp5WrOGvODyiJGzIxN9+5oiQlrFRFkZ7Sd5/UVEBDk65BOhncHIkXSfzs6mYxYUZHkuivvQ11/TNmgZkCc3yqVdkeSRCWv3CKX19umjLmgCAsh8UvKFivuiNaTnQkcHCSzu5iTBqDOPICwsDM8++yw2btyIiy++GDfeeCPGnKiGMZxIUouLi8P06dORmJiIpKQkvPDCCwgMDMSPP/7o7M0/dZCLCS3HLTWVKsyUnhTJyaYEQqWESyEmRPG1iEyEhVHMfNs28+V37AB+/JF+vvsOGD5cNc1DftO1uKGGh1PIPNWUZrdgAbBypdGpYuKvXQasWwfsOBjkkFvnZ48p2OVzlsXrUo+t3laOZS0h6HPXlWavTZ9Ohq7W7rwW2/kz3TS1GGkHDpCRbY2EBLpB//238t8diUwEBzuvRajcQy08avv3U+ci8RA580w6N9eutVyHFjExdSoZ5ytW0M9PP9keeWIv8fEUOMzLM03sVUOIibw8uuTFNknTOwC6pJOS6FJXikyoiQk9kQkfHzquP/zgnshEnz4mg9Fa+0qABNKzz1LEStQulJaSYLzjDvN0FTWEMT9pEr33yBHTrVya061XTJx5Jt3exbl2ySU0N0Ht84OCqJuNvO5K1EuodSnTQ2AgpRNJI8p5ecDmzSSsBXojE85KFzQY6Jp85hnL16dMoUJ5MbBOoCfNSeokq6ig46203d7edO/Lz1dvM6sUmZCnOAGm89vZKU6AeYqk2v1PRHlF63At55FSzYTwZcfH036rq7PvuO/ZQ5nHakjNHsG+ffQMu+EG6+uWngvCIcEF2BJCQkLg4eFhETWoqqpCqMoTatKkSVi1ahVWrVqFb775BuefKN/vqxIr9fLywsCBA1GkNGBAwlNPPYVHHnkEjzzyCDZt2qT7u/RoxN1kwgQ6+7SkjSmlOAmmTKG/NzYqRybErIn8fLraR482/U2e6rRwIbWNfegh+vnhB+Cyy1Q9MzYjEwAih4WjZB+1fMrKojB0QYEBpdXeThMT2Tl051t85FyHxERJCXkp5Eg9tnraw4rGXfKHh7c31U6opeLYQqTgiOJRa5+vpSjTYKDQsFqqkyM1E84SE/X1FIKXG5Xx8dQha/BgU2cQg0F5wFVTExnTtsSEpycFAc86i34mTTIvGnUmAwZQydNff1kaQHJGj6a86vnzadSMyOiTX8ZionBCAhmB0taxtgqwtUYmALon1Na6R0x4eNClrjWt55lnaFv+8Q+6Lj7/nI5rYqI+MeHvT+dESopyIa1eMeHhQV5gca7deKNyPri0Veedd9L2S/t3OKteAjAN4ZKKibQ0Em9Ll1JnOECfmGhspEedHg+1NcLCLL37gKlFrPwal4uJ1la6n9iKTGRk0P1NaTnAdO7oiUzIzxnAdH47u/haIBwRamIiPp4EmlqdghLWag/69KF7ybFj9kWkrLXvBpTrJpYsoVFcSteZ2nZXVFAkSKsTRc6mTZvwyCOP4IMPHkFJyVP2rcQGXS4mfHx8MGjQIOyRtGUpKipCcXExRtiI0YeHh8Pf3x/btm1Dnz59MESpNQeA9vZ25OTkINrGXWvevHlYsGABFixYgGnTpun+Lj0aYV0GBJB1IE91ysmxbA5vTUwkJtKdf/16uluqRSZ27aIaC39/09+kLs0VK4DnniO5f+iQ6eef/7Qa5rUamQAQMSEOpXmNQGMjli4FLrsMGNSvCWnB59u+6rUQE4PsIl9ccgnwVeU01If3t3tVojONHKnHVk+aU3U1PXiVHh533kkhZXsyBjMyKN3CVqpUbS09xLUYGdbqJoqL6cYufzC2tlJua1QUGTbyeQLCgLJnuJyc0lIyHuQepIQEOmXloe1bb6V0MKlnvqCAhJy8mNed+PmRKFAygOQEB9Pl/u675t83OZkub5GWIsREbCwZq3l5pmWtpTnpiUwApnuCNUNBTMOV39L0oFSADdD+0mpAe3qSAf7HHzTf4JNPTPtQ9LW3di1JjXlhyChNMtYrJuRIIx+C4mJKXRN+oHPOoc+VpkM5q5OTQF6EnZZGhprBQEFrwNIwDQuj76k09+TIETq3nCV41BAzK5TEhPS4iAiVWmRCzFCyZQjbEhPy+T1K5wxguiZdEZkAbIuJhASKaKrVKSghFWitrfRdhZgwGGidBw7QZzq75uzMM2mfi0YZLS0UkdJb6yEEkL0RvWnTpmHBggV4660FAObZtxIbuKWb05VXXok1a9bg119/RWZmJubPn4+kpCQkJiaitLQUt912Gw4dOtS5/Pfff4/09HTk5uZixYoVWLZsGWbPng3PE5J/9erVSE1NRUFBATIzM/Hqq6+iqqoKF154oTu+Xs9A6qqeNAltv+/A0aOSv40fDzz2mGn5lhZyW6qJCYOB7qCLF9MdSW4NxMWRe0Ca4iQ491yayCPawqxYYZEs2dxMhqDdkYlRUSjx6Y/21WuxdCl9zNiYEqSFnK/8ffTSty9yyoNw3RUtGIAcrNkVb9dqjMbOenML7I1MiLmCStlpI0eScfDqqxQA+uEHupkLr5+17czIoO65TU3WW+QVFZFH3VrqjEBNTIjoRv/+lkaSaPomHj7y6IQ0MuGsjj4esjur6BIin4PRty8VukujN2LAlnwd7iY+njyCtsQEQMcpLIxEuSA2lkTGL7+QUZyVBUyebBq0Jd33zo5M+PvTuaFG3750fBzp26FUMwHQ/tJjQEdEUDrcv/5F23PNNfR6TAwJYbUORi0tZBQpiQlHIxNypJEPQVoaHV9xHzEY6D769tume8eOHc411OVpG2lpFEy/7TZTxE8pMgEoO2SEUe6MNCxrDBxI57ytyERVFXmkpb41gXSGkqNiQinNScm55O1N+8+dYgLQZowLpPtUHHNp7UF8PAnPkBDl69cRfH1piJ24Ttato8+ZPNn2e6XpWc4ovgbovql0LjkDtzyuLr30UsycORNvvPEG5syZAz8/Pzz//PMAKKqQl5eHZomLKCcnB08++STuuusubNmyBc8++yymSNojtLa2YtGiRbjzzjvxxBNPoL6+Hm+88Qb6KF0NjDZkYuK7jd7o1GbPPEMN2z/+2DRFbM8eOlPlfVmlJCfTVasUIx0wwBSZkIuJ4GB67dZbyV2nMOUlK4u8p0o3IBGZEKFGRTERZUBJ6GBseWk72tuNmDYNGBuchTTDOPXvo4e+fZFdF4GEwFLM8vgUi1fbV4FYX09efGdGJpRyqqU8+yywYQPw6KP0c/XVgMpYmE6Ki+lGOGKE+YNdbdnoaG0P8KQkMpjkVFWRMTVkiKWXTYgJ8aBQExPOiEyoeafPP5+0t5K37/77aeK46PqhpV7CHSQkmFqs2uKaa4CnnrJssygM3JQU6rEgrkNpmlFLCxk3zopMnHcenYPWxJmHB92CHDn+SmlOAIlFvX6tSZMosvP00ybh5OtL61dLdTp4kO6BAwfS7xMn0n779VfniwnAMm1NaRrxnXeS40HcOw4cIN+Qs1CKTIwdS5/73Xd0D8zLMz9n/f1pPyk5ZFzZEU2KwQDMnk3nphR5fr94VindG6UzlGwVDwsxYdYWXfKZWtOcACquP+MM9c9yhNGj6Vm9a5fyfea006hRxyWXaF+ndJ9WVtKxlxrU0poqV4hIad2EqPXQ4iiSiiBp0bijuCri7eR6fO3MnDkTMxUqLqOjo5Eiq/icM2cO5syZo7qum266CTfddJPTt/GURiYmDmanI7sDqEjZi/BPP6Vq0o8+Ikvol1+Uh9XJSU6mGL2SWyMuju50v/9OVoicf/6T3Loq7YEyMsiQVLpIo6PJQKmqIm+popiIBEpaw7H42AW4/fo8eHnFYQz24NM6HWNPrdAWGYO8ligkeP2FkTFb8WSqAZmZ1qcVKyEenM6smbD24AAopDx9uun3AwfoUJ91FhlKSqSn0yENCKCb57hxdENUKiDTk/qQkEDHT76uoiL67n37KkcmfH3JqxYQoC4mwsLIO2402v9QUTMop0wxbw8pZepU6mJ17bXA9u0nt5gAtG2bWnFhcjJFuaqrTakegLmQE+etMyMT779vezlHOzqpHXt7i+KVvK/CKJwwwfJvaWlUjyPugb6+lGq0cSMNoZRiTUxoLfJMTqZWzuJ6SUuz9ANFR9M57SqkNRPl5SQcRo+m73DGGSTI6ustz1m1uomuEhMA8OSTlq/JIxNqxdcC4ShLT7feOa9fP2r0UF+vPTIhLVuUouVasheRIrl3r/J9pndvGoioB3ntgVIKamEhcMEFdm2yTaZMoTLP/HxqDqB1/wUGUtQJcM70a0GfPo5FYNU4yQLpzEmDVEzExyPDh5r/77n/Y+qZOmgQRSiOHqX+ltbqJQQJCeT+U4pMhIWRhVBbqzy7/vbbqdhaBWudGIKCaNWibkJNTJSVG/BtxwzcWfwaAGBs7S84UtlbsWhXL/mgO2NswxFExPljxgzbhclKiJC+klfN3siEWn6sGqNG0YTqW25RvylJH8ojRpC38MsvlZfVU5QZEmIy+pXWofRgrK83GZ9KrReFmBgwgFKyxA3cHtQMSlvMn0/G30MPnbxiQly2jmzb+eeTH+L7783FlTTFrKyMjofofuRoZEIrjhRht7bSNenqOhdrRdhKkYHkZDqnlSITTU2WdQN6IhMi8iHGQyl9vquRRibS0ugcFUbXrFlkxIlBnlLUxIQrBz9qQa+YiI6m8yEz03aak8gcdzQy4WrEOeSse6BS7YEUcV9zVtG9nAkT6Fp7/HG651lLt5Ria7vtxVXHlcUEo4xUTBgMSPcbA19DM9IKImm+PEBn+//+R/kbv/5qW0wAlESv5M42GMiaGzZMn9vxBLY8StK6iepqy/z8iAjysJ0+vgNDflsMHDmCvnl/IjKsFfv26d4cC7LrIhCHXHjt3QXExmLWLPOOI1opKaHdXlVl2UlFHpnQk+ak9wZz662UynL99cpzH+THY9Ys9VQnvUWZSkafWIctMSEvcOzooN+Dgyn0HR3tWKqLWt68LXx8KE/+669poNHJKCb0RCbUiIqi/grl5ZRLLF23NDJhbVqz3siEVhwRE+JaOxnFBKBcgA1YXit6xISIfKSk0D01K8v9YkL6+ddeSzUmSt3HlKZgizovd4sJ6THRIib++IP+b63DUr9+dGw9PCyFuFpkQo+DyZl0tZgQ9zVXHXdvb0rt++or+2s9nCkmXHWPYjFxqrN7t3JbC8ndxGgEMhrjMN24HmlJt5k/yW+4ga7C4uLOYXUffkiGpvh54gnJeh96SD3fIy7ONKxOJ7YeAtKOTko3aH9/unhn3edLSaGvvQYUFmLsaKNFwW9GBhnR4vvdcINyHr+U7DwvxHsXUpF6bCymTSND9qef9H3PkhJK52pro5aBgpYWOozSORPl5drGg9jrhXrzTfrc556z/Jv8eNxwA3kwlfaT3naRSrUNWiMTcs+f+L8woBxtEapWM6GFuDjqMVBe3nPFBEAG7sSJ5kaNNMVIHinrqsiEnjSnxkYKzIqBgyUl5KCQ14g4GzUx0dFBZWtyY378eNNwLSm9eil3PtMjJgBT3cTevVQg7uwCVltIC7Dl3z8wkO47SuerUmRC1HlZK/tzNUo1E9YaU/TtSw0xEhOtD5ETnciDgy2FldI9s7TUvWLC3995aT3uFhMAXSfh4VTvoRVXFGADLCYYV9DcTBVgW7aYv97WRnexE3eT4mKgttkX11/WhLTyOPNlDQZKAnzyyU531+ef08Nq8mTKW/2//1POz7Xghhss291oQHiUrIUp5ZEJJW/P6tUnPn7uXOpn2qsXxp7hYyEmXnuNLvLJk+knIAC46irzSaxycnKAhKBScp/FxMDLi3b97t36vmtJianOQprqJIqMpROwm5rMBYca9nqh/PyA//6XjrdctMjFRHAwNeBS+r72RCbkRp+eyIT0YS2KtYVx6mhHJ3vTnAQXX0zFetIalZOF+HhKT3L0YfTEE8CiReavibzlpiZtkQl3pzmlpwOvvGLKvHT0uGtFTUxkZZGwl3dX9/amQmS5/8ZgsPSCA/aJiW3bqGC2q6MSgHnNhFJk5qWXgNdft3yfkpjIyDDVebkLe9Kc8vJsp+j4+dE1pSRMgoJIFIuW2cePU7c1G536XcYFF1BdhLOKoYVRbjQqFzKHhNB9beRI53yeEnfdRd9JzxwgLsBmug+//EJnq7TBO2C6y56wMDMyKAPprHdmIj2eDFSzG+6wYfRkPUFNDYWYL7+cfn/jDfJcnXOOje254w67vkZZGV1sKmNHANBNV3R0UrtBd44aGTeOLP3ycowdZ8Brr5mWqa2ldJTffzeVdnR0UIOpWbNIkCjdBLOzgaG9a4AjDZ1uInu84KWlZDQHB1Oqk/C6CTEhjktoKIW0y8ttp4SUldmfL3r22aZ+82L/NzeTQS739Kh9X3siEz/8YLmOESP010zU1NBNWwyWcrSjkzOMyvPPd+z9rsJgUGykppv+/S3zhqOjyeA5dswyUqYUmXBFmlN8PB0/LesvKiJP/Ndf033N07Nr5oKoiYm0NKplUpqyLS10l6J0regVE+PHk4j57DPrk4BdRWQknS+1tfSckouJmBjT0EQp4eGWdWfuTnEC7CvABrRtd79+ypFqkfJWV0fPjW3bqGueuyIT3t7UlMJZBAZSOnFTk3ohszPua9YID9ffxcxW4bi9cM0E43zWr6d/5WKivJysUj8/AKabbL9+dELv3299tfIHkrVBY84gI4Psc2veyr59yeBsaqKHn7UbNAASR7NmYexYSs0RmWArV9K+kNaIe3iQd/7PP0k4KZGdDcT3PVFccEJM2GO4CmM1NNS8o1NdHYWGhVEsBqdpqZvQW4Atxd+fSmWkDdgyM+l1+ZBvte9bXOzayIRU/CoNhZKeq46mOdlbM3GqI0qmcnKU05zq603D2lwVmYiIoPPk2DHbyxYXU+3H8uXUaG7r1q6NTFibPK0VZ4gJb2+KziqlWHUFotZt2zY6Z5SEgxJqkYmTQUzorZkAtIsJpXWJa0l8rhgm2VMQ36+uzrnpQq6GC7AZ91NaamqxIcNoPPEgMhppgsqECZauLlnei+hwYTBoEwZdLSa0dOAQkQmRimRTTJx1FvDwwxg4kEKTYncuXqxcRNWnD7BqFeVRK7VCzMkBEuJPWACSyITelBqpmJCnOckNLK3tYe0pwJYi7zefnq7cplfp+7a20umqJzIhDH6pQWVvzYT8XFXKm9daJC8GCrKYsA9xXOXnozg+4ri5KjIhpuFqEZNFRXS+TZtG8xMWL+6a4x4bS5E/+XXtLjEBmAxPd4gJf3/6Hps20edrTY1RKsB2dycnQH3OhBp6IxNK6/LwMDVSBOherlbW2B3x9SXnWncTE1wzwbifTz+1bCx+gtWrqb4Y6en0RLz9duXIhERMSD02toSB0dj1YuLAAfISWkNEJqqrKeCitVDSw4OiEGlptMt27VIv6zjzTMrRve8+89ebmykfPGHIiRyEE+6zhATyggqPqxZEgW9YmGVkQm5gaW0P60hkAqAHT0qKybhX8/ApGWoi31lMp9bCgAEUbZBOvhXGnaNiIiGB2t0KAbFzJxm21iZ4C6qrSRx1RbpLT0ScH/LzUYhkUd/iqsgEQPeRdetsLyeNpj33HE367ooc84AAMiqk/h+jke5LentXyK+V5maK2uoVE5dcQsfOWjchVxIRYRITWlGKTGh5jrgavWlOIo1Ly3aPGmUaaChHnAtiMr0zBwu6G4PBZJh3JzEhzgWjkcUE4y7Ky1Xdazk5ZNhi/XqqdBoyxGZkQo+YaG4mg0ouJv7+W7mFqDP4+Wfb4+mlkQmbUQkZ4jsvWQJceaX1i3rWLPquotgbIMHg4wNEDwulkMKJnJv+/WlfFRZq3xaRRqOU5qQUmbAlJoxGxyMT8n7zamIiPp5ONWnzsKIi+mylXG81evWifSAiCM3NFKURtSSOiIn+/UlIFBTQDfy666hw/KabbB+nkhLygokcZEYfIiokPx89PemSEcfVVZEJgIp1V6ygdEZrSOt8PD2pyPmBB1yzTXLkdRN//03iWj4wzhZyMSH+r1dMjBxJ44ZcMT1YC5GRlFrpiJjIzqZj6qrJzloJDDRP6bP1vOrVi+5V1jo+CebMUU/DFfdN+WT6noJIH3NmIbOrCQykovjKSrKdnLXderIA9MBioidSWanq8q6oOPHQWL+eKub69aPIhDRnRCIm5MW0Y8dSzYTo/CBHeA/l3t6AAHroOZvycmDfPttFq3370m4pKbFPTOzcSQEfW32iw8IokrFtm+m1nBzypnucdSbN5DiBry+lLWhNdZKm0cjFhL1pTjU1dCwdERO+vlSILVKd1MREbCwZXtJAmN56CYE0ylFcTBGkiAjzzh0CeQG2tZoJHx/azqNHaXLxmDHk9bzkEmo2ptRFWSCEnruMqu6OWmQCMB23jg4ynF0VmRg4kK7zu+6i81gNpQ5kXXXc5WIiJYWKwPW2pZUL75oaEvV6Os6cDIj0Mr1iQpomunUrCQlXiVStiPNaNNSoquoaw15cXz2tXkIgnEjOLGR2NeJcEINhtQhGLcjTj50Fi4meSEWFqsu7ogKoq2mndkTTp5MrtqHB3DKViInMTEoLEsW0gwfTyZiervzRSg8kg8GUKuRsfv6ZQry28pUjImi7Dx+2T0ykptLD+oILbC+fnGyeFpOdfSIFoG9f4OmnzZbVU/BbVUWGf2Qk3RClD0N705zKyqg/uV5vpBxpqpOamPD0pNaL0u8r0pP0Ii3mLi6mfeLpSQ9FYXAK9EQmADomjz9OkZalS+n8ffdd8hLKDp8ZXC/hGKKmRmnuiTB2xHF1pdF3+eXA7Nk0Q0YYdXL0diBzJnIxsXWrfQagmrDubmI4MpKua9EyWwsiMiGcDieLES0tFgbsi6Tbg1RM9KR6CYGITHSnNCc/P3qm5ebSOSCaq5yssJjoiQgrU8HlXVEB1FW2UgJl//50FwkONncXS8SEMAyFmvX0BEaPVhcGag8kvXUTRiOwdq3t5bQ+BDw96aGTnq7/5jxiBAmJO+7QdkHLC5Kzs02DceToGZRVUkI3mMBA50UmxKF21IAQ/eaPH6ftUmvTK+/o5EhkQuw3qSBRmuxrj5jYv5/qi4Q3KCCAfv/gA/WcekcG1jF0bpSW0jFRExPi2Lnag/zKK3Rd3H+/5d+MRv2zUZyJVEx0dJBDxZliorsRGUnPJD0e17AwSmesraXjebKICeGIc4eY2L+fzADpZPqeQlAQXbPt7d1HTIhZMLm53WObWUz0REQyqILLu6ICqGvwMG8K3r+/uatLIiaUOlxYEwZqDyS9YuLoUeDqqy2L5OTo8aRER5M40ntz9vYm42L2bG3Ln3su7Xqhz3Jy1MWEnsiEMFYNBuWaCXsjE87oJy76za9aRcaOWhqKvKOTsyITwrATD2NHxMSsWfQ95AWtQ4ZQltpnnylvk73fhSF69zadN2ppTnV1dIxdPWnay4uE47JllimddXUUITkZIhN799L26a2XACzFRHo6RQ67G1dcQR219BAcTOKjooJm5JSXU4vrkwFxj2ptpWFyXSUm1q2znEzfUwgMpMxvD4/uJZjFdrOYYNxDZaVlPon4U4URTe0+aLtYIiZE3YRAITIhxV4xsXev9s5FosuPNa/98ePAoUM0X04LffvaF5kAyIjU6okMDqaOuyI60ZnmpIAeMSGdYSBPc1KLTNgSE44WXwtEv/n337feptCZkQmxHrkRb23ImRZv7OTJ1J1HicREKnhUIj/fNESQ0Y9ozRoUZCkWxHFTOs9dhUibkR/voiISrM7KYdaLVExs3Urnq5cd42fl14K96VLuZuJE4Kqr9L3Hw8PUHnbrVuoEfmKsktsRYkJzG3MnEBREz/qemOIEmDz8YWGuqxlwBRyZYNxLRQW5VZXSnIqbAQB1w083vShPwtUgJvbsUZ6mqSYmhg0jT0tmpravIMSENUNbTOrUeqFFR9tXgG0P0lQnZ6Y5CTGhNTJhK81JKT/dXpKTqc7AmpiQiyd7vfnStrpyQSI3kuRD62xFJqyhNoEYYDHhDOLjlSNl0shEVxXJenlR0FZ+DxL1Eu6qLZAOrnMkPUd6nYjBbz3VmFRCFGGfLClOApHfX11N56C/v+s/U6SHnkz7wZkEBpK/tDsY5VK603azmOhptLfTXWjcOOU0p0p6AtY1SpL/+/dXjEyoFdOOGkUPdSUjWM048/YGTjtNe6qTlsiE3mIxYXB2lZjYupU8qaWl1tOc8vLUu2NJsSYm7I1MOCvNCTA9iGyJCekxtTcyERdH4rSoSDkyIbqKAfrTnKzRrx/1NVAaZMdiwnESEpTFrTsiE4Cy2HdnvQRA51h9Pd2mf/nFfgEgFROHDtG1MHGi87bzZCcsjPbhyVZ0LDrSiXqJrhCtIhp4sqR6OZugIFNkojvRnbabxURPQ1iYCpGJ1lagtpnaLEkNKjN3qxg80Ls3SkuVi2l9famD0t69lh9fW6tunOnp6KQlMqHXoyQMzq4QE2efTUZHSgp5xdW8/6JLlnxuoBJSMaGlm1NEBOXcSpeT46w0J4COb3i49eFd8fG0XxobTYWs9kQmfH1pWFNOju3IhDPFREwMCQlxfkphMeE4iYnK54M7IhOAchqiOzs5AaaeGd99R57r0aPtX4+4TrZupXtWd2sL6wjh4cCvv9L94fTTbS/fVUjTnLoqlS48nI5/V0RB3EFgIDmBuoOHX0p32m4WEz2NykpyMYwcaeHyFjojOKDVfLCXNDIhKr9690Z6unoxbWws1SzIsWac6SnCLi0lI1xNTNgzqbMrIxO9elHf8k8+IYNEzbvk5UVedi2pTqWlpm5BoaG0r0UNipLHtndvqt1YvVp9nc6MTHh6Art3W/fyRUdTbvKxY/SwbG623zAThp7emgkxWBHQLyZ8fUnQyVOdRFoCiwnHuOsu4OOPLV93V2RCSUy4OzIB0Hn22WdUL2ZvDrj0OjnZUn26gvBw4Ouv7ZvR4UqkYqKrhsfNmgV89VXXfJY7CAwk51V3MMqldKftZjHR0xBTWYRVI7F6Kopb4I8GRESoRCZEVMLDAwgJUZ0XAFgO/RFoERNKtRZySkqoQ4makW3PpM6ujEwA9HBev149xUmgtQhbnuZkNJrSedQ8trNmAYsXq6/TmZEJ4MRwPit3FYPBlDpSXEyeMHu7a8THU9ev48f1RSYA0/lvTztMpbqJggISSd3hpn8y4+8PREVZvu6uyIRSmpO7IxMAOXPsbQkrkA4CPNXqJQC6VgsLTz4RJa2Z6KpnVUBAz56RI+773e3+3J22m8VET0PMixcub4mVWpF2DGGGKgSGepmLCenguvJyer+Hh00xodS21ZpxlpREnnCFWXoWlJRQ/m5OjrL4sCfPtSsjEwBtX1ubeicngbzDkRpSMREYSJEAIejUPLY33UTF8gcPKq/TmQXYWhHfV0QU7M0JTkgAdu2iKINaZEIMsBMGqCjEFlOynSUmRIpTdxv41V3gyIQ5wlfkiAAQtUX79lFbZ3vay3ZnhIF2sokoec0E4ziiwLw7GOVSutN2s5joaUjnxcuehBX78hHu34igIIO5mJAOrrPRyUlgj5jo1YvWpyXVSYiJhgbl/HR72hgKg7Or8lDPPJPSYrREJrSkOUnFhHzWhJrHNjSU5nUsWaK8Tsnh7jLEaWlv8bUgPp4mkwcFmX93qZhobKR/xd89POj/dXX0t/Z254oJxjW4s2aioIBS4wQnQ2SiXz9KeRw50v51iGnxGzZQuqi3t/O2rzsQFkb7QD5Pxt24I82ppyMcEN2hkFlKd9puFhM9DRGZACxi9BWHjiM8pL0zjGqGGFwnsS4PHqRCayXsERMAdYI6dMj21ygpoc2PirI0tAsLqcOB3kmdvXoBV15pO1LgLPz8gLvvph7m1tCS5tTeTodGOmFZKiaseWzvvJPyq0WdgMBodE9kQognR4e8JSRQHYl8HVIxUV9P/yqJDZEiJrw/WmEx0fUEB7snMtG3LxnZ0gYJJ0Nk4pxzgHvvdSwSJs777747+bzzXcH48bQP7ZnR4UpYTDif7pQuJKU7bTeLiZ6GlchE5dFKhEd6WXS0AWAaXHdCTFRXUz76mDHKH2OvmBCzHqzR0UFGbmSkcgrQ7t0U4bDHqFi7tms98W+/TREKa2hJcyovJ+NfKibCwmxHJgAyFPz8yAMppba2s9a+SxHf19HIhIj4yNchFRMNDRSNkHapEed/TQ3tF73Flywmuh5pZKIrxYSHB9UBieuztZXuTe6OTEydCrz8smPr8PUlobRjx8lXN9AVnHsuMH++u7fCEuHsq6piMeEsupNRLqU7bTeLiZ5GRYV6ZCK/AeGxAcpiQhaZ2LOHivykxqsUeWtSgS0xERlpW0xUVpInPiJC2WuflkbF3D2FhASKtkhTKeSUlJBBJW3dFxpqu2YCIIPozjstC7HLy6nuoqsfWGpdmPTSrx9tv63IRECAuQdXKibsKf5mMdH1SGsmujLNCTAX+6WlJOqVisS7I0FBdB9RcxoxXQ/XTDif7lR7IEVsN6c5MV1PZaVyZKK8HBV13ggfGNp5szJDFpmwZbDbG5mIiKAHsjVKSsgA7NVLuZtKTxMT0dHkJTx2TH0Zab2EQKQ5tbUBTU3Wjaw77gA2bqRogKCsrLPWvktJSKDT7MgRxyITYkKxtciEkvHpLDEhbQzAYsK1uCsyAZjXNBUVUVpgT6kvCAoCJk8mUc6cHHCak/PpTh5+KYGB5EDsDvM/WEx0E4xGYN48ZQPeDHlkQri89+9HRUB/hPX101Qz4SoxoSUyITWcT4XIhEilsFaEbU1MiLoAa0ZWfDwZDcuWmV5zdltYrYSH07ampTmeLpKQYDsyoSQmRM2EPWIiNpa630ini7OYcC1BQbTPKyq6PjIhvQc5mpp3shEcfGrWS5zMsJhwPt2pkFlKYGD32WYWE92EsjLg6aeB2247Mahs1y7ghx8sF5RGJkT1YG4uiYle/TsNOWdEJhoayCMuaGuj1xwVE6Wl5mJCamRXVdHvPUlMALaLsKUD6wQi1UypyFgJMXNCeNTdUXwNUMpRQgIZh44aZq+9Btxyi/lrolgXUBYTIjJnr5gICKDzX6Q6NTXRZcNiwnWIcH9xcddHJqRpTo6m5p1svPEGcPvt7t4KRoo75kz0dMLDgW++6X6zNM45R70T48kGi4luQnY2GY+HDgGvvw5g6VLgnXcsF5RGJqTVg/v2odI7Ul1MSCITTUEROHjQusEu9Iq0bkIYcFrEhLXBdSUlJsM5Pp7Sf8Sk5z17aHxGdwtX2sKWmLAWmairozQpW11JrrqKjKE//qDfnTn9Wi+ieNpRw2ziREsj3tVpToB53URBAWl2tfoixnH8/el2VlTknsiEcGj0tMjElCld1yqb0QbXTDgfgwG44oruNwfI35+aLXQHWEx0E7KzgcGDgVWrgP/8B0j53ce8X6FAGpkATE/C/ftRYQy1HploaAAyM3GgJg7BwaRD1PD2pvVIxURNDV2s1h72kZGUdWXx+RKkhnNcHHVQEYPuelqKk0CpNkSKrTQnLQaWvz9w880mT4e70pwA+r4Gg2s8RV0tJvLzKfWpq2tPTiUMBtOQNXfUTBw/TrfHnhaZYE4+OM2J6Y7w46+bkJNDBti4cRSavmnvv5BzzID6ejKYOtONpJEJgN6UlQUcOICKpgCEh5sbW50EBdGdq6oKaYVRGDPGtoqX100I48za+0JCSIhYS3WSGs6+vmSoCUO7p4oJeyITIs1JT1HqrFnAV1/ROePuyEREhGt6vAcF0UC6tjbX1EwAlmKCU5xcj0h16urIREQEpbbl5PS8yARz8hEYSMK1sZHFBNN9YDHhZl58kfK+bZGdbUoNuevaKlzasR4JVWkIDETnT+7hJlIV8shESgqMLa2orPFEWJhKZALotIjScsI0GexqYsIaBgM9nLWKCcA8ZzktrWe2MbQVmZDWkQikkQmtYmL8ePqsNWvcG5k47TRgyBDXrFsYnXV1rqmZAFhMuANxXLs6MmEwmK5PjkwwrkZ6fnMKGtNdYDHhZvLzqT7aFlIxYUg/hE8i/o0K7yhU7MxCRQUZhcVHToQbpGIiPh7YuRO1Qyegvd2gnuYEUN0EgLQMf5eJCcB2EbbccBZe+8ZGqhnpiZGJ/v1pn6jNmpDWkQikNRNavbUGg6kQ210F2ADlav/yi2vWLfZFbS15+LoizYnFhOtxV2QCMN2DODLBuBohJry9aagmw3QHWEy4maYm6/UDApHmBAA4dAiGUSMR1q8XwmqOISyMDMuaonp64kqboJ9QIBWDz4CnJxlP1iIT7QFB2Lffw61iQm44C6/ggQMU9j2heXoUkZGU8iNqQ+RYS3PSE5kAgJkzgdRUYN8+96U5GQyuK4bz8DClMnVVzQSLCdfjrsgEYIqOcmSCcTVeXiQiQkK6X8Ewc+rCYsLNNDUp1C/I6OggY1pEJnDwIDB8uKkDE8goqilusGxKfEKBVPQfjbAwUyGj4mf264fDIaejowMYOtT2toeHWxZgazHObA2ukxvOwiso6iV64g3W0xOIibGcrAxQtKK6WjnNqaGBjoEeb21EBHDZZe5Nc3I1olhXTMCW4syaCaORxURX4e7IxN69dL/myATjagIDuV6C6V6wmHAzWiITRUXUk7+zu9KhQ8CIEWbu0aAgoKa02bJnamQkEBCAiugRnX8KDKT1tbTIPqh/f6T5nIGkJG2FsWFhzo9MtLbSOq2JiZ6K1NstRQxHkxv+Ip+2oEC/t3bWLPrXXZEJVyMEsytrJhoaKM2MxUTXEBREotvXt+s/Oz4e2LGDOqIJUcMwroLFBNPdYDHhZrSIiZwc8lp3PkQPHTJFJk60hw0OBmrKWiwjEwYD8O23qBgwtvNPwvAUw846ufpq7Dnnfs0GuyvSnIThLE9zys8Hdu48NcVEQYFy5yM/P/rJz9fvrZ06FXjiCevtf7sz1sSEM9KcgoLovUePUttQFhOuJyiIjqU7IpMJCXQ+9e3bMyOjzMkFiwmmu8Fiws00NtoWE9LiazQ0kLoYPtzM+gwOBmoq25SnuV14ISprvTr/JNI+LD43NBRpx2PcKiZKS+km6uNjek0Yart2nZpiwuz4ywgNpffojUx4edHwQ3d4ebsCV4sJgI7Xzp1Uo8F59K5H1Hu5A2cNWWQYLYhO7QzTXWAx4Wa0RiY6jcmMDLrLREdbRiaqOiwjEyeQD8YWeeNSjEZ9qUSOiAm1mgmlQmMvL/qqAQGuayd6MqAmJsyOvwwhJtyRR34yY0tM1NbSj6Ni4o8/yFvt6enY9jK2EZEJdxAaSucK10swXQFHJpjuBosJN6OlADs7W9LJSRRfGwyWkYkag3JkApaz7JQ6OuXnUw74aadp23ZHCrDVIhNKYgIgYzopqWcbbfZEJsLC7KuZ6OnYEhOtrXTtOUNMcIpT1xAU5L7z3GCga5AjE0xXwGKC6W6wmHAzWiITZsakKL4GyF1fXg40NJCYqDNoikwAymJi717q4uTvr23bHSnALi2lLlVy1MREYiINXOvJWBMTnWJSRmgoGcYcmTDHVgG2wFExkZHBYqKrCA937xCvxEQ+1kzXEBbWc5tjMD0TDT17GFfS1EStP1tbzcdDSLFoC3v22fT/Pn2ouKCgAMHBg1HT4GU1MpGUZPpdLTKhZrQqER5OkYz2dooY6BET7e0U1ZDfMJUmPQPAvHk9OyoBkKFSVGR5LthKcwI4MiFHiAmloXX+/uRp9vR0bCiUMCzZwOwarrkGOO88933+O+9YthlmGFfw6qs9t56N6ZlwZMLNNDXRvxadlU7Q1kYTsqUD6zB8OP3fw4Msmbw8ikw0+ahGJiorzXWG0qyJ0lLLKcvWCA+nOovqavpdq5jo1YseykqpTkqTngHSTSpfrccQHU1GbnGx6TWLGSMyhJjgyIQ51iITHh70WnCwY515WEx0LX5+7t3XUVHcFpbpGiIjOc2J6V6wmHAzjY30r1rdhEh76d8fNBgiM9OU5gR05sYEBwM1zX4O1UyopRipERBAgRGR6qSnO47a4Dq929CT8PYmQSFNdRKRirg45ffI2/0yhFRMKHmTAwMdS3ECWEwwDMMwDMBiwq0YjabIhFrdRE4OCQkvL5CQ8PExtyxPdHQKDgZq2vwdqpnQa8gbDOZF2HrEhFp72FNZTACWdRMWM0ZkcGRCmaAgOufV6knEnAhHYDHBMAzDMFwz4Vba2iiNxWBQEROrViF7ZS/Ex19Kvx86BAwbRnkaAhGZCOxATUcgoByYcImYAEztYTs69LXaZDGhjFxMWCu+BrhmQo3gYBomByiLicBAxwVYaCgwbRpdkgzDMAxzqsKRCTciohK9e6uIiTffRM7qv5DQ+Df9LtrCShGRCc961CAYxlDLyERTE6VTSYMWzqiZAEwdnerrKdLiqJiwZxt6EkqRCbV6CcB0TDkyYU5QkKn2RE1MOBqZMBiAH37grisMwzDMqQ2LCTcixESfPgo1E2VlwB9/IPu8O5CQ9jWwerV58bVARCbaK9EKHzT7WlpIIg1JKiacHZmoqaHftRYoKg2ua2yk/cCRCdPv1mZMAByZUCMoiM57X1/lLmDOEBMMwzAMw7CYcCtSMWERmfjhB2D0aGS3xyF+zgzgjjuAn34yL74GTJGJ1nIAQE2tZXuaigoyrqTtRuVior2d9IsjYqJXL+3tW5UG15WWkrf3VPb02pvmxJEJc4SoVdsvzqiZYBiGYRiGxYRbaWoCfL3aEFSVaykm1q8HZsygNJerxwILF5L1PXKk+XL9+gEVFfAtzYePoaUzQiBFXi8BWIqJ8nJKU9KbYiQVE3qMM6U0p5ISElY9fZ6ENWJjOc3JGdgSE4GB3OaTYRiGYZwBiwk30tgI+BuaEHgkDXWVraY/tLYCP/yA5qmXoaDghDF5993AkSPAkCHmK4mIoA5P+/dT3YQOMSFNrSopIeNK7xAv0c3JGWJCbWDdqUS/fkBhIUWKxIwRa2IiIQFYuZIHHMmxJSYefJAuKYZhGIZhHIO7ObmRpibAD00IbK5AbeoBAGPpD7//Dvj5IbfPOHh7A337nnhDYqLlSgwGskD37UOwd5OimJAPrAPI2JJGJuw15EUBtrMiE6dy8TVAbWDb2mhftLTQa9Zaj3p4ANdd1zXb1p2wJSZGj+66bWEYhmGYngxHJtxIUxPg196AwJFxqPt9D+UZAZTiNH06cnI9MGCAeSdYRfr3JzHh16wamZCPn5CnOdnbktXeNKeICHpfW5vj29CT8POjfZOfT/USnTNGGF34+tJ+UxpYxzAMwzCM82Ax4Uaaalrg11GPoPPGk2G/ZQv94US9hK1OPp306wdkZCDYv9Xumgl3iAmAir4d3YaehijC1nz8GQsMBopOcC0JwzAMw7gWFhNupCm/DH6GZgTGBqNu8DhgwQKqi8jOBi68ELm5wIABGlbUvz/Q0YHgXh0OiQl7UozsFRO+vkBIiHmqU1YWpfmc6ggxkZNjvZMTYx0WEwzDMAzjelhMuJGmgnL4+RgRGOSB2tihQEoK8N//AuefDwQFoaxMo4F/Iqk+OEi7mJAPrbO3ZkIUYFdX62+1Ka2bqKgANmwArrpK/zb0NDgy4RxYTDAMwzCM62Ex4UYai6rg738iStDqB8ycCXz8MTBjBgDlWgdF+vcHAASHeGguwA4MpKZRosjXkTSnlhaaNmyPmBCD6778Ehg3Dhg2TP829DSkYoIjE/bDYoJhGIZhXA+LCTfSVFwFv16eps5Kc+dSm9fp0wEoiwBFRGQiTFlMqBVgA6ZUJ3vFREgI5afn5OgXE9LBdYsXA7Nm6f/8nog0zYkjE/bDYoJhGIZhXA+LCTfSVFoLvyBvU/3CqFFkXQ8cCEA5PUkREZno7a05zSkggESAVEzYUzPh4UFTmO0REyLNae9eID0duP56/Z/fE+nXj+pHOmeMMHbBYoJhGIZhXA83nXQjTeX18OvjY14MHRLS+XfNYqJPHyAkBMF9e6Fmr+WfS0uB3r3NXzMYzAfXOdJJKTwcOHrUPjFRXAwsWUKzEvS+v6fSrx8JCR8fyYwRRjc33MAF/QzDMAzjalhMuJGmygb4JfpbTKMWaBYTBgNw9CiC14Wj5lfzP9XX07qVjFIhYlpaqIDaETGRlWWfmPjrL2DnTuDrr+377J5IbCz9q2nGCKMKR7oYhmEYxvWwqeIuOjrQVN0Cv94BFtOoASqOrqnRWIANAOHhCA6GRZpTcTHg7a0sSoSYEEXQffro/hbiowHYVzOxeTN9x3PPte+zeyK9etE+4RQnhmEYhmFOdlhMuIuSEjS2e8O/TyACAyk6IDorAUBVFf2rKTJxAjUxERWl7OEWYqKkhNKg7J20LASPPZGJ9nbgzjspuMKY6NePOzkxDMMwDHPyw2LCXeTmosk/DH6BXp2dlerrTX+urAT8/AB/f+2rVBITRUXqefdi1oS9xdcCeyMT0dEkcm67zf7P7qnExQGDBrl7KxiGYRiGYazDNRPuIjcXTQG9OwWDwUCGvfDya66XkKAWmYiOVl5eWvhtb70EYL+YGD6cOjmd6GzLSHjvPbNafIZhGIZhmJMSFhPu4tgxNPmfCT8/8s736mVeN2GvmGhspHoLb296zVpkQoiJhgbniImgIH3vMxioGy5jyYluvwzDMAzDMCc1nObkLnJz0eQbAj8/+lVehG2vmADMO0NpiUw40hYWoO309aUfhmEYhmEY5tSBxYS7OHYMTd6BnWLCbNYElKdW2yIggKIc0lSnrqqZ4BkRDMMwDMMwpx4sJtxFbi6aPAM7C6zlsybsiUwYDJZ1E10RmRg2DDjvPPvfzzAMwzAMw3RPWEy4i9xcNMJfNTJRWalfTACWYkJLzURpqWNiYvBgYNUq+9/PMAzDMAzDdE9YTLgKo1H9b/X1QHk5muDr1JoJwFxMtLdT1MHVkQmGYRiGYRjm1ITFhCvIzwdiYqitkhK5uUBAAJpaPa3WTDgqJkpLgY4O62LCGTUTDMMwDMMwzKkJiwlXkJlJxQqHDin/PTcXiItDU5PBTEzIayb0FmAD5mKiuJjWodZlKSgIOH7c8dawDMMwDMMwzKkJiwlXUFBA/+7erfz3Y8eAAQPQ1ASXRias1UuIz8zOBjw97RMuDMMwDMMwzKkNiwlXIMTErl3Kf++MTKCzm5O8ZsIZBdjWOjkBJCYaGynFyYPPBIZhGIZhGEYnbEK6goICyhtSExPHjgFxcWhsVI5MGI1dF5kAuF6CYRiGYRiGsQ8WE66goACYMQPYs4daKsnJzUV7/3i0tkKxZqK2lt7m6shEUBD9y/USDMMwDMMwjD2wmHAFhYXAlCn0//R0y7/n5qI5egAA5chERQWlHQljXw/2RCZYTDAMwzAMwzD2wGLCFRQUAHFxwJgxlkXY7e1Afj6aIuMAQHHOhOjkZE8dg96aCYDFBMMwDMMwDGMfLCacTUcHRSZiY4Hx4y3rJtLTAS8vNIXHAFCPTNiT4gRYRiasiQl/fxIsXDPBMAzDMAzD2AOLCWdTWgq0tdHQOiUx8eWXwGWXoandG97e1JYVMBcT9nZyAiwjE9bSnAwG+lyOTDAMwzAMwzD2wGLC2RQUkBLw8yMxkZZG0QqA2jQtXw7MnGk2YwIwL8B2RmSithaor7cemRCfy2KCYRiGYRiGsQcWE85GpDgBwPDhVCNx+DD9vn07UF0NXHKJWVtYwPlpTsXFgI+P7WF0d98NTJhg32cxDMMwDMMwpzYsJpxNQYFJTHh5AaNHm4qwly8HrrsO8PGxiEwEBQGtrUBLi6kA2x6CgykqUVBAUQmDwfryL7xg2lyGYRiGYRiG0QOLCWcjFROAqW6itRVYuRKYORMAFNOcAIpOOBqZAIDMTOv1EgzDMAzDMAzjKCwmnI1cTIwbR5GJTZuAXr2Ac84BYCkm/P0pilBb65iYEKIkI8N2vQTDMAzDMAzDOAKLCWdTUECdnATjx5OYWLYMuPnmzuERTU0kIASis1JdnWPdnDw9aT2HD3NkgmEYhmEYhnEtXu7egB6HtAAbAEaOJOWwejWwd2/ny/LIBGAaXOdIZEKsJyODgiIMwzAMwzAM4ypYTDgbeZqTtzeQlESV1aNGdb4s7+YEmCITjhRgA1Q3kZXFkQmGYRiGYRjGtbCYcCaNjaQE5O2Rrr/eYsy0UmRCzJpwNDIRHExz87hmgmEYhmEYhnElLCacSWEhRSL69DF//fHHLRZVExOlpaRJHBUTAEcmGIZhGIZhGNfCBdjOpKCALHgP27tVrWYiL4/+72iaE8CRCYZhGIZhGMa1uC0y8cUXX+Drr79GXV0dxo8fj0cffRThKu743NxcvPvuu/j777/h4eGBCy64ALNnz4a3t3fnMqmpqfjggw9QWFiI+Ph4PPzwwxgxYkRXfR1CXnxtBXk3J4AiE3l5JCokX003QkxERdm/DoZhGIZhGIaxhVsiExs3bsSyZcvw4IMPYtGiRaivr8eLL76ouGxjYyOeeOIJhISE4J133sErr7yCPXv24L333utcJjc3F88//zwuuOACfPjhhxg1ahSefPJJVFdXd9VXIuTF11ZQS3M6dsyxqARAYqJ3b8DHx7H1MAzDMAzDMIw13CIm1q5di2uuuQaTJ09GYmIinnjiCezbtw+ZmZkWyx44cAAVFRV49NFHERcXh1GjRuHuu+/G+vXrUV9fDwBYt24dhgwZgltvvRXx8fF44IEHEBAQgM2bN3ftF9MhJtS6OeXmOlYvAZCY4HoJhmEYhmEYxtV0uZhoaWlBVlYWxo4d2/laTEwMoqOjcfDgQYvlW1tb4enpCS8vU0aWr68vWltbcfjwYQBAeno6xkmGKhgMBowdOxaHDh1y4TdRQD6wzgpqkYm8POeICa6XYBiGYRiGYVxNl4uJmpoadHR0IEyWyxMaGoqqqiqL5YcPHw6DwYClS5eitbUVlZWV+PzzzwEAFRUVAICqqiqEhoaavS8kJASVlZUu+Q6qOJjmFBRE4ygcFRPXXAP8+9+OrYNhGIZhGIZhbNHlYsJoNOpaPiwsDM8++yw2btyIiy++GDfeeCPGjBkDgCIQ9qzTZTihZgJwXEwMGgRMmeLYOhiGYRiGYRjGFl3ezSkkJAQeHh4WUQOl6IJg0qRJWLVqFSoqKuDv74/S0lIsWbIEfU8UBoSFhVlENaqrqy2iH3Keeuop+JyoUp42bRqmTZtm35cCAKPRKd2cAMfFBMMwDMMwDMNs2rQJmzZtAkClBq6gy8WEj48PBg0ahD179mD8+PEAgKKiIhQXF9ts5Spax27btg19+vTBkCFDAADDhg1DWlqa2bJpaWm49tprra5v3rx5CBZ9VB2lvJxylBysmQAc7+bEMAzDMAzDMFJneU1NDd555x2nf4ZbujldeeWVWLNmDX799VdkZmZi/vz5SEpKQmJiIkpLS3HbbbeZFU9///33SE9PR25uLlasWIFly5Zh9uzZ8PT0BABcdtllyMjIwPLly3Hs2DEsWrQIDQ0NuOiii7ruSxUUACEhQK9eKCoCQkNJLIifDz4wX1ytZgLgyATDMAzDMAzTPXDL0LpLL70UlZWVeOONNzqH1j322GMAgPb2duTl5aG5ublz+ZycHHz00UdoaGjAgAED8Oyzz2Ly5Mmdf4+Li8OLL76I999/H59++ini4+Px2muvISQkpOu+lKRe4vBhEgY7d9KfXn8d+Ptv88XVWsMCLCYYhmEYhmGY7oHbJmDPnDkTM2fOtHg9OjoaKSkpZq/NmTMHc+bMsbq+SZMmYdKkSU7dRl1IxER+PjBgADB4MP1pyBBg927zxV1ZgM0wDMMwDMMwXYFb0px6JJLi6/x8oF8/058iI4GSEvPFWUwwDMMwDMMw3R0WE85CFpmQiomICGUxIe/mJGomuACbYRiGYRiG6Q6wmHAWkunX9kYmwsKAe+7h6dUMwzAMwzBM98BtNRM9DiuRichI6hzb3g6caEClKCa8vCy7PjEMwzAMwzDMyQpHJpyFjTSnjg6gooJ+NxqVuzkxDMMwDMMwTHeCxYQzaG4GysqA2Fi0tADHjwP9+5v+HBAA9OplSnUSAwhZTDAMwzAMwzDdGRYTzqCoiPKXIiNRVAR4eFjWPUjrJpqa6F8WEwzDMAzDMEx3hsWEMygoIPXg6Yn8fKBvX1NthCAyEigtpf8LMSHv5sQwDMMwDMMw3QkWE87ASr2EQB6Z8PCggmuGYRiGYRiG6a6wmHAGdogJPz/AYOjCbWQYhmEYhmEYJ8NiwhloEBPSwXXcyYlhGIZhGIbpCbCYcAaFhZ1ioqBAPTIhrZlgMcEwDMMwDMN0d1hMOAMr068F8jQnLr5mGIZhGIZhujssJpyBnTUTDMMwDMMwDNOdYTHhKEZjp5hob6eMJ1s1EywmGIZhGIZhmJ4AiwlHqawkdRAbi+PHgY4OmjMhJzISqKqi6dcsJhiGYRiGYZieAIsJRyksBIKCgKAg5OcDUVGAj4/lYn360L9lZSwmGIZhGIZhmJ4BiwlH0VB8DZDACAujVCduDcswDMMwDMP0BFhMOIqG4muBqJvgbk4MwzAMwzBMT4DFhKPoEBOioxOnOTEMwzAMwzA9ARYTjqJTTJSWsphgGIZhGIZhegYsJhylsFBTzQTAkQmGYRiGYRimZ8FiwlHsrJlgMcEwDMMwDMN0d1hMOMoJMdHRQf/VEpngbk4MwzAMwzBMT4DFhCO0tpI6iI1FWRkNpDsRpFBEmubE3ZwYhmEYhmGY7g6LCUcoKqJ/o6ORn0+D6axFHLgAm2EYhmEYhulJsJhwhMJCGnnt5WWzXgLgAmyGYRiGYRimZ8FiwhF0FF8DVIBdXw+Ul7OYYBiGYRiGYbo/LCYcQaeYCA8HPDyAvDwWEwzDMAzDMEz3h8WEI0jERG6u9eJrAPD0pLqKwkIWEwzDMAzDMEz3h8WEI0jExI4dwNixtt8SGQm0t3M3J4ZhGIZhGKb7w2LCEQoKgJgY5OUB2dnAuefafktEBP3LkQmGYRiGYRimu8NiwhEKC4HYWKSkABMmAMHBtt8SGUn/sphgGIZhGIZhujssJuzFaOxMc0pJAZKTtb2NxQTDMAzDMAzTU2AxYS81NUB9PYwxsdi6FZgyRdvbWEwwDMMwDMMwPQUWE/aSlwcEBCC7IgRFRcDZZ2t7G9dMMAzDMAzDMD0FFhP2sm0bMGkSUrYZcMYZQECAtreJyAR3c2IYhmEYhmG6Oywm7GX9emDGDF0pTgCnOTEMwzAMwzA9BxYT9lBXB6SkwDh9hq7ia4DFBMMwDMMwDNNzYDFhD1u2AAkJONyRiIoK4Mwztb81IQF44w3A19dlW8cwDMMwDMMwXYKXuzegW7J+PTB9OrZupcJrPVEGLy/goYdct2kMwzAMwzAM01VwZEIvHR3A998DM/SnODEMwzAMwzBMT4LFhF527wYaGtBx1jnYto3FBMMwDMMwDHPqwmJCL99/D1x8MY7keKO2Fjj9dHdvEMMwDMMwDMO4BxYTejnRErasjDoz+fi4e4MYhmEYhmEYxj2wmNBDURGQlgZccglqa4HAQHdvEMMwDMMwDMO4DxYTetiwATjjDKBPH9TVsZhgGIZhGIZhTm1YTOjhu++A6dMB0Ny6oCA3bw/DMAzDMAzDuBEWE1qpqAB++AG4/noA4MgEwzAMwzAMc8rDYkIrq1cDY8cCiYkAwDUTDMMwDMMwzCkPiwmtLF8OzJzZ+StHJhiGYRiGYZhTHS93b0C3IDcXSE0FVq3qfIlrJhiGYRiGYZhTHY5MaOHLL4ELLqDBEifgyATDMAzDMAxzqsNiQguff26W4gRwzQTDMAzDMAzDsJiwxb59wNGjwJVXmr3MkQmGYRiGYRjmVIfFhC2WLweuuMJCOXDNBMMwDMMwDHOqw2JCRlsbYDSe+KWjg+olbrnFYjmOTDAMwzAMwzCnOiwmZFxxBbBu3Ylf0tKoOOKiiyyW45oJhmEYhmEY5lSHxYSMsjKgsPDEL6WlQEwM4O1tsRxHJhiGYRiGYZhTHRYTMlpaKOoAAKivV1UMLCYYhmEYhmGYUx0WEzLMxERdHdCrl8UyHR2kM7gAm2EYhmEYhjmVYTEhw0JMKIQfGhqoSJsjEwzDMAzDMMypDIsJGVrERF0d/asQtGAYhmEYhmGYUwYWEzJaW2U1EwqKoa4O8PGhH4ZhGIZhGIY5VWExIUNrZILrJRiGYRiGYZhTHRYTMrSICZ4xwTAMwzAMwzAsJiywaA2rkubEYoJhGIZhGIY51WExIcFoJDFRU3PiBStpTiwmGIZhGIZhmFMdFhMS2ttJUHDNBMMwDMMwDMPYhsWEhJYW+tdWmhPXTDAMwzAMwzAMiwkzhJhobqYWsZzmxDAMwzAMwzDqsJiQIMQEcCI6wWKCYRiGYRiGYVRhMSFBiAmD4YSYqK/nmgmGYRiGYRiGUYHFhISWFsDbm/RDZ2SCayYYhmEYhmEYRhEWExJaWgAfH4o61Fa1A01NnObEMAzDMAzDMCqwmJBgJiZKm+hFFhMMwzAMwzAMowiLCQlmYqKsmV7kCdgMwzAMwzAMowiLCQmiZiIoCKgtbwH8/ABPT4vluACbYRiGYRiGYVhMmNHaKolMVLaqhh+4AJthGIZhGIZhWEyYYZbmVNmumOIEcJoTwzAMwzAMwwAsJsyw6OakohhYTDAMwzAMwzAMiwkzzMREjVFRMXR00Cw7rplgGIZhGIZhTnVYTEgwExO1UExzqq+nfzkywTAMwzAMw5zqsJiQIBUTNXUG1RkTgGo5BcMwDMMwDMOcMrCYkCDERHAwUFvvqSomfH2phSzDMAzDMAzDnMqwmJBglubU6Kk6sI7rJRiGYRiGYRiGxYQZZmKiyVsxMsEzJhiGYRiGYRiGYDEhwUxMNPuopjmxmGAYhmEYhmEYFhNmmImJFl/VNCcWEwzDMAzDMAzDYsIMMzHR6gdjL+XIBNdMMAzDMAzDMAyLCTOkYqIDnmj0CbFYhmsmGIZhGIZhGIbwctcHf/HFF/j6669RV1eH8ePH49FHH0V4eLjistnZ2Xj33Xdx6NAheHp6YvTo0ZgzZw6ioqIAAHv27MHcuXPN3tOrVy+sX79e1zZJxQQA1HqGIkC2DKc5MQzDMAzDMAzhlsjExo0bsWzZMjz44INYtGgR6uvr8eKLL6ou/8wzzyAwMBDvvvsu/ve//6Gurg7/+c9/LJZbuXIl1qxZgzVr1mDZsmW6t6ulheZH+PoC3mhFrSHYYhkWEwzDMAzDMAxDuEVMrF27Ftdccw0mT56MxMREPPHEE9i3bx8yMzMtlq2qqkJhYSFmzpyJuLg4JCYm4tprr8Xhw4ctlg0PD+/8CQsL071dra0UmQCAII861MKyOIJrJhiGYRiGYRiG6HIx0dLSgqysLIwdO7bztZiYGERHR+PgwYMWywcHB6Nfv3748ccf0dLSgsbGRvz000+YMGGCxbK33HILrrvuOjzzzDPIzc21Y9skYgK1qDXynAmGYRiGYRiGUaPLayZqamrQ0dFhETkIDQ1FVVWVxfIeHh6YP38+nnrqKaxZswZGoxFDhw7F/PnzO5cJDw/HY489hqFDh6KhoQErV67EAw88gKVLl+qKUJiJCWMNatst38tpTgzDMAzDMAxDdHlkwmg06lq+o6MDb7zxBgYMGIB33nkHb775JgICAsxqJuLi4jB9+nQkJiYiKSkJL7zwAgIDA/Hjjz/q+qxOMdHWdkJMyMuvWUwwDMMwDMMwjKDLIxMhISHw8PBAZWWl2etVVVUIDQ21WD4tLQ1paWlYt24dfE6EDf7973/juuuuw9GjRzFw4ECL93h5eWHgwIEoKiqyui1PPfVU5zqnTZuGlpZpJCbq6ynNqc3f4j0sJhiGYRiGYZjuwKZNm7Bp0yYAVGrgCrpcTPj4+GDQoEHYs2cPxo8fDwAoKipCcXExRowYYbF8U1MTDAYDPDxMQRTx/46ODsXPaG9vR05ODkaOHGl1W+bNm4fgYFPHpjffPBGZqKsjMdHsY/Ge2louwGYYhmEYhmFOfqZNm4Zp06YBoFKDd955x+mf4ZZuTldeeSXWrFmDX3/9FZmZmZg/fz6SkpKQmJiI0tJS3HbbbTh06BAAYOTIkfD29sb//vc/5ObmIisrC//3f/+HmJgYDBgwAACwevVqpKamoqCgAJmZmXj11VdRVVWFCy+8UNd2daY51dUhyLMR/9/evcdFXed7HH/PIMh9BDERDDFIiVpTMW3LrDaNvK675CmP2s1cs02rhTzadrPtYnlDw0u67sn1UnkSt83NoItle7Sjx7xsaXokFVJQKi6J6HCZ8wfOxATK/H4Cg/F6Ph48gN9tPoO/xzjv+X6+v1/pybp/HkYmAAAAgBpeuWndkCFDVFRUpPT0dNdN69LS0iTVjCrk5eXpzJkzkmomZs+cOVNLly7Vgw8+KB8fHyUmJurFF1+Ur6+vJKmiokIZGRkqLCxUcHCwunfvrvT0dEVERBiqyxUmysoU6leuH36ouw1hAgAAAKjhtTtgjxkzRmPGjKmzPDIyUps2bXJbduWVV2r+/PnnPNbo0aM1evToC67JbWTCz64ThAkAAADgnLzS5tRSuYUJ/4o6IxNVVdKpU8yZAAAAACTChBu3MBFQWSdMlJXVfGdkAgAAACBMuKk9ZyIksKpOmDh5suZ7UFCzlwYAAAC0OISJWtxGJoId9YYJf3+pjddmmgAAAAAtB2GiFrtd8vVVTZgIUZ0wwT0mAAAAgB8RJmpxa3MKtdQ7MsF8CQAAAKAGYaKWiopabU6ECQAAAOC8CBO1uM2ZCGujU6dqLgfrRJgAAAAAfkSYqMWtzSmsZpa18wpOEnMmAAAAgNoIE2dVVUnV1T+OTASH+0lyn4TNyAQAAADwI8LEWXZ7zXdnmPAJDVJgIGECAAAAOBfCxFluYaKsTAoOrnN5WMIEAAAA8CPCxFk/HZlQUBBhAgAAADgPwsRZzjDhvGmdgoMVGiqVlv64DROwAQAAgB8RJs6y26U2bSSrVbQ5AQAAAB4gTJzluiysVG+bk8Mh7d8vderktRIBAACAFoUwcZYrTFRU1Pzyk5GJHTukI0ekYcO8WiYAAADQYhAmzqp9wzpJdcLEX/4i3XmnFBTktRIBAACAFqWNtwtoKVxhwnnL68BAV5goL5fWrJE2bvRqiQAAAECLwsjEWXZ7rSs5BQZKVqsrTKxfXzNX4tprvV0lAAAA0HJ4HCZWrFihgoKCpqzFq9xGJs5esskZJv7yF+m++ySLxbs1AgAAAC2Jx21OO3bs0F//+lddddVVuvXWW3XTTTcp6Gc0gaCiotacibPPKyRE+uIL6euvpVWrvFsfAAAA0NJ4HCYWLFig/Px8ZWdn680331RGRoauu+463XrrrbrmmmtktV7cHVPnGpnYv18aMUKKjPRufQAAAEBLY2gCdqdOnXT33Xfr7rvv1t69e5WVlaUnn3xSISEhGjhwoIYPH67OnTs3Va1N6lxhQqppcQIAAADgztRwQn5+vrZv364dO3bI399f/fv31zfffKP77rtPr7/+emPX2CzcLg17NkxERUmdO0tDhni3NgAAAKAl8nhk4uTJk/r444+VlZWlffv2qU+fPho/frz69+8vX19fSdKWLVv0/PPPa/To0U1WcFNxG5k4O2eiV6+a+RJnnx4AAACAWjwOEykpKerUqZOSk5P1zDPPqH379nW2ufrqq9WtW7dGLbC51NfmJBEkAAAAgHPxOEzMmzdPiYmJ590mKChI8+bNu+CivKG+NicAAAAA5+bxnAmr1aq9e/fWWb5v3z7t37+/UYvyhvranAAAAACcm8dhYt68efruu+/qLP/uu+8u2tGI2s7V5gQAAACgfh6HiSNHjig+Pr7O8ri4OB05cqRRi/IG2pwAAAAAYzwOE0FBQTp27Fid5ceOHZO/v3+jFuUNtDkBAAAAxngcJgYMGKBXXnlF//d//+daduDAAS1YsEA33nhjkxTXnAgTAAAAgDEeX81p4sSJmj17tiZOnKiAgABZLBaVl5frV7/6lSZNmtSUNTYLu13y95dUUSG1bevtcgAAAIAWz+MwYbfbNW3aNI0fP16HDx+Ww+FQ165d1alTp6asr9nY7VJIiKTKSsnHx9vlAAAAAC2eR2GiqqpKKSkpWr58uWJiYn42AaI2V5tTVRVhAgAAAPCAR3MmfHx81LlzZ508ebKp6/GaiopaYaKNxwM2AAAAQKvl8QTsSZMmadGiRfrXv/6l8vJyVVdXu31d7FwjE7Q5AQAAAB7x+CP4adOmSZIeeeSRetd/+OGHjVKQt9DmBAAAABjjcZiYO3duU9bhdW5hgjYnAAAAoEEev2vu2bNnE5bhfbQ5AQAAAMYY/gi+sLBQJ06cUGVlpdvyq6++utGK8gbanAAAAABjPA4TBQUFevbZZ/XVV1/JYrHI4XDIYrG41jNnAgAAAGhdPL6aU3p6uiIiIvTWW2+pbdu2+vOf/6x58+YpISFBL7/8clPW2Czc2pyYMwEAAAA0yOMw8eWXX+q+++5TeHi4rFar2rRpox49euiBBx5QRkZGU9bYLBiZAAAAAIzxOEz4+PiozdlP7MPCwlRQUCBJCg0Ndf18MSNMAAAAAMZ43M+TkJCgL774Qp07d1ZSUpJeffVV5eXlafPmzbr88subssZmQZsTAAAAYIzHIxMPPvigunbtKkmaMGGCEhIS9I9//EMhISH6j//4jyYrsLkwMgEAAAAY4/FH8DExMa6fg4KC9NhjjzVJQd5CmAAAAACM8ThMHDt27Lzro6KiLrgYb7LbJV9f0eYEAAAAeMjjd81jx451u6/ET3GfCQAAAKB18ThMrFmzxu33qqoq5eTkaNWqVbr33nsbvbDmVlEh+bWplhwOwgQAAADgAY/DRGRkZJ1l0dHRCg0N1ZIlS/TLX/6yUQtrTlVVNV9+PlU1C2hzAgAAABrk8dWczsVmsyk3N7cxavGaioqa764wwcgEAAAA0CCPP4L//PPP3X53OBz6/vvvlZmZqYSEhEYvrDnZ7TXfCRMAAACA5zwOE2lpaW6/WywW2Ww29ejRQw8++GCjF9acXGHCWlnzA21OAAAAQIM8ftf80UcfNWUdXuUME75WRiYAAAAAT13wnImfA7u9Jj/4iDABAAAAeMrjMPHUU0/pjTfeqLN87dq1evrppxu1qObmdo8JiTYnAAAAwAMeh4ndu3erX79+dZb37dtXu3fvbtSimpsrTFSenTPByAQAAADQII/DhN05seAnHA6HTp8+3WgFeYPbyITFUvMFAAAA4Lw8DhNXXHGFMjMz6yxft27dz+LSsK4wQYsTAAAA4BGP3zlPnDhRaWlp2rt3r66++mpJ0p49e3TixAnNnj27yQpsDm5tTrQ4AQAAAB7xOEx0795dq1atUmZmpg4fPiyHw6H+/fvrN7/5jWw2W1PW2OTsdsnXVzUjE4QJAAAAwCOGenpsNpvuvffepqrFa2hzAgAAAIzzeM7Eu+++q08++aTO8k8++UTvvfdeoxbV3GhzAgAAAIzzOEysXr263nam8PBwrV69ulGLam4VFbVGJggTAAAAgEc8DhOFhYXq2LFjneURERE6ceJEoxbV3GhzAgAAAIzzOExccskl2rNnT53lu3fvVkRERKMW1dxocwIAAACM8/hj+N/85jd65ZVXVFpa6ro07K5du/TXv/5V99xzT1PV1yzcRiYIEwAAAIBHPA4TKSkpatu2rVatWqXFixdLqhmtmDRpkoYOHdpkBTYH2pwAAAAA4wy9cx42bJiGDRum8vJyORwOBQYGNlVdzYo2JwAAAMA4Ux/DBwQENHYdXkWbEwAAAGCcx2GiurpaGzZs0CeffKLCwkJVVla6rV+zZk2jF9dcaHMCAAAAjPP4ak6vvfaaVq5cqT59+uj48eNKTk5Wz549VVZWppEjRzZhiU2PNicAAADAOI8/hs/OztZjjz2mvn37auXKlRo4cKCio6P197//XTt27GjKGpscbU4AAACAcR6PTJSUlCgmJkaSFBQUpNLSUknSNddco+3btzdNdc2ENicAAADAOI/DROfOnXXs2DFJUmxsrN577z2VlZXpo48+UkhISJMV2BxocwIAAACMM3TTuvz8fEnS3Xffrccff1wbNmyQj4+PUlNTm6zA5mC3S76+os0JAAAAMMDjMDFkyBDXz1dddZXefPNN5ebmqmPHjmrXrl1T1NZsaHMCAAAAjDP9zjkgIEDdu3dvzFq8hjYnAAAAwDiP50z8nFVUcDUnAAAAwCjChGhzAgAAAMwgTIg2JwAAAMAMwoS4aR0AAABgBmFCtDkBAAAAZhAmRJsTAAAAYAZhQrQ5AQAAAGYQJkSbEwAAAGAGYUK0OQEAAABmECZEmxMAAABghtd6etasWaPMzEydPHlSSUlJSk1NVXh4eL3bHjp0SIsWLdK+ffvk4+Ojq6++Wr///e/VsWNH1zZbt27Vq6++qmPHjik2NlaPPPKIEhMTPaqFNicAAADAOK+MTGzcuFErV67UlClTlJGRobKyMs2YMeOc2z/xxBMKDg7WokWLNGfOHJ08eVLPPfeca31ubq6efvpp3XLLLVq6dKmuuuoqTZs2TSUlJR7VQ5sTAAAAYJxXwsT69euVkpKiAQMGKD4+XlOnTtWePXt08ODBOtsWFxfr2LFjGjNmjGJiYhQfH6/bb79dBw4ccG3zzjvvqFu3bho3bpxiY2M1efJkBQYG6v333/eoHrtd8vUVbU4AAACAAc0eJux2u3JyctSrVy/XsqioKEVGRmrv3r11tg8NDVXnzp2VnZ0tu92u8vJyffjhh+rTp49rm6+++kq9e/d2/W6xWNSrVy/t27fPw5qYMwEAAAAY1exhorS0VNXV1QoLC3Nb3q5dOxUXF9fZ3mq1atasWfrf//1fDR48WEOHDtWxY8c0ffp01zbFxcVq166d2342m01FRUUN1lNdXZMhXG1OzJkAAAAAPNLsYcLhcBjavrq6Wunp6erSpYsWLlyo+fPnKzAw0G3OhNFj1lZRUfOdkQkAAADAmGb/GN5ms8lqtdYZNahvdEGSdu7cqZ07d+qdd96Rn5+fJGn69OkaNWqUvv76a1122WUKCwurM6pRUlJSZ/Tjpx5//HFJNcf89NNk/ZYwAQAAgJ+JrKwsZWVlSaqZatAUmj1M+Pn5KS4uTrt27VJSUpIkKT8/XwUFBfVeyvX06dOyWCyyWn8cRHH+XF1dLUlKSEjQzp073fbbuXOnbr/99vPW8sILL6iiIlQLF0qDB0va8CZtTgAAAPhZSE5OVnJysqSaqQYLFy5s9MfwytWcRo4cqXXr1unTTz/VwYMHNWvWLPXo0UPx8fEqLCzUXXfd5Zo8feWVV8rX11dz5sxRbm6ucnJyNHv2bEVFRalLly6SpOHDh2v//v1avXq1jhw5ooyMDJ06dUqDBg1qsBZnSKPNCQAAADDGKx/DDxkyREVFRUpPT3fdtC4tLU2SVFVVpby8PJ05c0ZSzcTsmTNnaunSpXrwwQfl4+OjxMREvfjii/L19ZUkxcTEaMaMGVqyZIlWrFih2NhYzZw5UzabrcFa7HbJaj2bIQgTAAAAgMcsmzZtMj97+SJVVlamYcOGqaSkRMePh6pHD6m8XNKdd0pJSdJjj3m7RAAAAKDRlJaWymazacOGDQoKCmq043qlzaklcd1jQmJkAgAAADCAMEGYAAAAAEwhTNQOE9y0DgAAAPAYYYKRCQAAAMAUwgRhAgAAADCl1YeJykrJ37/WL7Q5AQAAAB5p9WEiOVnavfvsL4xMAAAAAB5r9WHCDWECAAAA8BhhoraqKtqcAAAAAA8RJmqrrGRkAgAAAPAQYaI22pwAAAAAjxEmaqPNCQAAAPAYYaI22pwAAAAAjxEmaqPNCQAAAPAYYaI22pwAAAAAjxEmaqPNCQAAAPAYYaI22pwAAAAAjxEmaqPNCQAAAPAYYaI22pwAAAAAjxEmaqPNCQAAAPAYYaI22pwAAAAAjxEmaqPNCQAAAPAYYaI22pwAAAAAjxEmaqPNCQAAAPAYYaI22pwAAAAAjxEmaqPNCQAAAPAYYcKpulpyOGhzAgAAADxEmHCqqqr5zsgEAAAA4BHChBNhAgAAADCEMOHkDBO0OQEAAAAeIUw4VVbWfGdkAgAAAPAIYcKJNicAAADAEMKEE21OAAAAgCGECSfanAAAAABDCBNOtDkBAAAAhhAmnKqqJKtVsli8XQkAAABwUSBMOFVWMioBAAAAGECYcKqqIkwAAAAABhAmnKqquJITAAAAYABhwok2JwAAAMAQwoQTbU4AAACAIYQJJ9qcAAAAAEMIE060OQEAAACGECacaHMCAAAADCFMOBEmAAAAAEMIE06VlcyZAAAAAAwgTDgxMgEAAAAYQphwIkwAAAAAhhAmnGhzAgAAAAwhTDgxMgEAAAAYQphwIkwAAAAAhhAmnGhzAgAAAAwhTDgxMgEAAAAYQphwIkwAAAAAhhAmnGhzAgAAAAwhTDgxMgEAAAAYQphwIkwAAAAAhhAmnKqqaHMCAAAADCBMOFVWMjIBAAAAGECYcKLNCQAAADCEMOFEmxMAAABgCGHCiTYnAAAAwBDChBNtTgAAAIAhhAkn2pwAAAAAQwgTTrQ5AQAAAIYQJpxocwIAAAAMIUw40eYEAAAAGEKYcKLNCQAAADCEMOFEmxMAAABgCGHCiTYnAAAAwBDChBNtTgAAAIAhhAkn2pwAAAAAQwgTTrQ5AQAAAIYQJpxocwIAAAAMIUw40eYEAAAAGEKYcKLNCQAAADCEMOFEmxMAAABgCGHCiTYnAAAAwBDChBNtTgAAAIAhhAkn2pwAAAAAQwgTTrQ5AQAAAIYQJpxocwIAAAAMIUw40eYEAAAAGEKYcKLNCQAAADCEMOFEmxMAAABgCGHCiTYnAAAAwBDChBNtTgAAAIAhhAkn2pwAAAAAQwgTTrQ5AQAAAIYQJpxocwIAAAAMIUw40eYEAAAAGEKYcKLNCQAAADDEax/Fr1mzRpmZmTp58qSSkpKUmpqq8PDwOtsVFBRo9OjR9R4jMzNTYWFh2rVrlx599FG3dUFBQdqwYYPnBdHmBAAAABjilTCxceNGrVy5UtOnT1dUVJQyMjI0Y8YMzZ8/v862HTp00Lp169yWZWRkqLCwUGFhYW7L165dK5+zgcBisRgrijYnAAAAwBCvvHtev369UlJSNGDAAEnS1KlTNWbMGB08eFDx8fFu2/r4+LiNWJw5c0bbtm3TAw88UOe44eHhrjBhGG1OAAAAgCHNPmfCbrcrJydHvXr1ci2LiopSZGSk9u7d2+D+mzdvVmVlpW6++eY668aOHatRo0bpiSeeUG5urrHCaHMCAAAADGn2kYnS0lJVV1fXaVFq166diouLG9w/Oztb/fv3V1BQkGtZeHi40tLS1L17d506dUpr167V5MmT9dprr9V5nHMiTAAAAACGNPvIhMPhML1vYWGhPv/8cyUnJ7stj4mJ0dChQxUfH68ePXromWeeUXBwsLKzsz0/eGUlcyYAAAAAA5r93bPNZpPValVRUZHb8uLiYrVr1+68+2ZlZal9+/ZKSko673Zt2rTRZZddpvz8/PNu9/jjj8vPz0+SlHzmjJIZmQAAAMDPRFZWlrKysiTVTDVoCs0eJvz8/BQXF6ddu3a5QkF+fr4KCgqUmJh43n2zs7M1aNAgWa3nH1CpqqrS4cOHdeWVV553uxdeeEGhoaE1vyxdSpsTAAAAfjaSk5NdHT2lpaVauHBhoz+GV25aN3LkSK1bt06ffvqpDh48qFmzZqlHjx6Kj49XYWGh7rrrLu3bt89tny+//FJ5eXl1Wpwk6a233tLWrVt19OhRHTx4UC+++KKKi4s1cOBAz4uizQkAAAAwxCvvnocMGaKioiKlp6e7blqXlpYmqWZUIS8vT2fOnHHbJysrS4mJiYqJialzvIqKCte9J4KDg9W9e3elp6crIiLC86KYgA0AAAAYYtm0aZP5GdEXqbKyMg0bNkwlJSU/tjlZrdKhQ1KXLt4tDgAAAGhkpaWlstls2rBhg9tVUS+UV9qcWpzqasnhoM0JAAAAMIAwIdW0OEm0OQEAAAAGECYkwgQAAABgAmFCqrmSk0SbEwAAAGAAYUJiZAIAAAAwgTAhESYAAAAAEwgT0o9hgjYnAAAAwGOECenHOROMTAAAAAAeI0xItDkBAAAAJhAmpJowYbVKFou3KwEAAAAuGoQJqabNiVEJAAAAwBDChFQzMkGYAAAAAAwhTEg1YYIrOQEAAACGECYk2pwAAAAAEwgTEm1OAAAAgAmECYk2JwAAAMAEwoREmxMAAABgAmFCos0JAAAAMIEwIdHmBAAAAJhAmJBocwIAAABMIExItDkBAAAAJhAmJNqcAAAAABMIExJtTgAAAIAJhAmJNicAAADABMKERJsTAAAAYAJhQqLNCQAAADCBMCHR5gQAAACYQJiQaHMCAAAATCBMSLQ5AQAAACYQJiTanAAAAAATCBMSbU4AAACACYQJiTYnAAAAwATChESbEwAAAGACYUKizQkAAAAwgTAh0eYEAAAAmECYkGhzAgAAAEwgTEi0OQEAAAAmECYk2pwAAAAAEwgTEm1OAAAAgAmECYk2JwAAAMAEwoREmxMAAABgAmFCos0JAAAAMIEwIdHmBAAAAJhAmJBocwIAAABMIExItDkBAAAAJhAmJMIEAAAAYAJhQqppc2LOBAAAAGAIYUJiZAIAAAAwgTAhESYAAAAAEwgTEm1OAAAAgAmECYmRCQAAAMAEwoREmAAAAABMIExItDkBAAAAJhAmJEYmAAAAABMIExJhAgAAADCBMCHVhAnanAAAAABDCBNSzZwJRiYAAAAAQwgTEm1OAAAAgAmECYk2JwAAAMAEwoREmxMAAABgAmFCos0JAAAAMIEwIdHmBAAAAJhAmJBocwIAAABMIExItDkBAAAAJhAmJNqcAAAAABMIExJtTgAAAIAJhAmJNicAAADABMKERJsTAAAAYAJhQqLNCQAAADCBMCHR5gQAAACYQJiQaHMCAAAATCBMSLQ5AQAAACYQJiTanAAAAAATCBMSbU4AAACACYQJiTYnAAAAwATChESbEwAAAGACYUKizQkAAAAwgTBRXS05HIxMAAAAAAYRJqqqar4TJgAAAABDCBPOMEGbEwAAAGAIYaKysuY7IxMAAACAIYQJ2pwAAAAAUwgTtDkBAAAAphAmaHMCAAAATCFM0OYEAAAAmEKYIEwAAAAAphAmKislq1WyWLxdCQAAAHBRIUxUVTEqAQAAAJhAmKiq4kpOAAAAgAmEicpKRiYAAAAAEwgTtDkBAAAApnitv2fNmjXKzMzUyZMnlZSUpNTUVIWHh9fZrqCgQKNHj673GJmZmQoLC5Mkbd26Va+++qqOHTum2NhYPfLII0pMTGy4ENqcAAAAAFO8MjKxceNGrVy5UlOmTFFGRobKyso0Y8aMerft0KGD1q1b5/Z1880366qrrnIFidzcXD399NO65ZZbtHTpUl111VWaNm2aSkpKGi6GNicAAADAFK+EifXr1yslJUUDBgxQfHy8pk6dqj179ujgwYN1tvXx8VF4eLjrKygoSNu2bVNycrJrm3feeUfdunXTuHHjFBsbq8mTJyswMFDvv/9+w8XQ5gQAAACY0uxhwm63KycnR7169XIti4qKUmRkpPbu3dvg/ps3b1ZlZaVuvvlm17KvvvpKvXv3dv1usVjUq1cv7du3r+GCaHMCAAAATGn2MFFaWqrq6mpXi5JTu3btVFxc3OD+2dnZ6t+/v4KCglzLiouL1a5dO7ftbDabioqKGi6INicAAADAlGYPEw6Hw/S+hYWF+vzzz91anC70mLQ5AQAAAOY0e3+PzWaT1WqtM2pQ3+jCT2VlZal9+/ZKSkpyWx4WFlZnVKOkpKTO6MdPPf744/IrKJC++07JWVl1QgoAAABwscrKylJWVpakmqkGTaHZw4Sfn5/i4uK0a9cuVyjIz89XQUFBg5dyzc7O1qBBg2S1ug+oJCQkaOfOnW7Ldu7cqdtvv/28x3vhhRcUumWLtG+fRJAAAADAz0hycrLrw/LS0lItXLiw0R/DK1dzGjlypNatW6dPP/1UBw8e1KxZs9SjRw/Fx8ersLBQd911V53J019++aXy8vLqHT0YPny49u/fr9WrV+vIkSPKyMjQqVOnNGjQoIaLiYuTHnqosZ4aAAAA0Gp45TJGQ4YMUVFRkdLT0103rUtLS5MkVVVVKS8vT2fOnHHbJysrS4mJiYqJialzvJiYGM2YMUNLlizRihUrFBsbq5kzZ8pmszVczOWX13wBAAAAMMSyadOmC5i9fHEqKyvTsGHDVFJSotDQUG+XAwAAADSp0tJS2Ww2bdiwwe2qqBfKK21OAAAAAC5+hAkAAAAAphAmAAAAAJhCmAAAAABgCmECAAAAgCmECQAAAACmECYAAAAAmEKYAAAAAGAKYQIAAACAKYQJAAAAAKYQJgAAAACYQpgAAAAAYAphAgAAAIAphAkAAAAAphAmAAAAAJhCmAAAAABgCmECAAAAgCmECQAAAACmECYAAAAAmEKYAAAAAGAKYQIAAACAKYQJAAAAAKYQJgAAAACYQpgAAAAAYAphAgAAAIAphAkAAAAAphAmAAAAAJhCmAAAAABgCmECAAAAgCmECQAAAACmECYAAAAAmEKYAAAAAGAKYQIAAACAKYQJAAAAAKYQJgAAAACY0sbbBQAAAPycnT59Wna73dtloBXw8/OTv79/sz4mYQIAAKCJnD59Wl27dlVBQYG3S0ErEBkZqUOHDjVroCBMAAAANBG73a6CggLl5eUpNDTU2+XgZ6y0tFSXXnqp7HY7YQIAAODnJDQ0lDCBnyUmYAMAAAAwhTABAAAAwBTCBAAAAABTCBMAAAAATCFMAAAAwJC//e1vmjt3bqMf95577tFNN91keL/Dhw/LYrHo448/bvSacH5czQkAAACG/O1vf9MHH3ygP/zhD4163CeffFJnzpwxvF+nTp20detWJSYmNmo9aBhhAgAAAE2ivLxcAQEBHm8fFxdn6nHatm2ra6+91tS+uDC0OQEAAMBj99xzj1asWKGjR4/KYrHIYrEoNjZWH3/8sSwWi9atW6f77rtP7du3d40UfPHFFxo9erS6dOmigIAAxcfH66GHHlJJSUmdY9duc3Ie8+2339bEiRMVFhamjh07auLEiTp16pRru/ranG666Sb1799fWVlZuvrqqxUYGKhevXrpo48+qvOc5s+fr9jYWPn7+6tv377asmWLYmNj9cwzz3j8d/H0OUrSJ598okGDBslmsykoKEhXX321li9f7rbNsmXL1Lt3bwUEBCgsLEw33nijtmzZ4nE9zYWRCQAAAHjsySefVGFhobZv366///3vkmpGBpxvmqdMmaLhw4fr9ddfd7Us5eXl6bLLLtMdd9yh8PBw5eXlac6cORoyZIj++7//u8HHfPjhhzV8+HC9+eab2r9/v6ZOnaqIiAg9//zz590vJydHf/jDHzR9+nRFRERozpw5GjlypA4fPqzw8HBJ0vLly/XII49o/PjxGjVqlHJycjR69Oh6Q8D5ePoc3377baWkpOiGG27Qq6++qoiICH355Zc6cuSIa5u0tDTNmTNHEyZM0LPPPiuLxaLPPvtMubm5uu666wzV1dQIEwAAAC2FwyH98EPTHT8kRLJYLugQcXFx6tChg/z8/Nxai5yjAr/85S+1ZMkSt30GDx6swYMHu36vrKzUDTfcoC5dumjXrl3q2bPneR/zxhtv1CuvvCJJuvXWW7V//36tXbu2wTDx7bffavPmzbr88sslSb1791anTp20ceNGjRkzRtXV1XrmmWc0ePBg/fnPf3btFxkZqZSUlAb/Fkafo8Ph0MMPP6zevXvro48+kuXsv8XAgQNd++Xk5GjevHlKTU3V7NmzXcuHDh1qqJ7mQpsTAABAS/HDD5LN1nRfTRlUzvr1r39dZ1lFRYVmzpypxMREBQUFydfXV126dJEkffXVVw0e86dvpH/xi18oLy+vwf0uv/xyV5CQpEsuuUSXXHKJa99vvvlG33zzjUaNGlXnObRpY+wzd0+e44EDB3TkyBGNHz/eFSR+6oMPPlB1dbUmTJhg6PG9hZEJAACAliIkRDLYXmP4+E0sMjKyzrLp06dr8eLFeuaZZ5SUlKSQkBBVV1fr2muv1enTpxs8prMlyalt27YeXfXpp/s593U+Zn5+vqSakFGbj4+PIiIiGjx+bZ48x2+//VaSFB0dfc7jeLJNS0KYAAAAaCksFik01NtVXJD6PnF/4403NHXqVD322GOuZTk5Oc1ZVr06deokSTpx4oTb8qqqKtebek958hydAeXo0aPnPE7tbbp3726oBm+gzQkAAACGtG3bVuXl5R5vf+rUKbVt29Zt2bJlyxq7LMM6d+6szp0767/+67/clr/99tuqrKw0dCxPnmO3bt0UGxur5cuXy+Fw1HucgQMHymq1toi/jycYmQAAAIAhiYmJ+v7777V48WL16dNH/v7+591+8ODBmjVrljp06KCYmBi9++67+sc//tFM1Z6b1WrVM888o/vvv1/333+/Ro0apa+//lozZ86UzWaT1er55+6ePEeLxaL09HT99re/1a9+9Ss98MAD6tChg/bt26cTJ05oxowZiouL06OPPqq5c+eqtLRUI0aMkI+Pj7Zt26aEhATdcccdjf1nuCCMTAAAAMCQ+++/X3feeacef/xx9e3bV8OHDz/v9q+88ooGDx6sqVOnKiUlRYcOHdL777/fTNWe3/jx4zVv3jy9//77+vWvf63ly5dr1apVslgsstlsHh/H0+f461//2rV8/PjxGjFihJYuXarY2FjXNrNnz9aiRYv02WefKSUlRWPGjNGmTZsUExNzwc+3sVk2bdpU/xjLz1hZWZmGDRumkpIShV7kfYkAAKDlKi0tlc1m4z3HRWbbtm3q16+fNm/erBtuuMHb5XikoXPNuX7Dhg0KCgpqtMelzQkAAACt1qFDh7Rw4ULdcMMNCg0N1ZdffqkXX3xRffv2Vf/+/b1dXotHmAAAAECrFRAQoH/9619asWKFiouL1b59ew0dOlQvv/yy68pUDU3GNnpPip+T1vvMAQAA0OpFRkYqKyvrnOsPHz6srl27nvcYhw4dcpvz0JoQJgAAAIBziIqK0vbt2xvcprUiTAAAAADn4Ofnpz59+ni7jBaLS8MCAAAAMIUwAQAAAMAUwgQAAAAAUwgTAAAAAEwhTAAAAAAwhTABAAAAwBTCBAAAALzi8OHDslgs+vjjj71dCkwiTAAAAAAwhTABAAAAwBTCBAAAADy2du1aWSwW7dmzp866wYMHu+4W/fLLL6tfv34KCwtTWFiYrrvuOmVlZV3w47/99ttKTk5WZGSkgoKC9Itf/ELz589XdXV1nW2XLVum3r17KyAgQGFhYbrxxhu1ZcsW1/qysjJNmzZNcXFxatu2rSIjI5WSkqLjx49fcJ2tRRtvFwAAAICLx4gRI2Sz2bRq1Sq9/PLLruXHjx/XBx98oNmzZ0uqmQ/xu9/9TrGxsaqqqtKmTZs0bNgw/eMf/9Ctt95q+vFzcnKUnJyshx9+WIGBgdqzZ4+ee+45nThxQs8//7xru7S0NM2ZM0cTJkzQs88+K4vFos8++0y5ubm67rrrVFFRoUGDBmnPnj2aNm2a+vXrp5KSEmVlZamoqEgdO3Y0/0dqRQgTAAAALYTDIf3wQ9MdPyREslgu7Bj+/v66/fbbtWbNGs2cOVNWa02jy+uvvy5JGj16tCRp0aJFrn2qq6v1q1/9SoWFhVq0aNEFhYk//OEPrp8dDof69++vsLAwTZkyRc8995wsFotycnI0b948paamusKNJA0dOtT186pVq7R161a98847GjZsmGv57bffbrq21ogwAQAA0EL88INkszXd8UtKpNDQCz/OuHHjtHz5cn300UcaOHCgJGnlypVKTk7WJZdcIkn6/PPPNWPGDG3btk3Hjx+Xw+GQJHXv3v2CHrugoEDPPvus3n33XR09elSVlZWudcePH1dkZKQ++OADVVdXa8KECec8TnZ2tiIjI92CBIwjTAAAALQQISE1b/ib8viNYcCAAerSpYtWrlypgQMHat++ffr888/1xhtvSJK++eYb3XLLLbrmmmv0yiuvKDo6Wr6+vlq8eLE+/PBD04/rcDg0YsQIFRUV6amnntLll1+ugIAAbdu2Tb///e91+vRpSdK3334rSYqOjj7nsb799tvzrodnCBMAAAAthMXSOCMHTc1isWjMmDFasGCBFi9erJUrVyo0NFQjRoyQJL333ns6ffq03nnnHbVt29a1n91uv6DHzcnJ0fbt2/Xxxx/rxhtvdC3fvXu323YRERGSpKNHj55zJCQiIkJffPHFBdUDruYEAAAAE8aNG6eTJ08qMzNTq1ev1qhRoxQQECBJOnXqlNq0aeOaTyFJJ06c0Ntvv31Bj3nq1ClJcgsoDodDy5cvd9tu4MCBslqtWrZs2TmPdeutt6qgoEAbNmy4oJpaO0YmAAAAYFhCQoL69OmjadOm6ejRoxo3bpxr3cCBA5WamqoxY8bod7/7nQoKCvSnP/1Jl1xyidscBzOPGRsbqwceeEAzZsyQxWLRkiVLdOLECbft4uLi9Oijj2ru3LkqLS3ViBEj5OPjo23btikhIUF33HGHxo4dq2XLlunOO+/U9OnT1a9fP/3www/KysrSI488ooSEBNN1tiaMTAAAAMCUcePG6ejRo4qJidGAAQNcyxMTE/XGG2/oyy+/1LBhw/Tss8/q4Ycf1tixYy/o8fz8/PT3v/9dNptN//7v/67f/e536tatmxYsWFBn29mzZ2vRokX67LPPlJKSojFjxmjTpk2KiYmRJPn6+io7O1uTJk3S0qVLNWTIED344IP69ttvFR4efkF1tiaWTZs2ObxdRHMrKyvTsGHDVFJSotCLoTERAABclEpLS2Wz2XjPgSbX0LnmXL9hwwYFBQU12uMyMgEAAADAFOZMAAAAwOscDoeqqqrOud5iscjHx6cZK4InGJkAAACA161YsUK+vr7n/IqLi/N2iagHIxMAAADwuuHDh2v79u3nXF/7crBoOQgTAAAA8Lr27durffv23i4DBtHmBAAAAMAUwgQAAAAAUwgTAAAAAExhzgQAAEATKy0t9XYJ+Jnz1jlGmAAAAGgifn5+ioyM1KWXXurtUtAKREZGys/Pr1kfkzABAADQRPz9/XXo0CHZ7XZvl4JWwM/PT/7+/s36mF4LE2vWrFFmZqZOnjyppKQkpaamKjw8/Jzbf/jhh1qzZo3y8vIUGhqq22+/XXfeeackadeuXXr00Ufdtg8KCtKGDRua9DkAAAA0xN/fv9nf4AHNxSthYuPGjVq5cqWmT5+uqKgoZWRkaMaMGZo/f36922dnZ2vhwoWaNGmSfvGLX6isrExlZWV1tlu7dq3rNusWi6VJnwMAAADQ2nnlak7r169XSkqKBgwYoPj4eE2dOlV79uzRwYMH62xbWVmpJUuWaNKkSbrtttsUHR2tbt26qVevXnW2DQ8Pd32FhYU1x1MBvCIrK8vbJQCmcf7iYsb5C7hr9jBht9uVk5PjFgaioqIUGRmpvXv31tn+wIEDKioqUlVVle69917927/9m1588UWVlJTU2Xbs2LEaNWqUnnjiCeXm5jbp8wC8if/McDHj/MXFjPMXcNfsYaK0tFTV1dV1Rg7atWun4uLiOtsXFBRIqpljMXHiRD311FPKzc3Vc88959omPDxcaWlp+tOf/qQnn3xSkjR58mQVFRU13RMBAAAAWrlmnzPhcDgMbV9dXS1JGjdunK699lpJUmpqqiZMmKATJ07okksuUUxMjGJiYlz7JCYm6u6771Z2drbuuOOOc9bANZ9xsbLb7Zy/uGhx/uJixvmLi5XzvDX6XrwhzR4mbDabrFZrnVGD4uJitWvXrs72zhGM2mHB+bMzTPxUmzZtdNlllyk/P7/eGsrLyyWJaz7jorZw4UJvlwCYxvmLixnnLy5m5eXlCg4ObrTjNXuY8PPzU1xcnHbt2qWkpCRJUn5+vgoKCpSYmFhn++7du6tNmzY6evSoa/3Ro0clSR07dqz3MaqqqnT48GFdeeWV9a5v37691q5dq4CAAK76BAAAgJ89h8Oh8vJytW/fvlGP65VLw44cOVIZGRnq1q2bOnXqpEWLFqlHjx6Kj49XYWGhUlNTNX36dF1xxRUKDg5WcnKy/vM//1MdO3ZUUFCQFixYoH79+qlDhw6SpLfeekvR0dGKiYlReXm53njjDRUXF2vgwIH1Pr7VanXtCwAAALQGjTki4eSVMDFkyBAVFRUpPT3dddO6tLQ0STWjCnl5eTpz5oxr+8mTJ2vhwoX64x//KB8fH/Xt21cPPfSQa31FRYUyMjJUWFio4OBgde/eXenp6YqIiGj25wYAAAC0FpZNmzY17iwMAAAAAK2CV25aBwAAAODi55U2J29as2aNMjMzXe1VqampCg8P93ZZgJvXXntNK1ascFt2/fXXu+6vkpeXp7lz52rv3r0KCwvTXXfdpSFDhnijVECStHnzZv3tb3/TgQMHVFZWpg8++EA+Pj6u9Z6cs7w+w1saOn9vvvnmOvssW7ZM8fHxrt85f+Etq1at0ubNm5WXl6fAwED17dtXEydOdLtKalO+BreqkYmNGzdq5cqVmjJlijIyMlRWVqYZM2Z4uyygXgkJCVq3bp3ra9q0aZKkyspKTZ8+XTabTUuWLNG4ceM0d+5c7dixw8sVozU7c+aMevfurdGjR9dZ58k5y+szvOl856/T008/7faa3LVrV9c6zl940xdffKFRo0bp1Vdf1XPPPafDhw/r2Wefda1v6tfgVjUysX79eqWkpGjAgAGSpKlTp2rMmDE6ePCg26cLQEvQpk2bej8R+J//+R+dOHFCS5cuVWBgoLp27ardu3dr/fr1rsstA81t0KBBkqRdu3bVWefJOcvrM7zpfOevU0hIyDk/peX8hTfNnDnT7feHHnpIDz30kE6ePKng4OAmfw1uNSMTdrtdOTk56tWrl2tZVFSUIiMjtXfvXi9WBtQvJydHv/3tbzVu3Dilp6frhx9+kCR99dVXSkhIUGBgoGvb3r17a9++fd4qFTivhs5ZXp9xMZg5c6Z+85vfaMqUKdq6datrOecvWpqSkhL5+fkpICBAUtO/BreakYnS0lJVV1e77qjt1K5dOxUXF3unKOAcEhMTNX36dEVHR6ugoEDLli3TE088ofT0dBUVFdW5WzznMVqyhs5ZXp/R0o0fP169e/eWj4+P/vnPf+qPf/yjZs2apaSkJM5ftCh2u11//etflZyc7Jr309Svwa0mTDgcXAEXF4++ffu6fr7sssvUpUsXjR07VgcOHPBiVUDT4PUZLd3YsWNdP3fv3l3Hjx/XW2+9paSkJM5ftBhVVVV64YUXJEmTJk3yeL8LPYdbTZuTzWaT1WpVUVGR2/Li4uI6aQ1oaaKjoxUcHKz8/HyFhYXV+aSA8xgtWUPnLK/PuNh069ZN+fn5kjh/0TJUV1frpZdeUm5url5++WVXi5PU9K/BrSZM+Pn5KS4uzm1yVX5+vgoKCpSYmOi9wgAPHD9+XCdPnlRkZKQSEhK0f/9+lZeXu9bv3LlTV1xxhRcrBM6toXOW12dcbHJychQZGSmJ8xfe53A4NGvWLO3du1ezZ89WaGio2/qmfg1uNW1OkjRy5EhlZGSoW7du6tSpkxYtWqQePXpwpQW0OEuWLNH111+vDh06KD8/X0uWLNGVV16pbt26qaqqShEREXrppZd09913a9++ffroo4/qXM0BaE6lpaU6ceKEjh49Kkk6ePCgfHx8FB0drb59+zZ4zvL6DG863/m7a9cuFRcX64orrpCPj48+/fRTZWdnu9pJJM5feNfcuXO1detWvfjii5Kk77//XlLNiIOPj0+TvwZbNm3a1Kqa/VavXu12Q460tDRuKoMWZ8aMGdqzZ49KS0vVvn17XXPNNRo/frxruDE3N9d185nw8HCNGzdOQ4cO9W7RaNXee+89vfTSS3WWz5s3Tz179vTonOX1Gd5yvvPXbrfr1Vdf1bFjx2S1WhUTE6MxY8aof//+btty/sJb6rupoiS9/vrrrhG0pnwNbnVhAgAAAEDjaDVzJgAAAAA0LsIEAAAAAFMIEwAAAABMIUwAAAAAMIUwAQAAAMAUwgQAAAAAUwgTAAAAAEwhTAAAWpzXXntNkydP9nYZAIAGECYAAAAAmEKYAAAAAGBKG28XAABomaqqqrRixQpt3LhRZWVl6tatmyZPnqy4uDi99tpr2rFjh66//nq9+eabqqys1IgRI3T//ffLYrFIkr755hvNnz9fe/bsUWBgoJKTkzVhwgT5+PhIksrLy7V06VJ98sknKisrU5cuXfTII48oMTHRVcP69eu1atUqVVZWavDgwZo4caIsFoscDof+/Oc/Kzs7WyUlJYqIiNCdd96pESNGeOVvBQCtFWECAFCvFStW6LPPPtOTTz6p9u3ba+PGjXrssce0cuVKSVJOTo7CwsI0d+5c5ebm6uWXX9all16q2267TVVVVXriiScUFRWlxYsXq7CwUC+99JKCg4M1duxYSdKcOXN04MABTZ8+XVFRUTp48KAcDofr8b/++mtFRUVp7ty5ysvL04wZM9SjRw9dd911+vjjj/Xhhx/qqaeeUocOHZSfn6+ysjKv/J0AoDUjTAAA6rDb7Vq7dq0WL16srl27SpLuv/9+ffLJJ9qyZYskqbq6Wo899phCQkLUtWtXHTx4UOvXr9dtt92mHTt2KD8/XwsWLFBoaKguu+wy3XPPPfrLX/6isWPH6tixY/rwww+1ZMkSde/eXZIUHR3tVoOPj49SU1Pl5+enLl26qGfPntq9e7euu+46FRYWKjo6WldddZUsFosiIyOb9w8EAJBEmAAA1OPo0aM6c+aMHnzwQbfldrtdx44dk1Tz5j8kJMS1LiEhQevWrZMk5ebmqnPnzgoNDXWtT0xMVElJiUpLS3X48GH5+/u7gkR9oqOj5efn5/o9PDxcRUVFkqQBAwZo7dq1uvvuu9WvXz9df/316tmz5wU/bwCAMYQJAEAd5eXlkqT09HQFBwe7rQsJCVFmZuYFHd/hcLjmVpxLmzbu/0VZLBZVV1dLkiIjI7Vy5Upt27ZN27dv1x//+EclJydrypQpF1QXAMAYruYEAKijS5cu8vX11Xfffafo6Gi3L+dow9GjR3Xy5EnXPvv379ell14qSYqJidE333yj0tJS1/q9e/eqXbt2Cg0NVdeuXVVeXq79+/ebrjEgIEA33nij0tLSlJaWpnfffdf0sQAA5jAyAQCoIygoSCNHjtS8efNUUVGhbt266fvvv9eWLVs0cOBASZLVatWsWbN07733Kjc3V5mZmfr9738vSerTp486deqkl156Sffff79OnDih1157TSkpKZKkqKgo3XLLLXr++ec1ZcoURUVF6euvv1Z4eLjb1ZzO5b333pMkXXHFFbJarfrnP//pCjIAgOZDmAAA1OuBBx5QaGiolixZom+//VZhYWHq2bOnbDabJCkuLk7du3fXww8/rKqqKo0YMUK33XabpJqg8dxzzyk9PV0PPPCA69Kwo0ePdh0/NTVVS5Ys0Z/+9CedOXNGMTExevTRRz2qLTg4WKtXr9aCBQtktVqVmJioJ598svH/CACA87Js2rTJ0fBmAAD8yHmfiVdeecXbpQAAvIg5EwAAAABMIUwAAAAAMIU2JwAAAACmMDIBAAAAwBTCBAAAAABTCBMAAAAATCFMAAAAADCFMAEAAADAFMIEAAAAAFMIEwAAAABM+X/CfH6iWO10ngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(epochs , accuracy , color='#ff0000' , label='training_acc')\n",
    "plt.plot(epochs , val_accuracy , color='#0000FF',label='val_acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('trainig accuracy versus val_acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuarcy ')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b311e38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3236 - accuracy: 0.3127\n",
      "Epoch 1: val_accuracy improved from -inf to 0.32883, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 1s 830ms/step - loss: 1.3236 - accuracy: 0.3127 - val_loss: 1.2454 - val_accuracy: 0.3288\n",
      "Epoch 2/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2537 - accuracy: 0.3224\n",
      "Epoch 2: val_accuracy improved from 0.32883 to 0.34009, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.2537 - accuracy: 0.3224 - val_loss: 1.1843 - val_accuracy: 0.3401\n",
      "Epoch 3/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1873 - accuracy: 0.3379\n",
      "Epoch 3: val_accuracy improved from 0.34009 to 0.34910, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1873 - accuracy: 0.3379 - val_loss: 1.1266 - val_accuracy: 0.3491\n",
      "Epoch 4/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1247 - accuracy: 0.3466\n",
      "Epoch 4: val_accuracy improved from 0.34910 to 0.36712, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1247 - accuracy: 0.3466 - val_loss: 1.0725 - val_accuracy: 0.3671\n",
      "Epoch 5/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0659 - accuracy: 0.3591\n",
      "Epoch 5: val_accuracy improved from 0.36712 to 0.38964, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0659 - accuracy: 0.3591 - val_loss: 1.0220 - val_accuracy: 0.3896\n",
      "Epoch 6/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0109 - accuracy: 0.3785\n",
      "Epoch 6: val_accuracy improved from 0.38964 to 0.41441, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0109 - accuracy: 0.3785 - val_loss: 0.9748 - val_accuracy: 0.4144\n",
      "Epoch 7/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9595 - accuracy: 0.4105\n",
      "Epoch 7: val_accuracy improved from 0.41441 to 0.43919, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9595 - accuracy: 0.4105 - val_loss: 0.9307 - val_accuracy: 0.4392\n",
      "Epoch 8/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9115 - accuracy: 0.4318\n",
      "Epoch 8: val_accuracy improved from 0.43919 to 0.45270, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9115 - accuracy: 0.4318 - val_loss: 0.8894 - val_accuracy: 0.4527\n",
      "Epoch 9/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8667 - accuracy: 0.4637\n",
      "Epoch 9: val_accuracy improved from 0.45270 to 0.47748, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8667 - accuracy: 0.4637 - val_loss: 0.8507 - val_accuracy: 0.4775\n",
      "Epoch 10/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8250 - accuracy: 0.4821\n",
      "Epoch 10: val_accuracy improved from 0.47748 to 0.47973, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8250 - accuracy: 0.4821 - val_loss: 0.8143 - val_accuracy: 0.4797\n",
      "Epoch 11/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7863 - accuracy: 0.4927\n",
      "Epoch 11: val_accuracy improved from 0.47973 to 0.49324, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7863 - accuracy: 0.4927 - val_loss: 0.7802 - val_accuracy: 0.4932\n",
      "Epoch 12/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7502 - accuracy: 0.5082\n",
      "Epoch 12: val_accuracy improved from 0.49324 to 0.50225, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7502 - accuracy: 0.5082 - val_loss: 0.7482 - val_accuracy: 0.5023\n",
      "Epoch 13/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7169 - accuracy: 0.5247\n",
      "Epoch 13: val_accuracy improved from 0.50225 to 0.50901, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7169 - accuracy: 0.5247 - val_loss: 0.7180 - val_accuracy: 0.5090\n",
      "Epoch 14/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.5411\n",
      "Epoch 14: val_accuracy improved from 0.50901 to 0.51351, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6860 - accuracy: 0.5411 - val_loss: 0.6897 - val_accuracy: 0.5135\n",
      "Epoch 15/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6576 - accuracy: 0.5489\n",
      "Epoch 15: val_accuracy improved from 0.51351 to 0.52928, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6576 - accuracy: 0.5489 - val_loss: 0.6633 - val_accuracy: 0.5293\n",
      "Epoch 16/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.5586\n",
      "Epoch 16: val_accuracy improved from 0.52928 to 0.54505, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6313 - accuracy: 0.5586 - val_loss: 0.6385 - val_accuracy: 0.5450\n",
      "Epoch 17/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6070 - accuracy: 0.5731\n",
      "Epoch 17: val_accuracy improved from 0.54505 to 0.55405, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6070 - accuracy: 0.5731 - val_loss: 0.6154 - val_accuracy: 0.5541\n",
      "Epoch 18/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5845 - accuracy: 0.5818\n",
      "Epoch 18: val_accuracy improved from 0.55405 to 0.56081, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5845 - accuracy: 0.5818 - val_loss: 0.5937 - val_accuracy: 0.5608\n",
      "Epoch 19/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.5992\n",
      "Epoch 19: val_accuracy improved from 0.56081 to 0.57432, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5637 - accuracy: 0.5992 - val_loss: 0.5735 - val_accuracy: 0.5743\n",
      "Epoch 20/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5444 - accuracy: 0.6012\n",
      "Epoch 20: val_accuracy improved from 0.57432 to 0.58108, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5444 - accuracy: 0.6012 - val_loss: 0.5545 - val_accuracy: 0.5811\n",
      "Epoch 21/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5263 - accuracy: 0.6215\n",
      "Epoch 21: val_accuracy improved from 0.58108 to 0.59234, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5263 - accuracy: 0.6215 - val_loss: 0.5367 - val_accuracy: 0.5923\n",
      "Epoch 22/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.6292\n",
      "Epoch 22: val_accuracy improved from 0.59234 to 0.59910, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5094 - accuracy: 0.6292 - val_loss: 0.5200 - val_accuracy: 0.5991\n",
      "Epoch 23/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4936 - accuracy: 0.6350\n",
      "Epoch 23: val_accuracy improved from 0.59910 to 0.61712, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4936 - accuracy: 0.6350 - val_loss: 0.5042 - val_accuracy: 0.6171\n",
      "Epoch 24/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4787 - accuracy: 0.6467\n",
      "Epoch 24: val_accuracy improved from 0.61712 to 0.63063, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4787 - accuracy: 0.6467 - val_loss: 0.4894 - val_accuracy: 0.6306\n",
      "Epoch 25/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4647 - accuracy: 0.6486\n",
      "Epoch 25: val_accuracy improved from 0.63063 to 0.63739, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4647 - accuracy: 0.6486 - val_loss: 0.4753 - val_accuracy: 0.6374\n",
      "Epoch 26/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4516 - accuracy: 0.6505\n",
      "Epoch 26: val_accuracy did not improve from 0.63739\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4516 - accuracy: 0.6505 - val_loss: 0.4621 - val_accuracy: 0.6374\n",
      "Epoch 27/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4392 - accuracy: 0.6602\n",
      "Epoch 27: val_accuracy improved from 0.63739 to 0.65090, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4392 - accuracy: 0.6602 - val_loss: 0.4496 - val_accuracy: 0.6509\n",
      "Epoch 28/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4275 - accuracy: 0.6602\n",
      "Epoch 28: val_accuracy improved from 0.65090 to 0.65541, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.4275 - accuracy: 0.6602 - val_loss: 0.4379 - val_accuracy: 0.6554\n",
      "Epoch 29/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4165 - accuracy: 0.6699\n",
      "Epoch 29: val_accuracy did not improve from 0.65541\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4165 - accuracy: 0.6699 - val_loss: 0.4269 - val_accuracy: 0.6554\n",
      "Epoch 30/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4062 - accuracy: 0.6680\n",
      "Epoch 30: val_accuracy improved from 0.65541 to 0.67342, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.4062 - accuracy: 0.6680 - val_loss: 0.4167 - val_accuracy: 0.6734\n",
      "Epoch 31/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3965 - accuracy: 0.6728\n",
      "Epoch 31: val_accuracy improved from 0.67342 to 0.68468, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3965 - accuracy: 0.6728 - val_loss: 0.4071 - val_accuracy: 0.6847\n",
      "Epoch 32/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.6805\n",
      "Epoch 32: val_accuracy improved from 0.68468 to 0.70721, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3875 - accuracy: 0.6805 - val_loss: 0.3982 - val_accuracy: 0.7072\n",
      "Epoch 33/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3790 - accuracy: 0.6970\n",
      "Epoch 33: val_accuracy improved from 0.70721 to 0.71171, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3790 - accuracy: 0.6970 - val_loss: 0.3900 - val_accuracy: 0.7117\n",
      "Epoch 34/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3711 - accuracy: 0.7067\n",
      "Epoch 34: val_accuracy did not improve from 0.71171\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3711 - accuracy: 0.7067 - val_loss: 0.3823 - val_accuracy: 0.7117\n",
      "Epoch 35/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3638 - accuracy: 0.7067\n",
      "Epoch 35: val_accuracy improved from 0.71171 to 0.71847, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.3638 - accuracy: 0.7067 - val_loss: 0.3752 - val_accuracy: 0.7185\n",
      "Epoch 36/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3569 - accuracy: 0.7106\n",
      "Epoch 36: val_accuracy did not improve from 0.71847\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3569 - accuracy: 0.7106 - val_loss: 0.3687 - val_accuracy: 0.7162\n",
      "Epoch 37/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.7115\n",
      "Epoch 37: val_accuracy improved from 0.71847 to 0.72523, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3506 - accuracy: 0.7115 - val_loss: 0.3627 - val_accuracy: 0.7252\n",
      "Epoch 38/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3447 - accuracy: 0.7125\n",
      "Epoch 38: val_accuracy did not improve from 0.72523\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3447 - accuracy: 0.7125 - val_loss: 0.3572 - val_accuracy: 0.7252\n",
      "Epoch 39/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.7115\n",
      "Epoch 39: val_accuracy did not improve from 0.72523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3393 - accuracy: 0.7115 - val_loss: 0.3521 - val_accuracy: 0.7230\n",
      "Epoch 40/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.7135\n",
      "Epoch 40: val_accuracy improved from 0.72523 to 0.72973, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3343 - accuracy: 0.7135 - val_loss: 0.3475 - val_accuracy: 0.7297\n",
      "Epoch 41/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.7125\n",
      "Epoch 41: val_accuracy did not improve from 0.72973\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3297 - accuracy: 0.7125 - val_loss: 0.3433 - val_accuracy: 0.7252\n",
      "Epoch 42/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3255 - accuracy: 0.7173\n",
      "Epoch 42: val_accuracy did not improve from 0.72973\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3255 - accuracy: 0.7173 - val_loss: 0.3396 - val_accuracy: 0.7275\n",
      "Epoch 43/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.7222\n",
      "Epoch 43: val_accuracy improved from 0.72973 to 0.73423, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3216 - accuracy: 0.7222 - val_loss: 0.3361 - val_accuracy: 0.7342\n",
      "Epoch 44/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3180 - accuracy: 0.7270\n",
      "Epoch 44: val_accuracy did not improve from 0.73423\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3180 - accuracy: 0.7270 - val_loss: 0.3330 - val_accuracy: 0.7320\n",
      "Epoch 45/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.7309\n",
      "Epoch 45: val_accuracy did not improve from 0.73423\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3147 - accuracy: 0.7309 - val_loss: 0.3302 - val_accuracy: 0.7320\n",
      "Epoch 46/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.7396\n",
      "Epoch 46: val_accuracy did not improve from 0.73423\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3117 - accuracy: 0.7396 - val_loss: 0.3276 - val_accuracy: 0.7320\n",
      "Epoch 47/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.7415\n",
      "Epoch 47: val_accuracy did not improve from 0.73423\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3088 - accuracy: 0.7415 - val_loss: 0.3252 - val_accuracy: 0.7297\n",
      "Epoch 48/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.7425\n",
      "Epoch 48: val_accuracy did not improve from 0.73423\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3062 - accuracy: 0.7425 - val_loss: 0.3230 - val_accuracy: 0.7297\n",
      "Epoch 49/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.7425\n",
      "Epoch 49: val_accuracy did not improve from 0.73423\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3036 - accuracy: 0.7425 - val_loss: 0.3209 - val_accuracy: 0.7297\n",
      "Epoch 50/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.7464\n",
      "Epoch 50: val_accuracy improved from 0.73423 to 0.73649, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3012 - accuracy: 0.7464 - val_loss: 0.3190 - val_accuracy: 0.7365\n",
      "Epoch 51/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2989 - accuracy: 0.7531\n",
      "Epoch 51: val_accuracy did not improve from 0.73649\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2989 - accuracy: 0.7531 - val_loss: 0.3171 - val_accuracy: 0.7365\n",
      "Epoch 52/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.7551\n",
      "Epoch 52: val_accuracy did not improve from 0.73649\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2967 - accuracy: 0.7551 - val_loss: 0.3154 - val_accuracy: 0.7365\n",
      "Epoch 53/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2945 - accuracy: 0.7531\n",
      "Epoch 53: val_accuracy did not improve from 0.73649\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2945 - accuracy: 0.7531 - val_loss: 0.3137 - val_accuracy: 0.7365\n",
      "Epoch 54/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.7609\n",
      "Epoch 54: val_accuracy improved from 0.73649 to 0.74324, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2925 - accuracy: 0.7609 - val_loss: 0.3120 - val_accuracy: 0.7432\n",
      "Epoch 55/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.7648\n",
      "Epoch 55: val_accuracy did not improve from 0.74324\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2905 - accuracy: 0.7648 - val_loss: 0.3105 - val_accuracy: 0.7432\n",
      "Epoch 56/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.7667\n",
      "Epoch 56: val_accuracy did not improve from 0.74324\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2885 - accuracy: 0.7667 - val_loss: 0.3089 - val_accuracy: 0.7432\n",
      "Epoch 57/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.7648\n",
      "Epoch 57: val_accuracy improved from 0.74324 to 0.74775, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2866 - accuracy: 0.7648 - val_loss: 0.3074 - val_accuracy: 0.7477\n",
      "Epoch 58/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.7638\n",
      "Epoch 58: val_accuracy improved from 0.74775 to 0.75000, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2848 - accuracy: 0.7638 - val_loss: 0.3060 - val_accuracy: 0.7500\n",
      "Epoch 59/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.7686\n",
      "Epoch 59: val_accuracy did not improve from 0.75000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2830 - accuracy: 0.7686 - val_loss: 0.3045 - val_accuracy: 0.7500\n",
      "Epoch 60/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.7696\n",
      "Epoch 60: val_accuracy improved from 0.75000 to 0.75901, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2813 - accuracy: 0.7696 - val_loss: 0.3031 - val_accuracy: 0.7590\n",
      "Epoch 61/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.7744\n",
      "Epoch 61: val_accuracy improved from 0.75901 to 0.76351, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2797 - accuracy: 0.7744 - val_loss: 0.3017 - val_accuracy: 0.7635\n",
      "Epoch 62/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 0.7773\n",
      "Epoch 62: val_accuracy did not improve from 0.76351\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2781 - accuracy: 0.7773 - val_loss: 0.3003 - val_accuracy: 0.7635\n",
      "Epoch 63/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.7841\n",
      "Epoch 63: val_accuracy improved from 0.76351 to 0.76802, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2765 - accuracy: 0.7841 - val_loss: 0.2989 - val_accuracy: 0.7680\n",
      "Epoch 64/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.7832\n",
      "Epoch 64: val_accuracy did not improve from 0.76802\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2749 - accuracy: 0.7832 - val_loss: 0.2975 - val_accuracy: 0.7680\n",
      "Epoch 65/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.7832\n",
      "Epoch 65: val_accuracy improved from 0.76802 to 0.77027, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2734 - accuracy: 0.7832 - val_loss: 0.2961 - val_accuracy: 0.7703\n",
      "Epoch 66/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.7851\n",
      "Epoch 66: val_accuracy improved from 0.77027 to 0.77477, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2720 - accuracy: 0.7851 - val_loss: 0.2947 - val_accuracy: 0.7748\n",
      "Epoch 67/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.7841\n",
      "Epoch 67: val_accuracy improved from 0.77477 to 0.77703, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2705 - accuracy: 0.7841 - val_loss: 0.2933 - val_accuracy: 0.7770\n",
      "Epoch 68/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.7899\n",
      "Epoch 68: val_accuracy did not improve from 0.77703\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2691 - accuracy: 0.7899 - val_loss: 0.2919 - val_accuracy: 0.7748\n",
      "Epoch 69/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.7919\n",
      "Epoch 69: val_accuracy did not improve from 0.77703\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2677 - accuracy: 0.7919 - val_loss: 0.2905 - val_accuracy: 0.7748\n",
      "Epoch 70/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.7938\n",
      "Epoch 70: val_accuracy did not improve from 0.77703\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2663 - accuracy: 0.7938 - val_loss: 0.2892 - val_accuracy: 0.7725\n",
      "Epoch 71/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.7938\n",
      "Epoch 71: val_accuracy did not improve from 0.77703\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2650 - accuracy: 0.7938 - val_loss: 0.2878 - val_accuracy: 0.7748\n",
      "Epoch 72/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.7977\n",
      "Epoch 72: val_accuracy did not improve from 0.77703\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2637 - accuracy: 0.7977 - val_loss: 0.2865 - val_accuracy: 0.7748\n",
      "Epoch 73/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.7977\n",
      "Epoch 73: val_accuracy improved from 0.77703 to 0.77928, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.2624 - accuracy: 0.7977 - val_loss: 0.2852 - val_accuracy: 0.7793\n",
      "Epoch 74/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2611 - accuracy: 0.7996\n",
      "Epoch 74: val_accuracy did not improve from 0.77928\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2611 - accuracy: 0.7996 - val_loss: 0.2840 - val_accuracy: 0.7793\n",
      "Epoch 75/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.7986\n",
      "Epoch 75: val_accuracy did not improve from 0.77928\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2599 - accuracy: 0.7986 - val_loss: 0.2827 - val_accuracy: 0.7793\n",
      "Epoch 76/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.7986\n",
      "Epoch 76: val_accuracy did not improve from 0.77928\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2587 - accuracy: 0.7986 - val_loss: 0.2815 - val_accuracy: 0.7793\n",
      "Epoch 77/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.7986\n",
      "Epoch 77: val_accuracy improved from 0.77928 to 0.78153, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2575 - accuracy: 0.7986 - val_loss: 0.2803 - val_accuracy: 0.7815\n",
      "Epoch 78/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.8045\n",
      "Epoch 78: val_accuracy did not improve from 0.78153\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2564 - accuracy: 0.8045 - val_loss: 0.2792 - val_accuracy: 0.7815\n",
      "Epoch 79/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.8054\n",
      "Epoch 79: val_accuracy did not improve from 0.78153\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2552 - accuracy: 0.8054 - val_loss: 0.2780 - val_accuracy: 0.7815\n",
      "Epoch 80/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.8045\n",
      "Epoch 80: val_accuracy improved from 0.78153 to 0.78378, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2541 - accuracy: 0.8045 - val_loss: 0.2769 - val_accuracy: 0.7838\n",
      "Epoch 81/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.8045\n",
      "Epoch 81: val_accuracy improved from 0.78378 to 0.79279, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2530 - accuracy: 0.8045 - val_loss: 0.2759 - val_accuracy: 0.7928\n",
      "Epoch 82/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.8054\n",
      "Epoch 82: val_accuracy did not improve from 0.79279\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2520 - accuracy: 0.8054 - val_loss: 0.2748 - val_accuracy: 0.7928\n",
      "Epoch 83/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.8103\n",
      "Epoch 83: val_accuracy did not improve from 0.79279\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2509 - accuracy: 0.8103 - val_loss: 0.2738 - val_accuracy: 0.7905\n",
      "Epoch 84/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.8122\n",
      "Epoch 84: val_accuracy did not improve from 0.79279\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2499 - accuracy: 0.8122 - val_loss: 0.2728 - val_accuracy: 0.7905\n",
      "Epoch 85/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.8122\n",
      "Epoch 85: val_accuracy did not improve from 0.79279\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2488 - accuracy: 0.8122 - val_loss: 0.2719 - val_accuracy: 0.7928\n",
      "Epoch 86/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.8141\n",
      "Epoch 86: val_accuracy did not improve from 0.79279\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2478 - accuracy: 0.8141 - val_loss: 0.2710 - val_accuracy: 0.7928\n",
      "Epoch 87/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.8141\n",
      "Epoch 87: val_accuracy improved from 0.79279 to 0.79505, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2469 - accuracy: 0.8141 - val_loss: 0.2701 - val_accuracy: 0.7950\n",
      "Epoch 88/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.8161\n",
      "Epoch 88: val_accuracy did not improve from 0.79505\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2459 - accuracy: 0.8161 - val_loss: 0.2692 - val_accuracy: 0.7950\n",
      "Epoch 89/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.8161\n",
      "Epoch 89: val_accuracy improved from 0.79505 to 0.79730, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2449 - accuracy: 0.8161 - val_loss: 0.2683 - val_accuracy: 0.7973\n",
      "Epoch 90/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.8180\n",
      "Epoch 90: val_accuracy did not improve from 0.79730\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2440 - accuracy: 0.8180 - val_loss: 0.2675 - val_accuracy: 0.7973\n",
      "Epoch 91/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.8180\n",
      "Epoch 91: val_accuracy improved from 0.79730 to 0.79955, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2430 - accuracy: 0.8180 - val_loss: 0.2667 - val_accuracy: 0.7995\n",
      "Epoch 92/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.8209\n",
      "Epoch 92: val_accuracy improved from 0.79955 to 0.80180, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2421 - accuracy: 0.8209 - val_loss: 0.2659 - val_accuracy: 0.8018\n",
      "Epoch 93/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.8209\n",
      "Epoch 93: val_accuracy did not improve from 0.80180\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2412 - accuracy: 0.8209 - val_loss: 0.2651 - val_accuracy: 0.7995\n",
      "Epoch 94/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.8219\n",
      "Epoch 94: val_accuracy did not improve from 0.80180\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2403 - accuracy: 0.8219 - val_loss: 0.2644 - val_accuracy: 0.8018\n",
      "Epoch 95/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.8219\n",
      "Epoch 95: val_accuracy improved from 0.80180 to 0.80405, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2394 - accuracy: 0.8219 - val_loss: 0.2637 - val_accuracy: 0.8041\n",
      "Epoch 96/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.8219\n",
      "Epoch 96: val_accuracy did not improve from 0.80405\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2385 - accuracy: 0.8219 - val_loss: 0.2629 - val_accuracy: 0.8041\n",
      "Epoch 97/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.8228\n",
      "Epoch 97: val_accuracy did not improve from 0.80405\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2377 - accuracy: 0.8228 - val_loss: 0.2622 - val_accuracy: 0.8041\n",
      "Epoch 98/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 0.8228\n",
      "Epoch 98: val_accuracy did not improve from 0.80405\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2368 - accuracy: 0.8228 - val_loss: 0.2616 - val_accuracy: 0.8041\n",
      "Epoch 99/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.8238\n",
      "Epoch 99: val_accuracy improved from 0.80405 to 0.80631, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2360 - accuracy: 0.8238 - val_loss: 0.2609 - val_accuracy: 0.8063\n",
      "Epoch 100/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.8238\n",
      "Epoch 100: val_accuracy improved from 0.80631 to 0.80856, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2351 - accuracy: 0.8238 - val_loss: 0.2602 - val_accuracy: 0.8086\n",
      "Epoch 101/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.8238\n",
      "Epoch 101: val_accuracy improved from 0.80856 to 0.81081, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2343 - accuracy: 0.8238 - val_loss: 0.2595 - val_accuracy: 0.8108\n",
      "Epoch 102/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.8238\n",
      "Epoch 102: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2335 - accuracy: 0.8238 - val_loss: 0.2589 - val_accuracy: 0.8086\n",
      "Epoch 103/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.8238\n",
      "Epoch 103: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2327 - accuracy: 0.8238 - val_loss: 0.2582 - val_accuracy: 0.8086\n",
      "Epoch 104/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2319 - accuracy: 0.8258\n",
      "Epoch 104: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2319 - accuracy: 0.8258 - val_loss: 0.2576 - val_accuracy: 0.8086\n",
      "Epoch 105/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2311 - accuracy: 0.8258\n",
      "Epoch 105: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2311 - accuracy: 0.8258 - val_loss: 0.2569 - val_accuracy: 0.8086\n",
      "Epoch 106/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.8267\n",
      "Epoch 106: val_accuracy did not improve from 0.81081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2303 - accuracy: 0.8267 - val_loss: 0.2563 - val_accuracy: 0.8108\n",
      "Epoch 107/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2295 - accuracy: 0.8258\n",
      "Epoch 107: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2295 - accuracy: 0.8258 - val_loss: 0.2557 - val_accuracy: 0.8108\n",
      "Epoch 108/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.8267\n",
      "Epoch 108: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2288 - accuracy: 0.8267 - val_loss: 0.2550 - val_accuracy: 0.8108\n",
      "Epoch 109/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2280 - accuracy: 0.8267\n",
      "Epoch 109: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2280 - accuracy: 0.8267 - val_loss: 0.2544 - val_accuracy: 0.8108\n",
      "Epoch 110/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.8277\n",
      "Epoch 110: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2273 - accuracy: 0.8277 - val_loss: 0.2538 - val_accuracy: 0.8108\n",
      "Epoch 111/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.8277\n",
      "Epoch 111: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2266 - accuracy: 0.8277 - val_loss: 0.2532 - val_accuracy: 0.8108\n",
      "Epoch 112/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2258 - accuracy: 0.8287\n",
      "Epoch 112: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2258 - accuracy: 0.8287 - val_loss: 0.2525 - val_accuracy: 0.8108\n",
      "Epoch 113/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2251 - accuracy: 0.8287\n",
      "Epoch 113: val_accuracy did not improve from 0.81081\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2251 - accuracy: 0.8287 - val_loss: 0.2519 - val_accuracy: 0.8108\n",
      "Epoch 114/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.8296\n",
      "Epoch 114: val_accuracy improved from 0.81081 to 0.81306, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2244 - accuracy: 0.8296 - val_loss: 0.2513 - val_accuracy: 0.8131\n",
      "Epoch 115/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.8296\n",
      "Epoch 115: val_accuracy did not improve from 0.81306\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2237 - accuracy: 0.8296 - val_loss: 0.2507 - val_accuracy: 0.8131\n",
      "Epoch 116/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.8316\n",
      "Epoch 116: val_accuracy did not improve from 0.81306\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2230 - accuracy: 0.8316 - val_loss: 0.2501 - val_accuracy: 0.8131\n",
      "Epoch 117/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2223 - accuracy: 0.8316\n",
      "Epoch 117: val_accuracy improved from 0.81306 to 0.81532, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2223 - accuracy: 0.8316 - val_loss: 0.2495 - val_accuracy: 0.8153\n",
      "Epoch 118/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.8325\n",
      "Epoch 118: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2216 - accuracy: 0.8325 - val_loss: 0.2489 - val_accuracy: 0.8153\n",
      "Epoch 119/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2209 - accuracy: 0.8335\n",
      "Epoch 119: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2209 - accuracy: 0.8335 - val_loss: 0.2483 - val_accuracy: 0.8153\n",
      "Epoch 120/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2203 - accuracy: 0.8335\n",
      "Epoch 120: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2203 - accuracy: 0.8335 - val_loss: 0.2478 - val_accuracy: 0.8153\n",
      "Epoch 121/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.8335\n",
      "Epoch 121: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2196 - accuracy: 0.8335 - val_loss: 0.2472 - val_accuracy: 0.8153\n",
      "Epoch 122/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.8345\n",
      "Epoch 122: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2189 - accuracy: 0.8345 - val_loss: 0.2466 - val_accuracy: 0.8131\n",
      "Epoch 123/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.8345\n",
      "Epoch 123: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2183 - accuracy: 0.8345 - val_loss: 0.2460 - val_accuracy: 0.8131\n",
      "Epoch 124/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.8345\n",
      "Epoch 124: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2176 - accuracy: 0.8345 - val_loss: 0.2455 - val_accuracy: 0.8131\n",
      "Epoch 125/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2170 - accuracy: 0.8354\n",
      "Epoch 125: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2170 - accuracy: 0.8354 - val_loss: 0.2449 - val_accuracy: 0.8153\n",
      "Epoch 126/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.8364\n",
      "Epoch 126: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2164 - accuracy: 0.8364 - val_loss: 0.2444 - val_accuracy: 0.8153\n",
      "Epoch 127/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2157 - accuracy: 0.8364\n",
      "Epoch 127: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2157 - accuracy: 0.8364 - val_loss: 0.2438 - val_accuracy: 0.8153\n",
      "Epoch 128/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 0.8374\n",
      "Epoch 128: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2151 - accuracy: 0.8374 - val_loss: 0.2433 - val_accuracy: 0.8153\n",
      "Epoch 129/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.8383\n",
      "Epoch 129: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2145 - accuracy: 0.8383 - val_loss: 0.2427 - val_accuracy: 0.8153\n",
      "Epoch 130/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2139 - accuracy: 0.8383\n",
      "Epoch 130: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2139 - accuracy: 0.8383 - val_loss: 0.2422 - val_accuracy: 0.8153\n",
      "Epoch 131/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.8393\n",
      "Epoch 131: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2133 - accuracy: 0.8393 - val_loss: 0.2416 - val_accuracy: 0.8153\n",
      "Epoch 132/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.8403\n",
      "Epoch 132: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2127 - accuracy: 0.8403 - val_loss: 0.2411 - val_accuracy: 0.8153\n",
      "Epoch 133/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.8403\n",
      "Epoch 133: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2121 - accuracy: 0.8403 - val_loss: 0.2406 - val_accuracy: 0.8153\n",
      "Epoch 134/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.8412\n",
      "Epoch 134: val_accuracy did not improve from 0.81532\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2115 - accuracy: 0.8412 - val_loss: 0.2401 - val_accuracy: 0.8153\n",
      "Epoch 135/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.8412\n",
      "Epoch 135: val_accuracy improved from 0.81532 to 0.81757, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step - loss: 0.2109 - accuracy: 0.8412 - val_loss: 0.2395 - val_accuracy: 0.8176\n",
      "Epoch 136/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.8412\n",
      "Epoch 136: val_accuracy did not improve from 0.81757\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2103 - accuracy: 0.8412 - val_loss: 0.2390 - val_accuracy: 0.8176\n",
      "Epoch 137/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.8412\n",
      "Epoch 137: val_accuracy did not improve from 0.81757\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2097 - accuracy: 0.8412 - val_loss: 0.2385 - val_accuracy: 0.8176\n",
      "Epoch 138/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.8412\n",
      "Epoch 138: val_accuracy did not improve from 0.81757\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2091 - accuracy: 0.8412 - val_loss: 0.2380 - val_accuracy: 0.8176\n",
      "Epoch 139/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.8412\n",
      "Epoch 139: val_accuracy improved from 0.81757 to 0.81982, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2086 - accuracy: 0.8412 - val_loss: 0.2375 - val_accuracy: 0.8198\n",
      "Epoch 140/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.8441\n",
      "Epoch 140: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2080 - accuracy: 0.8441 - val_loss: 0.2370 - val_accuracy: 0.8176\n",
      "Epoch 141/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.8441\n",
      "Epoch 141: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2074 - accuracy: 0.8441 - val_loss: 0.2365 - val_accuracy: 0.8176\n",
      "Epoch 142/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.8451\n",
      "Epoch 142: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2069 - accuracy: 0.8451 - val_loss: 0.2360 - val_accuracy: 0.8176\n",
      "Epoch 143/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.8461\n",
      "Epoch 143: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2063 - accuracy: 0.8461 - val_loss: 0.2355 - val_accuracy: 0.8176\n",
      "Epoch 144/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.8470\n",
      "Epoch 144: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2058 - accuracy: 0.8470 - val_loss: 0.2350 - val_accuracy: 0.8176\n",
      "Epoch 145/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2052 - accuracy: 0.8470\n",
      "Epoch 145: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2052 - accuracy: 0.8470 - val_loss: 0.2345 - val_accuracy: 0.8176\n",
      "Epoch 146/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.8470\n",
      "Epoch 146: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2047 - accuracy: 0.8470 - val_loss: 0.2340 - val_accuracy: 0.8176\n",
      "Epoch 147/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.8480\n",
      "Epoch 147: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2041 - accuracy: 0.8480 - val_loss: 0.2335 - val_accuracy: 0.8176\n",
      "Epoch 148/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.8490\n",
      "Epoch 148: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2036 - accuracy: 0.8490 - val_loss: 0.2330 - val_accuracy: 0.8176\n",
      "Epoch 149/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.8500\n",
      "Epoch 149: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2031 - accuracy: 0.8500 - val_loss: 0.2325 - val_accuracy: 0.8176\n",
      "Epoch 150/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.8500\n",
      "Epoch 150: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2025 - accuracy: 0.8500 - val_loss: 0.2321 - val_accuracy: 0.8198\n",
      "Epoch 151/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.8509\n",
      "Epoch 151: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2020 - accuracy: 0.8509 - val_loss: 0.2316 - val_accuracy: 0.8198\n",
      "Epoch 152/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2015 - accuracy: 0.8509\n",
      "Epoch 152: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2015 - accuracy: 0.8509 - val_loss: 0.2311 - val_accuracy: 0.8198\n",
      "Epoch 153/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2010 - accuracy: 0.8509\n",
      "Epoch 153: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2010 - accuracy: 0.8509 - val_loss: 0.2306 - val_accuracy: 0.8198\n",
      "Epoch 154/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2005 - accuracy: 0.8519\n",
      "Epoch 154: val_accuracy did not improve from 0.81982\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2005 - accuracy: 0.8519 - val_loss: 0.2302 - val_accuracy: 0.8198\n",
      "Epoch 155/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.8519\n",
      "Epoch 155: val_accuracy improved from 0.81982 to 0.82207, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2000 - accuracy: 0.8519 - val_loss: 0.2297 - val_accuracy: 0.8221\n",
      "Epoch 156/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.8519\n",
      "Epoch 156: val_accuracy did not improve from 0.82207\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1995 - accuracy: 0.8519 - val_loss: 0.2293 - val_accuracy: 0.8221\n",
      "Epoch 157/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1989 - accuracy: 0.8519\n",
      "Epoch 157: val_accuracy did not improve from 0.82207\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1989 - accuracy: 0.8519 - val_loss: 0.2288 - val_accuracy: 0.8221\n",
      "Epoch 158/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.8519\n",
      "Epoch 158: val_accuracy did not improve from 0.82207\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1984 - accuracy: 0.8519 - val_loss: 0.2283 - val_accuracy: 0.8221\n",
      "Epoch 159/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.8519\n",
      "Epoch 159: val_accuracy did not improve from 0.82207\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1980 - accuracy: 0.8519 - val_loss: 0.2279 - val_accuracy: 0.8221\n",
      "Epoch 160/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.8519\n",
      "Epoch 160: val_accuracy did not improve from 0.82207\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1975 - accuracy: 0.8519 - val_loss: 0.2274 - val_accuracy: 0.8221\n",
      "Epoch 161/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.8519\n",
      "Epoch 161: val_accuracy improved from 0.82207 to 0.82432, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1970 - accuracy: 0.8519 - val_loss: 0.2270 - val_accuracy: 0.8243\n",
      "Epoch 162/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.8529\n",
      "Epoch 162: val_accuracy did not improve from 0.82432\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1965 - accuracy: 0.8529 - val_loss: 0.2266 - val_accuracy: 0.8243\n",
      "Epoch 163/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1960 - accuracy: 0.8538\n",
      "Epoch 163: val_accuracy did not improve from 0.82432\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1960 - accuracy: 0.8538 - val_loss: 0.2261 - val_accuracy: 0.8243\n",
      "Epoch 164/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.8538\n",
      "Epoch 164: val_accuracy did not improve from 0.82432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1955 - accuracy: 0.8538 - val_loss: 0.2257 - val_accuracy: 0.8243\n",
      "Epoch 165/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.8538\n",
      "Epoch 165: val_accuracy did not improve from 0.82432\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1950 - accuracy: 0.8538 - val_loss: 0.2253 - val_accuracy: 0.8243\n",
      "Epoch 166/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.8538\n",
      "Epoch 166: val_accuracy did not improve from 0.82432\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1946 - accuracy: 0.8538 - val_loss: 0.2248 - val_accuracy: 0.8221\n",
      "Epoch 167/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.8538\n",
      "Epoch 167: val_accuracy did not improve from 0.82432\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1941 - accuracy: 0.8538 - val_loss: 0.2244 - val_accuracy: 0.8221\n",
      "Epoch 168/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.8538\n",
      "Epoch 168: val_accuracy did not improve from 0.82432\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1936 - accuracy: 0.8538 - val_loss: 0.2240 - val_accuracy: 0.8221\n",
      "Epoch 169/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.8538\n",
      "Epoch 169: val_accuracy did not improve from 0.82432\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1932 - accuracy: 0.8538 - val_loss: 0.2235 - val_accuracy: 0.8243\n",
      "Epoch 170/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.8538\n",
      "Epoch 170: val_accuracy improved from 0.82432 to 0.82658, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1927 - accuracy: 0.8538 - val_loss: 0.2231 - val_accuracy: 0.8266\n",
      "Epoch 171/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1922 - accuracy: 0.8538\n",
      "Epoch 171: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1922 - accuracy: 0.8538 - val_loss: 0.2227 - val_accuracy: 0.8266\n",
      "Epoch 172/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.8529\n",
      "Epoch 172: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1918 - accuracy: 0.8529 - val_loss: 0.2223 - val_accuracy: 0.8266\n",
      "Epoch 173/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1913 - accuracy: 0.8529\n",
      "Epoch 173: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1913 - accuracy: 0.8529 - val_loss: 0.2219 - val_accuracy: 0.8266\n",
      "Epoch 174/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.8548\n",
      "Epoch 174: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1909 - accuracy: 0.8548 - val_loss: 0.2214 - val_accuracy: 0.8266\n",
      "Epoch 175/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.8538\n",
      "Epoch 175: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1904 - accuracy: 0.8538 - val_loss: 0.2210 - val_accuracy: 0.8266\n",
      "Epoch 176/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.8538\n",
      "Epoch 176: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1900 - accuracy: 0.8538 - val_loss: 0.2206 - val_accuracy: 0.8266\n",
      "Epoch 177/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1895 - accuracy: 0.8538\n",
      "Epoch 177: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1895 - accuracy: 0.8538 - val_loss: 0.2202 - val_accuracy: 0.8266\n",
      "Epoch 178/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.8538\n",
      "Epoch 178: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1891 - accuracy: 0.8538 - val_loss: 0.2198 - val_accuracy: 0.8266\n",
      "Epoch 179/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1886 - accuracy: 0.8538\n",
      "Epoch 179: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1886 - accuracy: 0.8538 - val_loss: 0.2194 - val_accuracy: 0.8266\n",
      "Epoch 180/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1882 - accuracy: 0.8548\n",
      "Epoch 180: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1882 - accuracy: 0.8548 - val_loss: 0.2190 - val_accuracy: 0.8266\n",
      "Epoch 181/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.8548\n",
      "Epoch 181: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1878 - accuracy: 0.8548 - val_loss: 0.2186 - val_accuracy: 0.8266\n",
      "Epoch 182/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.8548\n",
      "Epoch 182: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1873 - accuracy: 0.8548 - val_loss: 0.2182 - val_accuracy: 0.8266\n",
      "Epoch 183/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.8558\n",
      "Epoch 183: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1869 - accuracy: 0.8558 - val_loss: 0.2178 - val_accuracy: 0.8266\n",
      "Epoch 184/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.8558\n",
      "Epoch 184: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1865 - accuracy: 0.8558 - val_loss: 0.2174 - val_accuracy: 0.8266\n",
      "Epoch 185/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.8558\n",
      "Epoch 185: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1861 - accuracy: 0.8558 - val_loss: 0.2170 - val_accuracy: 0.8266\n",
      "Epoch 186/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.8558\n",
      "Epoch 186: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1856 - accuracy: 0.8558 - val_loss: 0.2166 - val_accuracy: 0.8266\n",
      "Epoch 187/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.8558\n",
      "Epoch 187: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1852 - accuracy: 0.8558 - val_loss: 0.2162 - val_accuracy: 0.8266\n",
      "Epoch 188/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1848 - accuracy: 0.8567\n",
      "Epoch 188: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1848 - accuracy: 0.8567 - val_loss: 0.2159 - val_accuracy: 0.8266\n",
      "Epoch 189/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.8577\n",
      "Epoch 189: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1844 - accuracy: 0.8577 - val_loss: 0.2155 - val_accuracy: 0.8266\n",
      "Epoch 190/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.8587\n",
      "Epoch 190: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1840 - accuracy: 0.8587 - val_loss: 0.2151 - val_accuracy: 0.8266\n",
      "Epoch 191/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.8596\n",
      "Epoch 191: val_accuracy did not improve from 0.82658\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1836 - accuracy: 0.8596 - val_loss: 0.2147 - val_accuracy: 0.8266\n",
      "Epoch 192/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1831 - accuracy: 0.8596\n",
      "Epoch 192: val_accuracy improved from 0.82658 to 0.82883, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1831 - accuracy: 0.8596 - val_loss: 0.2143 - val_accuracy: 0.8288\n",
      "Epoch 193/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1827 - accuracy: 0.8596\n",
      "Epoch 193: val_accuracy improved from 0.82883 to 0.83108, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1827 - accuracy: 0.8596 - val_loss: 0.2139 - val_accuracy: 0.8311\n",
      "Epoch 194/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.8596\n",
      "Epoch 194: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1823 - accuracy: 0.8596 - val_loss: 0.2136 - val_accuracy: 0.8311\n",
      "Epoch 195/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.8606\n",
      "Epoch 195: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1819 - accuracy: 0.8606 - val_loss: 0.2132 - val_accuracy: 0.8311\n",
      "Epoch 196/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.8606\n",
      "Epoch 196: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1815 - accuracy: 0.8606 - val_loss: 0.2128 - val_accuracy: 0.8311\n",
      "Epoch 197/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.8606\n",
      "Epoch 197: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1811 - accuracy: 0.8606 - val_loss: 0.2124 - val_accuracy: 0.8311\n",
      "Epoch 198/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.8606\n",
      "Epoch 198: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1807 - accuracy: 0.8606 - val_loss: 0.2121 - val_accuracy: 0.8311\n",
      "Epoch 199/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 0.8606\n",
      "Epoch 199: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1803 - accuracy: 0.8606 - val_loss: 0.2117 - val_accuracy: 0.8311\n",
      "Epoch 200/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.8606\n",
      "Epoch 200: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1800 - accuracy: 0.8606 - val_loss: 0.2113 - val_accuracy: 0.8311\n",
      "Epoch 201/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.8606\n",
      "Epoch 201: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1796 - accuracy: 0.8606 - val_loss: 0.2110 - val_accuracy: 0.8311\n",
      "Epoch 202/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1792 - accuracy: 0.8606\n",
      "Epoch 202: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1792 - accuracy: 0.8606 - val_loss: 0.2106 - val_accuracy: 0.8311\n",
      "Epoch 203/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.8606\n",
      "Epoch 203: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1788 - accuracy: 0.8606 - val_loss: 0.2102 - val_accuracy: 0.8311\n",
      "Epoch 204/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1784 - accuracy: 0.8616\n",
      "Epoch 204: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1784 - accuracy: 0.8616 - val_loss: 0.2099 - val_accuracy: 0.8311\n",
      "Epoch 205/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.8616\n",
      "Epoch 205: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1780 - accuracy: 0.8616 - val_loss: 0.2095 - val_accuracy: 0.8311\n",
      "Epoch 206/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.8616\n",
      "Epoch 206: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1776 - accuracy: 0.8616 - val_loss: 0.2092 - val_accuracy: 0.8311\n",
      "Epoch 207/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.8616\n",
      "Epoch 207: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1773 - accuracy: 0.8616 - val_loss: 0.2088 - val_accuracy: 0.8311\n",
      "Epoch 208/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.8616\n",
      "Epoch 208: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1769 - accuracy: 0.8616 - val_loss: 0.2084 - val_accuracy: 0.8311\n",
      "Epoch 209/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.8625\n",
      "Epoch 209: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1765 - accuracy: 0.8625 - val_loss: 0.2081 - val_accuracy: 0.8311\n",
      "Epoch 210/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.8625\n",
      "Epoch 210: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1761 - accuracy: 0.8625 - val_loss: 0.2077 - val_accuracy: 0.8311\n",
      "Epoch 211/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.8625\n",
      "Epoch 211: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1758 - accuracy: 0.8625 - val_loss: 0.2074 - val_accuracy: 0.8311\n",
      "Epoch 212/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.8625\n",
      "Epoch 212: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1754 - accuracy: 0.8625 - val_loss: 0.2070 - val_accuracy: 0.8311\n",
      "Epoch 213/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.8625\n",
      "Epoch 213: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1750 - accuracy: 0.8625 - val_loss: 0.2067 - val_accuracy: 0.8311\n",
      "Epoch 214/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.8625\n",
      "Epoch 214: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1747 - accuracy: 0.8625 - val_loss: 0.2063 - val_accuracy: 0.8311\n",
      "Epoch 215/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1743 - accuracy: 0.8625\n",
      "Epoch 215: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1743 - accuracy: 0.8625 - val_loss: 0.2060 - val_accuracy: 0.8311\n",
      "Epoch 216/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.8625\n",
      "Epoch 216: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1739 - accuracy: 0.8625 - val_loss: 0.2057 - val_accuracy: 0.8311\n",
      "Epoch 217/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1736 - accuracy: 0.8625\n",
      "Epoch 217: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1736 - accuracy: 0.8625 - val_loss: 0.2053 - val_accuracy: 0.8311\n",
      "Epoch 218/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.8625\n",
      "Epoch 218: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1732 - accuracy: 0.8625 - val_loss: 0.2050 - val_accuracy: 0.8311\n",
      "Epoch 219/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.8625\n",
      "Epoch 219: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1729 - accuracy: 0.8625 - val_loss: 0.2046 - val_accuracy: 0.8311\n",
      "Epoch 220/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.8625\n",
      "Epoch 220: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1725 - accuracy: 0.8625 - val_loss: 0.2043 - val_accuracy: 0.8311\n",
      "Epoch 221/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.8616\n",
      "Epoch 221: val_accuracy did not improve from 0.83108\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1721 - accuracy: 0.8616 - val_loss: 0.2040 - val_accuracy: 0.8311\n",
      "Epoch 222/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.8625\n",
      "Epoch 222: val_accuracy improved from 0.83108 to 0.83333, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1718 - accuracy: 0.8625 - val_loss: 0.2036 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.8625\n",
      "Epoch 223: val_accuracy did not improve from 0.83333\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1714 - accuracy: 0.8625 - val_loss: 0.2033 - val_accuracy: 0.8333\n",
      "Epoch 224/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.8625\n",
      "Epoch 224: val_accuracy did not improve from 0.83333\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1711 - accuracy: 0.8625 - val_loss: 0.2030 - val_accuracy: 0.8333\n",
      "Epoch 225/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.8625\n",
      "Epoch 225: val_accuracy improved from 0.83333 to 0.83559, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1707 - accuracy: 0.8625 - val_loss: 0.2026 - val_accuracy: 0.8356\n",
      "Epoch 226/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.8654\n",
      "Epoch 226: val_accuracy improved from 0.83559 to 0.83784, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1704 - accuracy: 0.8654 - val_loss: 0.2023 - val_accuracy: 0.8378\n",
      "Epoch 227/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.8654\n",
      "Epoch 227: val_accuracy did not improve from 0.83784\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1701 - accuracy: 0.8654 - val_loss: 0.2020 - val_accuracy: 0.8378\n",
      "Epoch 228/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1697 - accuracy: 0.8654\n",
      "Epoch 228: val_accuracy did not improve from 0.83784\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1697 - accuracy: 0.8654 - val_loss: 0.2016 - val_accuracy: 0.8378\n",
      "Epoch 229/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.8654\n",
      "Epoch 229: val_accuracy did not improve from 0.83784\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1694 - accuracy: 0.8654 - val_loss: 0.2013 - val_accuracy: 0.8378\n",
      "Epoch 230/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.8654\n",
      "Epoch 230: val_accuracy did not improve from 0.83784\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1690 - accuracy: 0.8654 - val_loss: 0.2010 - val_accuracy: 0.8378\n",
      "Epoch 231/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.8674\n",
      "Epoch 231: val_accuracy did not improve from 0.83784\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1687 - accuracy: 0.8674 - val_loss: 0.2007 - val_accuracy: 0.8378\n",
      "Epoch 232/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.8683\n",
      "Epoch 232: val_accuracy did not improve from 0.83784\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1683 - accuracy: 0.8683 - val_loss: 0.2003 - val_accuracy: 0.8378\n",
      "Epoch 233/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.8683\n",
      "Epoch 233: val_accuracy improved from 0.83784 to 0.84009, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1680 - accuracy: 0.8683 - val_loss: 0.2000 - val_accuracy: 0.8401\n",
      "Epoch 234/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.8683\n",
      "Epoch 234: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1677 - accuracy: 0.8683 - val_loss: 0.1997 - val_accuracy: 0.8401\n",
      "Epoch 235/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.8683\n",
      "Epoch 235: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1673 - accuracy: 0.8683 - val_loss: 0.1994 - val_accuracy: 0.8401\n",
      "Epoch 236/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.8683\n",
      "Epoch 236: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1670 - accuracy: 0.8683 - val_loss: 0.1991 - val_accuracy: 0.8401\n",
      "Epoch 237/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.8683\n",
      "Epoch 237: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1667 - accuracy: 0.8683 - val_loss: 0.1988 - val_accuracy: 0.8401\n",
      "Epoch 238/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.8683\n",
      "Epoch 238: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1664 - accuracy: 0.8683 - val_loss: 0.1984 - val_accuracy: 0.8401\n",
      "Epoch 239/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.8683\n",
      "Epoch 239: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1660 - accuracy: 0.8683 - val_loss: 0.1981 - val_accuracy: 0.8401\n",
      "Epoch 240/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.8683\n",
      "Epoch 240: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1657 - accuracy: 0.8683 - val_loss: 0.1978 - val_accuracy: 0.8401\n",
      "Epoch 241/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.8683\n",
      "Epoch 241: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1654 - accuracy: 0.8683 - val_loss: 0.1975 - val_accuracy: 0.8401\n",
      "Epoch 242/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.8683\n",
      "Epoch 242: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1651 - accuracy: 0.8683 - val_loss: 0.1972 - val_accuracy: 0.8401\n",
      "Epoch 243/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.8693\n",
      "Epoch 243: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1647 - accuracy: 0.8693 - val_loss: 0.1969 - val_accuracy: 0.8401\n",
      "Epoch 244/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.8693\n",
      "Epoch 244: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1644 - accuracy: 0.8693 - val_loss: 0.1966 - val_accuracy: 0.8401\n",
      "Epoch 245/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.8693\n",
      "Epoch 245: val_accuracy did not improve from 0.84009\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1641 - accuracy: 0.8693 - val_loss: 0.1963 - val_accuracy: 0.8401\n",
      "Epoch 246/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.8693\n",
      "Epoch 246: val_accuracy improved from 0.84009 to 0.84234, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1638 - accuracy: 0.8693 - val_loss: 0.1960 - val_accuracy: 0.8423\n",
      "Epoch 247/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1635 - accuracy: 0.8693\n",
      "Epoch 247: val_accuracy did not improve from 0.84234\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1635 - accuracy: 0.8693 - val_loss: 0.1957 - val_accuracy: 0.8423\n",
      "Epoch 248/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.8693\n",
      "Epoch 248: val_accuracy did not improve from 0.84234\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1631 - accuracy: 0.8693 - val_loss: 0.1954 - val_accuracy: 0.8423\n",
      "Epoch 249/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.8693\n",
      "Epoch 249: val_accuracy improved from 0.84234 to 0.84459, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1628 - accuracy: 0.8693 - val_loss: 0.1951 - val_accuracy: 0.8446\n",
      "Epoch 250/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.8693\n",
      "Epoch 250: val_accuracy did not improve from 0.84459\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1625 - accuracy: 0.8693 - val_loss: 0.1948 - val_accuracy: 0.8446\n",
      "Epoch 251/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.8693\n",
      "Epoch 251: val_accuracy did not improve from 0.84459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1622 - accuracy: 0.8693 - val_loss: 0.1945 - val_accuracy: 0.8446\n",
      "Epoch 252/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1619 - accuracy: 0.8693\n",
      "Epoch 252: val_accuracy did not improve from 0.84459\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1619 - accuracy: 0.8693 - val_loss: 0.1942 - val_accuracy: 0.8446\n",
      "Epoch 253/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.8703\n",
      "Epoch 253: val_accuracy did not improve from 0.84459\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1616 - accuracy: 0.8703 - val_loss: 0.1939 - val_accuracy: 0.8446\n",
      "Epoch 254/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.8712\n",
      "Epoch 254: val_accuracy improved from 0.84459 to 0.84685, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1613 - accuracy: 0.8712 - val_loss: 0.1936 - val_accuracy: 0.8468\n",
      "Epoch 255/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.8712\n",
      "Epoch 255: val_accuracy improved from 0.84685 to 0.84910, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1610 - accuracy: 0.8712 - val_loss: 0.1933 - val_accuracy: 0.8491\n",
      "Epoch 256/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.8722\n",
      "Epoch 256: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1607 - accuracy: 0.8722 - val_loss: 0.1930 - val_accuracy: 0.8491\n",
      "Epoch 257/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.8742\n",
      "Epoch 257: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1604 - accuracy: 0.8742 - val_loss: 0.1927 - val_accuracy: 0.8491\n",
      "Epoch 258/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.8742\n",
      "Epoch 258: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1601 - accuracy: 0.8742 - val_loss: 0.1924 - val_accuracy: 0.8491\n",
      "Epoch 259/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.8742\n",
      "Epoch 259: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1598 - accuracy: 0.8742 - val_loss: 0.1921 - val_accuracy: 0.8491\n",
      "Epoch 260/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1595 - accuracy: 0.8742\n",
      "Epoch 260: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1595 - accuracy: 0.8742 - val_loss: 0.1918 - val_accuracy: 0.8491\n",
      "Epoch 261/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.8751\n",
      "Epoch 261: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1592 - accuracy: 0.8751 - val_loss: 0.1915 - val_accuracy: 0.8491\n",
      "Epoch 262/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1589 - accuracy: 0.8751\n",
      "Epoch 262: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1589 - accuracy: 0.8751 - val_loss: 0.1912 - val_accuracy: 0.8491\n",
      "Epoch 263/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.8761\n",
      "Epoch 263: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1586 - accuracy: 0.8761 - val_loss: 0.1909 - val_accuracy: 0.8491\n",
      "Epoch 264/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1583 - accuracy: 0.8771\n",
      "Epoch 264: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1583 - accuracy: 0.8771 - val_loss: 0.1907 - val_accuracy: 0.8491\n",
      "Epoch 265/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1580 - accuracy: 0.8771\n",
      "Epoch 265: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1580 - accuracy: 0.8771 - val_loss: 0.1904 - val_accuracy: 0.8491\n",
      "Epoch 266/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.8771\n",
      "Epoch 266: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1577 - accuracy: 0.8771 - val_loss: 0.1901 - val_accuracy: 0.8491\n",
      "Epoch 267/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.8771\n",
      "Epoch 267: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1574 - accuracy: 0.8771 - val_loss: 0.1898 - val_accuracy: 0.8491\n",
      "Epoch 268/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.8790\n",
      "Epoch 268: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1571 - accuracy: 0.8790 - val_loss: 0.1895 - val_accuracy: 0.8491\n",
      "Epoch 269/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.8790\n",
      "Epoch 269: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1568 - accuracy: 0.8790 - val_loss: 0.1892 - val_accuracy: 0.8491\n",
      "Epoch 270/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.8790\n",
      "Epoch 270: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1565 - accuracy: 0.8790 - val_loss: 0.1890 - val_accuracy: 0.8491\n",
      "Epoch 271/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.8790\n",
      "Epoch 271: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1562 - accuracy: 0.8790 - val_loss: 0.1887 - val_accuracy: 0.8491\n",
      "Epoch 272/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.8800\n",
      "Epoch 272: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1560 - accuracy: 0.8800 - val_loss: 0.1884 - val_accuracy: 0.8491\n",
      "Epoch 273/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.8800\n",
      "Epoch 273: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1557 - accuracy: 0.8800 - val_loss: 0.1881 - val_accuracy: 0.8491\n",
      "Epoch 274/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.8800\n",
      "Epoch 274: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1554 - accuracy: 0.8800 - val_loss: 0.1878 - val_accuracy: 0.8491\n",
      "Epoch 275/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.8800\n",
      "Epoch 275: val_accuracy did not improve from 0.84910\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1551 - accuracy: 0.8800 - val_loss: 0.1876 - val_accuracy: 0.8491\n",
      "Epoch 276/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.8809\n",
      "Epoch 276: val_accuracy improved from 0.84910 to 0.85135, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1548 - accuracy: 0.8809 - val_loss: 0.1873 - val_accuracy: 0.8514\n",
      "Epoch 277/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1545 - accuracy: 0.8819\n",
      "Epoch 277: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1545 - accuracy: 0.8819 - val_loss: 0.1870 - val_accuracy: 0.8514\n",
      "Epoch 278/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.8819\n",
      "Epoch 278: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1543 - accuracy: 0.8819 - val_loss: 0.1867 - val_accuracy: 0.8514\n",
      "Epoch 279/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.8819\n",
      "Epoch 279: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1540 - accuracy: 0.8819 - val_loss: 0.1865 - val_accuracy: 0.8514\n",
      "Epoch 280/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1537 - accuracy: 0.8819\n",
      "Epoch 280: val_accuracy did not improve from 0.85135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1537 - accuracy: 0.8819 - val_loss: 0.1862 - val_accuracy: 0.8514\n",
      "Epoch 281/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.8819\n",
      "Epoch 281: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1534 - accuracy: 0.8819 - val_loss: 0.1859 - val_accuracy: 0.8514\n",
      "Epoch 282/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1532 - accuracy: 0.8819\n",
      "Epoch 282: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1532 - accuracy: 0.8819 - val_loss: 0.1857 - val_accuracy: 0.8514\n",
      "Epoch 283/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.8819\n",
      "Epoch 283: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1529 - accuracy: 0.8819 - val_loss: 0.1854 - val_accuracy: 0.8514\n",
      "Epoch 284/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.8829\n",
      "Epoch 284: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1526 - accuracy: 0.8829 - val_loss: 0.1851 - val_accuracy: 0.8514\n",
      "Epoch 285/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1523 - accuracy: 0.8838\n",
      "Epoch 285: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1523 - accuracy: 0.8838 - val_loss: 0.1849 - val_accuracy: 0.8514\n",
      "Epoch 286/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.8838\n",
      "Epoch 286: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1521 - accuracy: 0.8838 - val_loss: 0.1846 - val_accuracy: 0.8514\n",
      "Epoch 287/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.8838\n",
      "Epoch 287: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1518 - accuracy: 0.8838 - val_loss: 0.1843 - val_accuracy: 0.8514\n",
      "Epoch 288/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.8838\n",
      "Epoch 288: val_accuracy did not improve from 0.85135\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1515 - accuracy: 0.8838 - val_loss: 0.1841 - val_accuracy: 0.8514\n",
      "Epoch 289/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.8838\n",
      "Epoch 289: val_accuracy improved from 0.85135 to 0.85360, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1513 - accuracy: 0.8838 - val_loss: 0.1838 - val_accuracy: 0.8536\n",
      "Epoch 290/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1510 - accuracy: 0.8838\n",
      "Epoch 290: val_accuracy did not improve from 0.85360\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1510 - accuracy: 0.8838 - val_loss: 0.1835 - val_accuracy: 0.8536\n",
      "Epoch 291/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1507 - accuracy: 0.8838\n",
      "Epoch 291: val_accuracy did not improve from 0.85360\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1507 - accuracy: 0.8838 - val_loss: 0.1833 - val_accuracy: 0.8536\n",
      "Epoch 292/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.8838\n",
      "Epoch 292: val_accuracy did not improve from 0.85360\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1505 - accuracy: 0.8838 - val_loss: 0.1830 - val_accuracy: 0.8536\n",
      "Epoch 293/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.8838\n",
      "Epoch 293: val_accuracy did not improve from 0.85360\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1502 - accuracy: 0.8838 - val_loss: 0.1827 - val_accuracy: 0.8536\n",
      "Epoch 294/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.8838\n",
      "Epoch 294: val_accuracy did not improve from 0.85360\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1499 - accuracy: 0.8838 - val_loss: 0.1825 - val_accuracy: 0.8536\n",
      "Epoch 295/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.8838\n",
      "Epoch 295: val_accuracy did not improve from 0.85360\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1497 - accuracy: 0.8838 - val_loss: 0.1822 - val_accuracy: 0.8536\n",
      "Epoch 296/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1494 - accuracy: 0.8838\n",
      "Epoch 296: val_accuracy did not improve from 0.85360\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1494 - accuracy: 0.8838 - val_loss: 0.1820 - val_accuracy: 0.8536\n",
      "Epoch 297/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.8838\n",
      "Epoch 297: val_accuracy did not improve from 0.85360\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1491 - accuracy: 0.8838 - val_loss: 0.1817 - val_accuracy: 0.8536\n",
      "Epoch 298/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.8838\n",
      "Epoch 298: val_accuracy did not improve from 0.85360\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1489 - accuracy: 0.8838 - val_loss: 0.1814 - val_accuracy: 0.8536\n",
      "Epoch 299/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.8838\n",
      "Epoch 299: val_accuracy improved from 0.85360 to 0.85586, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1486 - accuracy: 0.8838 - val_loss: 0.1812 - val_accuracy: 0.8559\n",
      "Epoch 300/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.8838\n",
      "Epoch 300: val_accuracy did not improve from 0.85586\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1484 - accuracy: 0.8838 - val_loss: 0.1809 - val_accuracy: 0.8559\n",
      "Epoch 301/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.8838\n",
      "Epoch 301: val_accuracy did not improve from 0.85586\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1481 - accuracy: 0.8838 - val_loss: 0.1807 - val_accuracy: 0.8559\n",
      "Epoch 302/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.8838\n",
      "Epoch 302: val_accuracy did not improve from 0.85586\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1479 - accuracy: 0.8838 - val_loss: 0.1804 - val_accuracy: 0.8559\n",
      "Epoch 303/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.8838\n",
      "Epoch 303: val_accuracy did not improve from 0.85586\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1476 - accuracy: 0.8838 - val_loss: 0.1802 - val_accuracy: 0.8559\n",
      "Epoch 304/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.8848\n",
      "Epoch 304: val_accuracy did not improve from 0.85586\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1473 - accuracy: 0.8848 - val_loss: 0.1799 - val_accuracy: 0.8559\n",
      "Epoch 305/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 0.8848\n",
      "Epoch 305: val_accuracy did not improve from 0.85586\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1471 - accuracy: 0.8848 - val_loss: 0.1797 - val_accuracy: 0.8559\n",
      "Epoch 306/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.8858\n",
      "Epoch 306: val_accuracy did not improve from 0.85586\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1468 - accuracy: 0.8858 - val_loss: 0.1794 - val_accuracy: 0.8559\n",
      "Epoch 307/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.8858\n",
      "Epoch 307: val_accuracy did not improve from 0.85586\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1466 - accuracy: 0.8858 - val_loss: 0.1792 - val_accuracy: 0.8559\n",
      "Epoch 308/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.8858\n",
      "Epoch 308: val_accuracy did not improve from 0.85586\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1463 - accuracy: 0.8858 - val_loss: 0.1789 - val_accuracy: 0.8559\n",
      "Epoch 309/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.8858\n",
      "Epoch 309: val_accuracy improved from 0.85586 to 0.85811, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1461 - accuracy: 0.8858 - val_loss: 0.1787 - val_accuracy: 0.8581\n",
      "Epoch 310/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.8867\n",
      "Epoch 310: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1458 - accuracy: 0.8867 - val_loss: 0.1784 - val_accuracy: 0.8581\n",
      "Epoch 311/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.8867\n",
      "Epoch 311: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1456 - accuracy: 0.8867 - val_loss: 0.1782 - val_accuracy: 0.8581\n",
      "Epoch 312/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.8867\n",
      "Epoch 312: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1453 - accuracy: 0.8867 - val_loss: 0.1779 - val_accuracy: 0.8581\n",
      "Epoch 313/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.8877\n",
      "Epoch 313: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1451 - accuracy: 0.8877 - val_loss: 0.1777 - val_accuracy: 0.8581\n",
      "Epoch 314/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.8877\n",
      "Epoch 314: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1448 - accuracy: 0.8877 - val_loss: 0.1774 - val_accuracy: 0.8581\n",
      "Epoch 315/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.8877\n",
      "Epoch 315: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1446 - accuracy: 0.8877 - val_loss: 0.1772 - val_accuracy: 0.8581\n",
      "Epoch 316/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.8877\n",
      "Epoch 316: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1444 - accuracy: 0.8877 - val_loss: 0.1769 - val_accuracy: 0.8581\n",
      "Epoch 317/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.8877\n",
      "Epoch 317: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1441 - accuracy: 0.8877 - val_loss: 0.1767 - val_accuracy: 0.8581\n",
      "Epoch 318/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.8887\n",
      "Epoch 318: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1439 - accuracy: 0.8887 - val_loss: 0.1765 - val_accuracy: 0.8581\n",
      "Epoch 319/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.8896\n",
      "Epoch 319: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1436 - accuracy: 0.8896 - val_loss: 0.1762 - val_accuracy: 0.8581\n",
      "Epoch 320/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.8896\n",
      "Epoch 320: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1434 - accuracy: 0.8896 - val_loss: 0.1760 - val_accuracy: 0.8581\n",
      "Epoch 321/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.8916\n",
      "Epoch 321: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1431 - accuracy: 0.8916 - val_loss: 0.1757 - val_accuracy: 0.8581\n",
      "Epoch 322/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1429 - accuracy: 0.8925\n",
      "Epoch 322: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1429 - accuracy: 0.8925 - val_loss: 0.1755 - val_accuracy: 0.8581\n",
      "Epoch 323/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.8925\n",
      "Epoch 323: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1427 - accuracy: 0.8925 - val_loss: 0.1753 - val_accuracy: 0.8581\n",
      "Epoch 324/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.8925\n",
      "Epoch 324: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1424 - accuracy: 0.8925 - val_loss: 0.1750 - val_accuracy: 0.8581\n",
      "Epoch 325/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.8925\n",
      "Epoch 325: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1422 - accuracy: 0.8925 - val_loss: 0.1748 - val_accuracy: 0.8581\n",
      "Epoch 326/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.8925\n",
      "Epoch 326: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1420 - accuracy: 0.8925 - val_loss: 0.1745 - val_accuracy: 0.8581\n",
      "Epoch 327/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.8916\n",
      "Epoch 327: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1417 - accuracy: 0.8916 - val_loss: 0.1743 - val_accuracy: 0.8581\n",
      "Epoch 328/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.8916\n",
      "Epoch 328: val_accuracy did not improve from 0.85811\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1415 - accuracy: 0.8916 - val_loss: 0.1741 - val_accuracy: 0.8581\n",
      "Epoch 329/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.8916\n",
      "Epoch 329: val_accuracy improved from 0.85811 to 0.86036, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1413 - accuracy: 0.8916 - val_loss: 0.1738 - val_accuracy: 0.8604\n",
      "Epoch 330/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.8916\n",
      "Epoch 330: val_accuracy did not improve from 0.86036\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1410 - accuracy: 0.8916 - val_loss: 0.1736 - val_accuracy: 0.8604\n",
      "Epoch 331/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1408 - accuracy: 0.8916\n",
      "Epoch 331: val_accuracy improved from 0.86036 to 0.86261, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1408 - accuracy: 0.8916 - val_loss: 0.1733 - val_accuracy: 0.8626\n",
      "Epoch 332/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.8916\n",
      "Epoch 332: val_accuracy did not improve from 0.86261\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1406 - accuracy: 0.8916 - val_loss: 0.1732 - val_accuracy: 0.8626\n",
      "Epoch 333/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.8916\n",
      "Epoch 333: val_accuracy improved from 0.86261 to 0.86486, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1403 - accuracy: 0.8916 - val_loss: 0.1728 - val_accuracy: 0.8649\n",
      "Epoch 334/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.8916\n",
      "Epoch 334: val_accuracy did not improve from 0.86486\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1401 - accuracy: 0.8916 - val_loss: 0.1727 - val_accuracy: 0.8649\n",
      "Epoch 335/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.8916\n",
      "Epoch 335: val_accuracy did not improve from 0.86486\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1399 - accuracy: 0.8916 - val_loss: 0.1724 - val_accuracy: 0.8649\n",
      "Epoch 336/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.8916\n",
      "Epoch 336: val_accuracy improved from 0.86486 to 0.86712, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1396 - accuracy: 0.8916 - val_loss: 0.1722 - val_accuracy: 0.8671\n",
      "Epoch 337/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1394 - accuracy: 0.8925\n",
      "Epoch 337: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1394 - accuracy: 0.8925 - val_loss: 0.1720 - val_accuracy: 0.8671\n",
      "Epoch 338/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.8925\n",
      "Epoch 338: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1392 - accuracy: 0.8925 - val_loss: 0.1717 - val_accuracy: 0.8671\n",
      "Epoch 339/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.8925\n",
      "Epoch 339: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1390 - accuracy: 0.8925 - val_loss: 0.1716 - val_accuracy: 0.8671\n",
      "Epoch 340/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.8925\n",
      "Epoch 340: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1387 - accuracy: 0.8925 - val_loss: 0.1713 - val_accuracy: 0.8671\n",
      "Epoch 341/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.8955\n",
      "Epoch 341: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1385 - accuracy: 0.8955 - val_loss: 0.1710 - val_accuracy: 0.8671\n",
      "Epoch 342/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.8955\n",
      "Epoch 342: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1383 - accuracy: 0.8955 - val_loss: 0.1709 - val_accuracy: 0.8671\n",
      "Epoch 343/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.8955\n",
      "Epoch 343: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1381 - accuracy: 0.8955 - val_loss: 0.1706 - val_accuracy: 0.8671\n",
      "Epoch 344/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1378 - accuracy: 0.8964\n",
      "Epoch 344: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1378 - accuracy: 0.8964 - val_loss: 0.1704 - val_accuracy: 0.8671\n",
      "Epoch 345/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 0.8964\n",
      "Epoch 345: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1376 - accuracy: 0.8964 - val_loss: 0.1702 - val_accuracy: 0.8671\n",
      "Epoch 346/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 0.8964\n",
      "Epoch 346: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1374 - accuracy: 0.8964 - val_loss: 0.1699 - val_accuracy: 0.8671\n",
      "Epoch 347/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.8964\n",
      "Epoch 347: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1372 - accuracy: 0.8964 - val_loss: 0.1697 - val_accuracy: 0.8671\n",
      "Epoch 348/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.8964\n",
      "Epoch 348: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1370 - accuracy: 0.8964 - val_loss: 0.1695 - val_accuracy: 0.8671\n",
      "Epoch 349/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.8964\n",
      "Epoch 349: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1367 - accuracy: 0.8964 - val_loss: 0.1692 - val_accuracy: 0.8671\n",
      "Epoch 350/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.8964\n",
      "Epoch 350: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1365 - accuracy: 0.8964 - val_loss: 0.1690 - val_accuracy: 0.8671\n",
      "Epoch 351/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.8974\n",
      "Epoch 351: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1363 - accuracy: 0.8974 - val_loss: 0.1688 - val_accuracy: 0.8671\n",
      "Epoch 352/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.8974\n",
      "Epoch 352: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1361 - accuracy: 0.8974 - val_loss: 0.1686 - val_accuracy: 0.8671\n",
      "Epoch 353/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.8974\n",
      "Epoch 353: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1359 - accuracy: 0.8974 - val_loss: 0.1684 - val_accuracy: 0.8671\n",
      "Epoch 354/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.8974\n",
      "Epoch 354: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1357 - accuracy: 0.8974 - val_loss: 0.1682 - val_accuracy: 0.8671\n",
      "Epoch 355/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.8974\n",
      "Epoch 355: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1354 - accuracy: 0.8974 - val_loss: 0.1679 - val_accuracy: 0.8671\n",
      "Epoch 356/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.8984\n",
      "Epoch 356: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1352 - accuracy: 0.8984 - val_loss: 0.1677 - val_accuracy: 0.8671\n",
      "Epoch 357/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.8984\n",
      "Epoch 357: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1350 - accuracy: 0.8984 - val_loss: 0.1675 - val_accuracy: 0.8671\n",
      "Epoch 358/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.8984\n",
      "Epoch 358: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1348 - accuracy: 0.8984 - val_loss: 0.1673 - val_accuracy: 0.8671\n",
      "Epoch 359/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.8984\n",
      "Epoch 359: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1346 - accuracy: 0.8984 - val_loss: 0.1671 - val_accuracy: 0.8671\n",
      "Epoch 360/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.8984\n",
      "Epoch 360: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1344 - accuracy: 0.8984 - val_loss: 0.1668 - val_accuracy: 0.8671\n",
      "Epoch 361/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.8984\n",
      "Epoch 361: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1342 - accuracy: 0.8984 - val_loss: 0.1666 - val_accuracy: 0.8671\n",
      "Epoch 362/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.8984\n",
      "Epoch 362: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1340 - accuracy: 0.8984 - val_loss: 0.1664 - val_accuracy: 0.8671\n",
      "Epoch 363/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.8984\n",
      "Epoch 363: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1337 - accuracy: 0.8984 - val_loss: 0.1662 - val_accuracy: 0.8671\n",
      "Epoch 364/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.8984\n",
      "Epoch 364: val_accuracy did not improve from 0.86712\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1335 - accuracy: 0.8984 - val_loss: 0.1660 - val_accuracy: 0.8671\n",
      "Epoch 365/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.8984\n",
      "Epoch 365: val_accuracy improved from 0.86712 to 0.86937, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1333 - accuracy: 0.8984 - val_loss: 0.1658 - val_accuracy: 0.8694\n",
      "Epoch 366/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.8984\n",
      "Epoch 366: val_accuracy did not improve from 0.86937\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1331 - accuracy: 0.8984 - val_loss: 0.1655 - val_accuracy: 0.8694\n",
      "Epoch 367/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1329 - accuracy: 0.8984\n",
      "Epoch 367: val_accuracy did not improve from 0.86937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1329 - accuracy: 0.8984 - val_loss: 0.1653 - val_accuracy: 0.8694\n",
      "Epoch 368/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.8993\n",
      "Epoch 368: val_accuracy did not improve from 0.86937\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1327 - accuracy: 0.8993 - val_loss: 0.1651 - val_accuracy: 0.8694\n",
      "Epoch 369/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.8993\n",
      "Epoch 369: val_accuracy improved from 0.86937 to 0.87162, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1325 - accuracy: 0.8993 - val_loss: 0.1649 - val_accuracy: 0.8716\n",
      "Epoch 370/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.8993\n",
      "Epoch 370: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1323 - accuracy: 0.8993 - val_loss: 0.1647 - val_accuracy: 0.8716\n",
      "Epoch 371/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9003\n",
      "Epoch 371: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1321 - accuracy: 0.9003 - val_loss: 0.1645 - val_accuracy: 0.8716\n",
      "Epoch 372/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9003\n",
      "Epoch 372: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1319 - accuracy: 0.9003 - val_loss: 0.1643 - val_accuracy: 0.8716\n",
      "Epoch 373/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9003\n",
      "Epoch 373: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1317 - accuracy: 0.9003 - val_loss: 0.1641 - val_accuracy: 0.8716\n",
      "Epoch 374/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9003\n",
      "Epoch 374: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1315 - accuracy: 0.9003 - val_loss: 0.1639 - val_accuracy: 0.8716\n",
      "Epoch 375/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9003\n",
      "Epoch 375: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1313 - accuracy: 0.9003 - val_loss: 0.1636 - val_accuracy: 0.8716\n",
      "Epoch 376/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1311 - accuracy: 0.9003\n",
      "Epoch 376: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1311 - accuracy: 0.9003 - val_loss: 0.1634 - val_accuracy: 0.8716\n",
      "Epoch 377/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9003\n",
      "Epoch 377: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1309 - accuracy: 0.9003 - val_loss: 0.1632 - val_accuracy: 0.8716\n",
      "Epoch 378/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9003\n",
      "Epoch 378: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1307 - accuracy: 0.9003 - val_loss: 0.1630 - val_accuracy: 0.8716\n",
      "Epoch 379/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9003\n",
      "Epoch 379: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1305 - accuracy: 0.9003 - val_loss: 0.1628 - val_accuracy: 0.8716\n",
      "Epoch 380/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 0.9003\n",
      "Epoch 380: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1303 - accuracy: 0.9003 - val_loss: 0.1626 - val_accuracy: 0.8716\n",
      "Epoch 381/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9003\n",
      "Epoch 381: val_accuracy did not improve from 0.87162\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1301 - accuracy: 0.9003 - val_loss: 0.1624 - val_accuracy: 0.8716\n",
      "Epoch 382/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9003\n",
      "Epoch 382: val_accuracy improved from 0.87162 to 0.87387, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1299 - accuracy: 0.9003 - val_loss: 0.1622 - val_accuracy: 0.8739\n",
      "Epoch 383/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9003\n",
      "Epoch 383: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1297 - accuracy: 0.9003 - val_loss: 0.1620 - val_accuracy: 0.8739\n",
      "Epoch 384/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9003\n",
      "Epoch 384: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1295 - accuracy: 0.9003 - val_loss: 0.1618 - val_accuracy: 0.8739\n",
      "Epoch 385/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9003\n",
      "Epoch 385: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1293 - accuracy: 0.9003 - val_loss: 0.1616 - val_accuracy: 0.8739\n",
      "Epoch 386/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9022\n",
      "Epoch 386: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1291 - accuracy: 0.9022 - val_loss: 0.1614 - val_accuracy: 0.8739\n",
      "Epoch 387/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9022\n",
      "Epoch 387: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1289 - accuracy: 0.9022 - val_loss: 0.1612 - val_accuracy: 0.8739\n",
      "Epoch 388/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9022\n",
      "Epoch 388: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1287 - accuracy: 0.9022 - val_loss: 0.1610 - val_accuracy: 0.8739\n",
      "Epoch 389/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9022\n",
      "Epoch 389: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1285 - accuracy: 0.9022 - val_loss: 0.1608 - val_accuracy: 0.8739\n",
      "Epoch 390/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9032\n",
      "Epoch 390: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1283 - accuracy: 0.9032 - val_loss: 0.1606 - val_accuracy: 0.8739\n",
      "Epoch 391/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9042\n",
      "Epoch 391: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1281 - accuracy: 0.9042 - val_loss: 0.1604 - val_accuracy: 0.8739\n",
      "Epoch 392/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9042\n",
      "Epoch 392: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1279 - accuracy: 0.9042 - val_loss: 0.1602 - val_accuracy: 0.8739\n",
      "Epoch 393/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9042\n",
      "Epoch 393: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1277 - accuracy: 0.9042 - val_loss: 0.1600 - val_accuracy: 0.8739\n",
      "Epoch 394/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9051\n",
      "Epoch 394: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1276 - accuracy: 0.9051 - val_loss: 0.1598 - val_accuracy: 0.8739\n",
      "Epoch 395/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9051\n",
      "Epoch 395: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1274 - accuracy: 0.9051 - val_loss: 0.1596 - val_accuracy: 0.8739\n",
      "Epoch 396/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9051\n",
      "Epoch 396: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1272 - accuracy: 0.9051 - val_loss: 0.1594 - val_accuracy: 0.8739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 397/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9051\n",
      "Epoch 397: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1270 - accuracy: 0.9051 - val_loss: 0.1592 - val_accuracy: 0.8739\n",
      "Epoch 398/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9051\n",
      "Epoch 398: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1268 - accuracy: 0.9051 - val_loss: 0.1590 - val_accuracy: 0.8739\n",
      "Epoch 399/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1266 - accuracy: 0.9051\n",
      "Epoch 399: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1266 - accuracy: 0.9051 - val_loss: 0.1588 - val_accuracy: 0.8739\n",
      "Epoch 400/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1264 - accuracy: 0.9051\n",
      "Epoch 400: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1264 - accuracy: 0.9051 - val_loss: 0.1586 - val_accuracy: 0.8739\n",
      "Epoch 401/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9051\n",
      "Epoch 401: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1262 - accuracy: 0.9051 - val_loss: 0.1584 - val_accuracy: 0.8739\n",
      "Epoch 402/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9051\n",
      "Epoch 402: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1260 - accuracy: 0.9051 - val_loss: 0.1582 - val_accuracy: 0.8739\n",
      "Epoch 403/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9051\n",
      "Epoch 403: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1259 - accuracy: 0.9051 - val_loss: 0.1580 - val_accuracy: 0.8739\n",
      "Epoch 404/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9051\n",
      "Epoch 404: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1257 - accuracy: 0.9051 - val_loss: 0.1578 - val_accuracy: 0.8739\n",
      "Epoch 405/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9051\n",
      "Epoch 405: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1255 - accuracy: 0.9051 - val_loss: 0.1576 - val_accuracy: 0.8739\n",
      "Epoch 406/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9051\n",
      "Epoch 406: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1253 - accuracy: 0.9051 - val_loss: 0.1574 - val_accuracy: 0.8739\n",
      "Epoch 407/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9061\n",
      "Epoch 407: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1251 - accuracy: 0.9061 - val_loss: 0.1572 - val_accuracy: 0.8739\n",
      "Epoch 408/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9061\n",
      "Epoch 408: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1249 - accuracy: 0.9061 - val_loss: 0.1570 - val_accuracy: 0.8739\n",
      "Epoch 409/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 0.9061\n",
      "Epoch 409: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1248 - accuracy: 0.9061 - val_loss: 0.1568 - val_accuracy: 0.8739\n",
      "Epoch 410/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9061\n",
      "Epoch 410: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1246 - accuracy: 0.9061 - val_loss: 0.1566 - val_accuracy: 0.8739\n",
      "Epoch 411/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9061\n",
      "Epoch 411: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1244 - accuracy: 0.9061 - val_loss: 0.1565 - val_accuracy: 0.8739\n",
      "Epoch 412/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9061\n",
      "Epoch 412: val_accuracy did not improve from 0.87387\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1242 - accuracy: 0.9061 - val_loss: 0.1563 - val_accuracy: 0.8739\n",
      "Epoch 413/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9061\n",
      "Epoch 413: val_accuracy improved from 0.87387 to 0.87613, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1240 - accuracy: 0.9061 - val_loss: 0.1561 - val_accuracy: 0.8761\n",
      "Epoch 414/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9061\n",
      "Epoch 414: val_accuracy improved from 0.87613 to 0.87838, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1239 - accuracy: 0.9061 - val_loss: 0.1559 - val_accuracy: 0.8784\n",
      "Epoch 415/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9061\n",
      "Epoch 415: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1237 - accuracy: 0.9061 - val_loss: 0.1557 - val_accuracy: 0.8784\n",
      "Epoch 416/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9061\n",
      "Epoch 416: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1235 - accuracy: 0.9061 - val_loss: 0.1555 - val_accuracy: 0.8784\n",
      "Epoch 417/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9061\n",
      "Epoch 417: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1233 - accuracy: 0.9061 - val_loss: 0.1553 - val_accuracy: 0.8784\n",
      "Epoch 418/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9061\n",
      "Epoch 418: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1231 - accuracy: 0.9061 - val_loss: 0.1551 - val_accuracy: 0.8784\n",
      "Epoch 419/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9061\n",
      "Epoch 419: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1230 - accuracy: 0.9061 - val_loss: 0.1549 - val_accuracy: 0.8784\n",
      "Epoch 420/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1228 - accuracy: 0.9061\n",
      "Epoch 420: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1228 - accuracy: 0.9061 - val_loss: 0.1547 - val_accuracy: 0.8784\n",
      "Epoch 421/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9061\n",
      "Epoch 421: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1226 - accuracy: 0.9061 - val_loss: 0.1546 - val_accuracy: 0.8784\n",
      "Epoch 422/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1224 - accuracy: 0.9061\n",
      "Epoch 422: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1224 - accuracy: 0.9061 - val_loss: 0.1544 - val_accuracy: 0.8784\n",
      "Epoch 423/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9061\n",
      "Epoch 423: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1223 - accuracy: 0.9061 - val_loss: 0.1542 - val_accuracy: 0.8784\n",
      "Epoch 424/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9061\n",
      "Epoch 424: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1221 - accuracy: 0.9061 - val_loss: 0.1540 - val_accuracy: 0.8784\n",
      "Epoch 425/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9061\n",
      "Epoch 425: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1219 - accuracy: 0.9061 - val_loss: 0.1538 - val_accuracy: 0.8784\n",
      "Epoch 426/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9061\n",
      "Epoch 426: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1217 - accuracy: 0.9061 - val_loss: 0.1536 - val_accuracy: 0.8784\n",
      "Epoch 427/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9071\n",
      "Epoch 427: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1216 - accuracy: 0.9071 - val_loss: 0.1534 - val_accuracy: 0.8784\n",
      "Epoch 428/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9071\n",
      "Epoch 428: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1214 - accuracy: 0.9071 - val_loss: 0.1533 - val_accuracy: 0.8784\n",
      "Epoch 429/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9071\n",
      "Epoch 429: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1212 - accuracy: 0.9071 - val_loss: 0.1531 - val_accuracy: 0.8784\n",
      "Epoch 430/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9071\n",
      "Epoch 430: val_accuracy did not improve from 0.87838\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1210 - accuracy: 0.9071 - val_loss: 0.1529 - val_accuracy: 0.8784\n",
      "Epoch 431/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9071\n",
      "Epoch 431: val_accuracy improved from 0.87838 to 0.88063, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1209 - accuracy: 0.9071 - val_loss: 0.1527 - val_accuracy: 0.8806\n",
      "Epoch 432/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9061\n",
      "Epoch 432: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1207 - accuracy: 0.9061 - val_loss: 0.1525 - val_accuracy: 0.8806\n",
      "Epoch 433/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9061\n",
      "Epoch 433: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1205 - accuracy: 0.9061 - val_loss: 0.1523 - val_accuracy: 0.8806\n",
      "Epoch 434/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9061\n",
      "Epoch 434: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1204 - accuracy: 0.9061 - val_loss: 0.1522 - val_accuracy: 0.8806\n",
      "Epoch 435/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9061\n",
      "Epoch 435: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1202 - accuracy: 0.9061 - val_loss: 0.1520 - val_accuracy: 0.8806\n",
      "Epoch 436/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9061\n",
      "Epoch 436: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1200 - accuracy: 0.9061 - val_loss: 0.1518 - val_accuracy: 0.8806\n",
      "Epoch 437/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9061\n",
      "Epoch 437: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1198 - accuracy: 0.9061 - val_loss: 0.1516 - val_accuracy: 0.8806\n",
      "Epoch 438/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9061\n",
      "Epoch 438: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1197 - accuracy: 0.9061 - val_loss: 0.1514 - val_accuracy: 0.8806\n",
      "Epoch 439/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9061\n",
      "Epoch 439: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1195 - accuracy: 0.9061 - val_loss: 0.1513 - val_accuracy: 0.8806\n",
      "Epoch 440/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9061\n",
      "Epoch 440: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1193 - accuracy: 0.9061 - val_loss: 0.1511 - val_accuracy: 0.8806\n",
      "Epoch 441/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.9061\n",
      "Epoch 441: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1192 - accuracy: 0.9061 - val_loss: 0.1509 - val_accuracy: 0.8806\n",
      "Epoch 442/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9061\n",
      "Epoch 442: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1190 - accuracy: 0.9061 - val_loss: 0.1507 - val_accuracy: 0.8806\n",
      "Epoch 443/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9061\n",
      "Epoch 443: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1188 - accuracy: 0.9061 - val_loss: 0.1505 - val_accuracy: 0.8806\n",
      "Epoch 444/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9061\n",
      "Epoch 444: val_accuracy did not improve from 0.88063\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1187 - accuracy: 0.9061 - val_loss: 0.1504 - val_accuracy: 0.8806\n",
      "Epoch 445/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9061\n",
      "Epoch 445: val_accuracy improved from 0.88063 to 0.88288, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1185 - accuracy: 0.9061 - val_loss: 0.1502 - val_accuracy: 0.8829\n",
      "Epoch 446/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9061\n",
      "Epoch 446: val_accuracy did not improve from 0.88288\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1183 - accuracy: 0.9061 - val_loss: 0.1500 - val_accuracy: 0.8829\n",
      "Epoch 447/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9061\n",
      "Epoch 447: val_accuracy did not improve from 0.88288\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1182 - accuracy: 0.9061 - val_loss: 0.1498 - val_accuracy: 0.8829\n",
      "Epoch 448/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9061\n",
      "Epoch 448: val_accuracy improved from 0.88288 to 0.88514, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1180 - accuracy: 0.9061 - val_loss: 0.1497 - val_accuracy: 0.8851\n",
      "Epoch 449/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9061\n",
      "Epoch 449: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1179 - accuracy: 0.9061 - val_loss: 0.1495 - val_accuracy: 0.8851\n",
      "Epoch 450/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9061\n",
      "Epoch 450: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1177 - accuracy: 0.9061 - val_loss: 0.1493 - val_accuracy: 0.8851\n",
      "Epoch 451/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9061\n",
      "Epoch 451: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1175 - accuracy: 0.9061 - val_loss: 0.1491 - val_accuracy: 0.8851\n",
      "Epoch 452/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9061\n",
      "Epoch 452: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1174 - accuracy: 0.9061 - val_loss: 0.1489 - val_accuracy: 0.8851\n",
      "Epoch 453/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9061\n",
      "Epoch 453: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1172 - accuracy: 0.9061 - val_loss: 0.1488 - val_accuracy: 0.8851\n",
      "Epoch 454/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9061\n",
      "Epoch 454: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1170 - accuracy: 0.9061 - val_loss: 0.1486 - val_accuracy: 0.8851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9061\n",
      "Epoch 455: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1169 - accuracy: 0.9061 - val_loss: 0.1484 - val_accuracy: 0.8851\n",
      "Epoch 456/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9071\n",
      "Epoch 456: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1167 - accuracy: 0.9071 - val_loss: 0.1483 - val_accuracy: 0.8851\n",
      "Epoch 457/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9071\n",
      "Epoch 457: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1166 - accuracy: 0.9071 - val_loss: 0.1481 - val_accuracy: 0.8851\n",
      "Epoch 458/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9071\n",
      "Epoch 458: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1164 - accuracy: 0.9071 - val_loss: 0.1479 - val_accuracy: 0.8851\n",
      "Epoch 459/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9071\n",
      "Epoch 459: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1162 - accuracy: 0.9071 - val_loss: 0.1477 - val_accuracy: 0.8851\n",
      "Epoch 460/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9071\n",
      "Epoch 460: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1161 - accuracy: 0.9071 - val_loss: 0.1476 - val_accuracy: 0.8851\n",
      "Epoch 461/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9071\n",
      "Epoch 461: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1159 - accuracy: 0.9071 - val_loss: 0.1474 - val_accuracy: 0.8851\n",
      "Epoch 462/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9071\n",
      "Epoch 462: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1158 - accuracy: 0.9071 - val_loss: 0.1472 - val_accuracy: 0.8851\n",
      "Epoch 463/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9071\n",
      "Epoch 463: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1156 - accuracy: 0.9071 - val_loss: 0.1470 - val_accuracy: 0.8851\n",
      "Epoch 464/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9071\n",
      "Epoch 464: val_accuracy did not improve from 0.88514\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1155 - accuracy: 0.9071 - val_loss: 0.1469 - val_accuracy: 0.8851\n",
      "Epoch 465/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9071\n",
      "Epoch 465: val_accuracy improved from 0.88514 to 0.88739, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1153 - accuracy: 0.9071 - val_loss: 0.1467 - val_accuracy: 0.8874\n",
      "Epoch 466/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.9080\n",
      "Epoch 466: val_accuracy did not improve from 0.88739\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1151 - accuracy: 0.9080 - val_loss: 0.1465 - val_accuracy: 0.8874\n",
      "Epoch 467/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9080\n",
      "Epoch 467: val_accuracy did not improve from 0.88739\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1150 - accuracy: 0.9080 - val_loss: 0.1464 - val_accuracy: 0.8874\n",
      "Epoch 468/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9080\n",
      "Epoch 468: val_accuracy did not improve from 0.88739\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1148 - accuracy: 0.9080 - val_loss: 0.1462 - val_accuracy: 0.8874\n",
      "Epoch 469/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9080\n",
      "Epoch 469: val_accuracy did not improve from 0.88739\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1147 - accuracy: 0.9080 - val_loss: 0.1460 - val_accuracy: 0.8874\n",
      "Epoch 470/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9080\n",
      "Epoch 470: val_accuracy did not improve from 0.88739\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1145 - accuracy: 0.9080 - val_loss: 0.1459 - val_accuracy: 0.8874\n",
      "Epoch 471/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9090\n",
      "Epoch 471: val_accuracy improved from 0.88739 to 0.88964, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1144 - accuracy: 0.9090 - val_loss: 0.1457 - val_accuracy: 0.8896\n",
      "Epoch 472/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9090\n",
      "Epoch 472: val_accuracy did not improve from 0.88964\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1142 - accuracy: 0.9090 - val_loss: 0.1455 - val_accuracy: 0.8896\n",
      "Epoch 473/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9090\n",
      "Epoch 473: val_accuracy did not improve from 0.88964\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1141 - accuracy: 0.9090 - val_loss: 0.1454 - val_accuracy: 0.8896\n",
      "Epoch 474/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1139 - accuracy: 0.9090\n",
      "Epoch 474: val_accuracy did not improve from 0.88964\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1139 - accuracy: 0.9090 - val_loss: 0.1452 - val_accuracy: 0.8896\n",
      "Epoch 475/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.9090\n",
      "Epoch 475: val_accuracy did not improve from 0.88964\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1138 - accuracy: 0.9090 - val_loss: 0.1450 - val_accuracy: 0.8896\n",
      "Epoch 476/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9090\n",
      "Epoch 476: val_accuracy did not improve from 0.88964\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1136 - accuracy: 0.9090 - val_loss: 0.1449 - val_accuracy: 0.8896\n",
      "Epoch 477/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9090\n",
      "Epoch 477: val_accuracy did not improve from 0.88964\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1134 - accuracy: 0.9090 - val_loss: 0.1447 - val_accuracy: 0.8896\n",
      "Epoch 478/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9100\n",
      "Epoch 478: val_accuracy did not improve from 0.88964\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1133 - accuracy: 0.9100 - val_loss: 0.1445 - val_accuracy: 0.8896\n",
      "Epoch 479/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9100\n",
      "Epoch 479: val_accuracy did not improve from 0.88964\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1131 - accuracy: 0.9100 - val_loss: 0.1444 - val_accuracy: 0.8896\n",
      "Epoch 480/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9100\n",
      "Epoch 480: val_accuracy improved from 0.88964 to 0.89189, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1130 - accuracy: 0.9100 - val_loss: 0.1442 - val_accuracy: 0.8919\n",
      "Epoch 481/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9100\n",
      "Epoch 481: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1128 - accuracy: 0.9100 - val_loss: 0.1440 - val_accuracy: 0.8919\n",
      "Epoch 482/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9109\n",
      "Epoch 482: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1127 - accuracy: 0.9109 - val_loss: 0.1439 - val_accuracy: 0.8919\n",
      "Epoch 483/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9109\n",
      "Epoch 483: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1125 - accuracy: 0.9109 - val_loss: 0.1437 - val_accuracy: 0.8919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 484/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9109\n",
      "Epoch 484: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1124 - accuracy: 0.9109 - val_loss: 0.1435 - val_accuracy: 0.8919\n",
      "Epoch 485/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.9109\n",
      "Epoch 485: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1122 - accuracy: 0.9109 - val_loss: 0.1434 - val_accuracy: 0.8919\n",
      "Epoch 486/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9109\n",
      "Epoch 486: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1121 - accuracy: 0.9109 - val_loss: 0.1432 - val_accuracy: 0.8919\n",
      "Epoch 487/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9109\n",
      "Epoch 487: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1119 - accuracy: 0.9109 - val_loss: 0.1430 - val_accuracy: 0.8919\n",
      "Epoch 488/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9109\n",
      "Epoch 488: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1118 - accuracy: 0.9109 - val_loss: 0.1429 - val_accuracy: 0.8919\n",
      "Epoch 489/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9119\n",
      "Epoch 489: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1117 - accuracy: 0.9119 - val_loss: 0.1427 - val_accuracy: 0.8919\n",
      "Epoch 490/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9119\n",
      "Epoch 490: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1115 - accuracy: 0.9119 - val_loss: 0.1426 - val_accuracy: 0.8919\n",
      "Epoch 491/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9119\n",
      "Epoch 491: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1114 - accuracy: 0.9119 - val_loss: 0.1424 - val_accuracy: 0.8919\n",
      "Epoch 492/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9119\n",
      "Epoch 492: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1112 - accuracy: 0.9119 - val_loss: 0.1422 - val_accuracy: 0.8919\n",
      "Epoch 493/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1111 - accuracy: 0.9119\n",
      "Epoch 493: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1111 - accuracy: 0.9119 - val_loss: 0.1421 - val_accuracy: 0.8919\n",
      "Epoch 494/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9119\n",
      "Epoch 494: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1109 - accuracy: 0.9119 - val_loss: 0.1419 - val_accuracy: 0.8919\n",
      "Epoch 495/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9119\n",
      "Epoch 495: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1108 - accuracy: 0.9119 - val_loss: 0.1418 - val_accuracy: 0.8919\n",
      "Epoch 496/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9129\n",
      "Epoch 496: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1106 - accuracy: 0.9129 - val_loss: 0.1416 - val_accuracy: 0.8919\n",
      "Epoch 497/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9129\n",
      "Epoch 497: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1105 - accuracy: 0.9129 - val_loss: 0.1414 - val_accuracy: 0.8919\n",
      "Epoch 498/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9129\n",
      "Epoch 498: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1103 - accuracy: 0.9129 - val_loss: 0.1413 - val_accuracy: 0.8919\n",
      "Epoch 499/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9129\n",
      "Epoch 499: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1102 - accuracy: 0.9129 - val_loss: 0.1411 - val_accuracy: 0.8919\n",
      "Epoch 500/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9129\n",
      "Epoch 500: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1101 - accuracy: 0.9129 - val_loss: 0.1410 - val_accuracy: 0.8919\n",
      "Epoch 501/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9129\n",
      "Epoch 501: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1099 - accuracy: 0.9129 - val_loss: 0.1408 - val_accuracy: 0.8919\n",
      "Epoch 502/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9129\n",
      "Epoch 502: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1098 - accuracy: 0.9129 - val_loss: 0.1407 - val_accuracy: 0.8919\n",
      "Epoch 503/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9129\n",
      "Epoch 503: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1096 - accuracy: 0.9129 - val_loss: 0.1405 - val_accuracy: 0.8919\n",
      "Epoch 504/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9129\n",
      "Epoch 504: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1095 - accuracy: 0.9129 - val_loss: 0.1403 - val_accuracy: 0.8919\n",
      "Epoch 505/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9129\n",
      "Epoch 505: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1093 - accuracy: 0.9129 - val_loss: 0.1402 - val_accuracy: 0.8919\n",
      "Epoch 506/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9129\n",
      "Epoch 506: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1092 - accuracy: 0.9129 - val_loss: 0.1400 - val_accuracy: 0.8919\n",
      "Epoch 507/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9129\n",
      "Epoch 507: val_accuracy did not improve from 0.89189\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1091 - accuracy: 0.9129 - val_loss: 0.1399 - val_accuracy: 0.8919\n",
      "Epoch 508/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9129\n",
      "Epoch 508: val_accuracy improved from 0.89189 to 0.89414, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1089 - accuracy: 0.9129 - val_loss: 0.1397 - val_accuracy: 0.8941\n",
      "Epoch 509/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.9129\n",
      "Epoch 509: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1088 - accuracy: 0.9129 - val_loss: 0.1396 - val_accuracy: 0.8941\n",
      "Epoch 510/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9129\n",
      "Epoch 510: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1086 - accuracy: 0.9129 - val_loss: 0.1394 - val_accuracy: 0.8941\n",
      "Epoch 511/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9129\n",
      "Epoch 511: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1085 - accuracy: 0.9129 - val_loss: 0.1392 - val_accuracy: 0.8941\n",
      "Epoch 512/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1084 - accuracy: 0.9129\n",
      "Epoch 512: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1084 - accuracy: 0.9129 - val_loss: 0.1391 - val_accuracy: 0.8941\n",
      "Epoch 513/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9129\n",
      "Epoch 513: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1082 - accuracy: 0.9129 - val_loss: 0.1389 - val_accuracy: 0.8941\n",
      "Epoch 514/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9129\n",
      "Epoch 514: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1081 - accuracy: 0.9129 - val_loss: 0.1388 - val_accuracy: 0.8941\n",
      "Epoch 515/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9129\n",
      "Epoch 515: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1080 - accuracy: 0.9129 - val_loss: 0.1386 - val_accuracy: 0.8941\n",
      "Epoch 516/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9129\n",
      "Epoch 516: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1078 - accuracy: 0.9129 - val_loss: 0.1385 - val_accuracy: 0.8941\n",
      "Epoch 517/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9129\n",
      "Epoch 517: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1077 - accuracy: 0.9129 - val_loss: 0.1383 - val_accuracy: 0.8941\n",
      "Epoch 518/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9129\n",
      "Epoch 518: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1075 - accuracy: 0.9129 - val_loss: 0.1382 - val_accuracy: 0.8941\n",
      "Epoch 519/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9129\n",
      "Epoch 519: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1074 - accuracy: 0.9129 - val_loss: 0.1380 - val_accuracy: 0.8941\n",
      "Epoch 520/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9129\n",
      "Epoch 520: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1073 - accuracy: 0.9129 - val_loss: 0.1379 - val_accuracy: 0.8941\n",
      "Epoch 521/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9129\n",
      "Epoch 521: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1071 - accuracy: 0.9129 - val_loss: 0.1377 - val_accuracy: 0.8941\n",
      "Epoch 522/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9129\n",
      "Epoch 522: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1070 - accuracy: 0.9129 - val_loss: 0.1376 - val_accuracy: 0.8941\n",
      "Epoch 523/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9129\n",
      "Epoch 523: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1069 - accuracy: 0.9129 - val_loss: 0.1374 - val_accuracy: 0.8941\n",
      "Epoch 524/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9129\n",
      "Epoch 524: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1067 - accuracy: 0.9129 - val_loss: 0.1373 - val_accuracy: 0.8941\n",
      "Epoch 525/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9129\n",
      "Epoch 525: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1066 - accuracy: 0.9129 - val_loss: 0.1371 - val_accuracy: 0.8941\n",
      "Epoch 526/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9129\n",
      "Epoch 526: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1065 - accuracy: 0.9129 - val_loss: 0.1370 - val_accuracy: 0.8941\n",
      "Epoch 527/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.9129\n",
      "Epoch 527: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1063 - accuracy: 0.9129 - val_loss: 0.1368 - val_accuracy: 0.8941\n",
      "Epoch 528/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9129\n",
      "Epoch 528: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1062 - accuracy: 0.9129 - val_loss: 0.1367 - val_accuracy: 0.8941\n",
      "Epoch 529/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9129\n",
      "Epoch 529: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1061 - accuracy: 0.9129 - val_loss: 0.1365 - val_accuracy: 0.8941\n",
      "Epoch 530/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9129\n",
      "Epoch 530: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1059 - accuracy: 0.9129 - val_loss: 0.1364 - val_accuracy: 0.8941\n",
      "Epoch 531/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9129\n",
      "Epoch 531: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1058 - accuracy: 0.9129 - val_loss: 0.1362 - val_accuracy: 0.8941\n",
      "Epoch 532/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9129\n",
      "Epoch 532: val_accuracy did not improve from 0.89414\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1057 - accuracy: 0.9129 - val_loss: 0.1361 - val_accuracy: 0.8941\n",
      "Epoch 533/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9138\n",
      "Epoch 533: val_accuracy improved from 0.89414 to 0.89640, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1055 - accuracy: 0.9138 - val_loss: 0.1359 - val_accuracy: 0.8964\n",
      "Epoch 534/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1054 - accuracy: 0.9138\n",
      "Epoch 534: val_accuracy did not improve from 0.89640\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1054 - accuracy: 0.9138 - val_loss: 0.1358 - val_accuracy: 0.8964\n",
      "Epoch 535/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9138\n",
      "Epoch 535: val_accuracy did not improve from 0.89640\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1053 - accuracy: 0.9138 - val_loss: 0.1356 - val_accuracy: 0.8964\n",
      "Epoch 536/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9158\n",
      "Epoch 536: val_accuracy did not improve from 0.89640\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1051 - accuracy: 0.9158 - val_loss: 0.1355 - val_accuracy: 0.8964\n",
      "Epoch 537/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9158\n",
      "Epoch 537: val_accuracy did not improve from 0.89640\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1050 - accuracy: 0.9158 - val_loss: 0.1353 - val_accuracy: 0.8964\n",
      "Epoch 538/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9158\n",
      "Epoch 538: val_accuracy did not improve from 0.89640\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1049 - accuracy: 0.9158 - val_loss: 0.1352 - val_accuracy: 0.8964\n",
      "Epoch 539/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9158\n",
      "Epoch 539: val_accuracy did not improve from 0.89640\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1047 - accuracy: 0.9158 - val_loss: 0.1350 - val_accuracy: 0.8964\n",
      "Epoch 540/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9158\n",
      "Epoch 540: val_accuracy did not improve from 0.89640\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1046 - accuracy: 0.9158 - val_loss: 0.1349 - val_accuracy: 0.8964\n",
      "Epoch 541/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9158\n",
      "Epoch 541: val_accuracy did not improve from 0.89640\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1045 - accuracy: 0.9158 - val_loss: 0.1347 - val_accuracy: 0.8964\n",
      "Epoch 542/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9158\n",
      "Epoch 542: val_accuracy improved from 0.89640 to 0.89865, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1044 - accuracy: 0.9158 - val_loss: 0.1346 - val_accuracy: 0.8986\n",
      "Epoch 543/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9158\n",
      "Epoch 543: val_accuracy improved from 0.89865 to 0.90090, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1042 - accuracy: 0.9158 - val_loss: 0.1345 - val_accuracy: 0.9009\n",
      "Epoch 544/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9158\n",
      "Epoch 544: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1041 - accuracy: 0.9158 - val_loss: 0.1343 - val_accuracy: 0.9009\n",
      "Epoch 545/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9177\n",
      "Epoch 545: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1040 - accuracy: 0.9177 - val_loss: 0.1342 - val_accuracy: 0.9009\n",
      "Epoch 546/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9187\n",
      "Epoch 546: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1038 - accuracy: 0.9187 - val_loss: 0.1340 - val_accuracy: 0.9009\n",
      "Epoch 547/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9187\n",
      "Epoch 547: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1037 - accuracy: 0.9187 - val_loss: 0.1339 - val_accuracy: 0.9009\n",
      "Epoch 548/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9187\n",
      "Epoch 548: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1036 - accuracy: 0.9187 - val_loss: 0.1337 - val_accuracy: 0.9009\n",
      "Epoch 549/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9187\n",
      "Epoch 549: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1035 - accuracy: 0.9187 - val_loss: 0.1336 - val_accuracy: 0.9009\n",
      "Epoch 550/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9187\n",
      "Epoch 550: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1033 - accuracy: 0.9187 - val_loss: 0.1334 - val_accuracy: 0.9009\n",
      "Epoch 551/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9187\n",
      "Epoch 551: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1032 - accuracy: 0.9187 - val_loss: 0.1333 - val_accuracy: 0.9009\n",
      "Epoch 552/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9187\n",
      "Epoch 552: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1031 - accuracy: 0.9187 - val_loss: 0.1332 - val_accuracy: 0.9009\n",
      "Epoch 553/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9187\n",
      "Epoch 553: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1029 - accuracy: 0.9187 - val_loss: 0.1330 - val_accuracy: 0.9009\n",
      "Epoch 554/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9197\n",
      "Epoch 554: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1028 - accuracy: 0.9197 - val_loss: 0.1329 - val_accuracy: 0.9009\n",
      "Epoch 555/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9197\n",
      "Epoch 555: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1027 - accuracy: 0.9197 - val_loss: 0.1327 - val_accuracy: 0.9009\n",
      "Epoch 556/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9197\n",
      "Epoch 556: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1026 - accuracy: 0.9197 - val_loss: 0.1326 - val_accuracy: 0.9009\n",
      "Epoch 557/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9197\n",
      "Epoch 557: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1024 - accuracy: 0.9197 - val_loss: 0.1324 - val_accuracy: 0.9009\n",
      "Epoch 558/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9197\n",
      "Epoch 558: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1023 - accuracy: 0.9197 - val_loss: 0.1323 - val_accuracy: 0.9009\n",
      "Epoch 559/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9197\n",
      "Epoch 559: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1022 - accuracy: 0.9197 - val_loss: 0.1322 - val_accuracy: 0.9009\n",
      "Epoch 560/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9197\n",
      "Epoch 560: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1021 - accuracy: 0.9197 - val_loss: 0.1320 - val_accuracy: 0.9009\n",
      "Epoch 561/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9197\n",
      "Epoch 561: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1020 - accuracy: 0.9197 - val_loss: 0.1319 - val_accuracy: 0.9009\n",
      "Epoch 562/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9197\n",
      "Epoch 562: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1018 - accuracy: 0.9197 - val_loss: 0.1317 - val_accuracy: 0.9009\n",
      "Epoch 563/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9197\n",
      "Epoch 563: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1017 - accuracy: 0.9197 - val_loss: 0.1316 - val_accuracy: 0.9009\n",
      "Epoch 564/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9197\n",
      "Epoch 564: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1016 - accuracy: 0.9197 - val_loss: 0.1315 - val_accuracy: 0.9009\n",
      "Epoch 565/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9197\n",
      "Epoch 565: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1015 - accuracy: 0.9197 - val_loss: 0.1313 - val_accuracy: 0.9009\n",
      "Epoch 566/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9197\n",
      "Epoch 566: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1013 - accuracy: 0.9197 - val_loss: 0.1312 - val_accuracy: 0.9009\n",
      "Epoch 567/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9197\n",
      "Epoch 567: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1012 - accuracy: 0.9197 - val_loss: 0.1310 - val_accuracy: 0.9009\n",
      "Epoch 568/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9197\n",
      "Epoch 568: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1011 - accuracy: 0.9197 - val_loss: 0.1309 - val_accuracy: 0.9009\n",
      "Epoch 569/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9197\n",
      "Epoch 569: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1010 - accuracy: 0.9197 - val_loss: 0.1308 - val_accuracy: 0.9009\n",
      "Epoch 570/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9197\n",
      "Epoch 570: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1009 - accuracy: 0.9197 - val_loss: 0.1306 - val_accuracy: 0.9009\n",
      "Epoch 571/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1007 - accuracy: 0.9197\n",
      "Epoch 571: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1007 - accuracy: 0.9197 - val_loss: 0.1305 - val_accuracy: 0.9009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 572/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9197\n",
      "Epoch 572: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1006 - accuracy: 0.9197 - val_loss: 0.1304 - val_accuracy: 0.9009\n",
      "Epoch 573/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9197\n",
      "Epoch 573: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1005 - accuracy: 0.9197 - val_loss: 0.1302 - val_accuracy: 0.9009\n",
      "Epoch 574/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9197\n",
      "Epoch 574: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1004 - accuracy: 0.9197 - val_loss: 0.1301 - val_accuracy: 0.9009\n",
      "Epoch 575/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9197\n",
      "Epoch 575: val_accuracy did not improve from 0.90090\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1003 - accuracy: 0.9197 - val_loss: 0.1299 - val_accuracy: 0.9009\n",
      "Epoch 576/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9197\n",
      "Epoch 576: val_accuracy improved from 0.90090 to 0.90315, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1001 - accuracy: 0.9197 - val_loss: 0.1298 - val_accuracy: 0.9032\n",
      "Epoch 577/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.9197\n",
      "Epoch 577: val_accuracy did not improve from 0.90315\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1000 - accuracy: 0.9197 - val_loss: 0.1297 - val_accuracy: 0.9032\n",
      "Epoch 578/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9197\n",
      "Epoch 578: val_accuracy did not improve from 0.90315\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0999 - accuracy: 0.9197 - val_loss: 0.1296 - val_accuracy: 0.9032\n",
      "Epoch 579/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.9197\n",
      "Epoch 579: val_accuracy did not improve from 0.90315\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0998 - accuracy: 0.9197 - val_loss: 0.1294 - val_accuracy: 0.9032\n",
      "Epoch 580/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9197\n",
      "Epoch 580: val_accuracy did not improve from 0.90315\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0997 - accuracy: 0.9197 - val_loss: 0.1293 - val_accuracy: 0.9032\n",
      "Epoch 581/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.9197\n",
      "Epoch 581: val_accuracy did not improve from 0.90315\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0995 - accuracy: 0.9197 - val_loss: 0.1291 - val_accuracy: 0.9032\n",
      "Epoch 582/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9197\n",
      "Epoch 582: val_accuracy did not improve from 0.90315\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0994 - accuracy: 0.9197 - val_loss: 0.1290 - val_accuracy: 0.9032\n",
      "Epoch 583/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9206\n",
      "Epoch 583: val_accuracy did not improve from 0.90315\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0993 - accuracy: 0.9206 - val_loss: 0.1288 - val_accuracy: 0.9032\n",
      "Epoch 584/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9206\n",
      "Epoch 584: val_accuracy improved from 0.90315 to 0.90541, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0992 - accuracy: 0.9206 - val_loss: 0.1288 - val_accuracy: 0.9054\n",
      "Epoch 585/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9206\n",
      "Epoch 585: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0991 - accuracy: 0.9206 - val_loss: 0.1286 - val_accuracy: 0.9054\n",
      "Epoch 586/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9206\n",
      "Epoch 586: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0990 - accuracy: 0.9206 - val_loss: 0.1285 - val_accuracy: 0.9054\n",
      "Epoch 587/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9206\n",
      "Epoch 587: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0988 - accuracy: 0.9206 - val_loss: 0.1284 - val_accuracy: 0.9054\n",
      "Epoch 588/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9206\n",
      "Epoch 588: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0987 - accuracy: 0.9206 - val_loss: 0.1282 - val_accuracy: 0.9054\n",
      "Epoch 589/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9206\n",
      "Epoch 589: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0986 - accuracy: 0.9206 - val_loss: 0.1281 - val_accuracy: 0.9054\n",
      "Epoch 590/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9206\n",
      "Epoch 590: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0985 - accuracy: 0.9206 - val_loss: 0.1279 - val_accuracy: 0.9054\n",
      "Epoch 591/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9206\n",
      "Epoch 591: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0984 - accuracy: 0.9206 - val_loss: 0.1278 - val_accuracy: 0.9054\n",
      "Epoch 592/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9206\n",
      "Epoch 592: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0983 - accuracy: 0.9206 - val_loss: 0.1277 - val_accuracy: 0.9054\n",
      "Epoch 593/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9206\n",
      "Epoch 593: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0981 - accuracy: 0.9206 - val_loss: 0.1275 - val_accuracy: 0.9054\n",
      "Epoch 594/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9206\n",
      "Epoch 594: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0980 - accuracy: 0.9206 - val_loss: 0.1274 - val_accuracy: 0.9054\n",
      "Epoch 595/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9206\n",
      "Epoch 595: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0979 - accuracy: 0.9206 - val_loss: 0.1273 - val_accuracy: 0.9054\n",
      "Epoch 596/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9206\n",
      "Epoch 596: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0978 - accuracy: 0.9206 - val_loss: 0.1271 - val_accuracy: 0.9054\n",
      "Epoch 597/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9206\n",
      "Epoch 597: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0977 - accuracy: 0.9206 - val_loss: 0.1270 - val_accuracy: 0.9054\n",
      "Epoch 598/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9206\n",
      "Epoch 598: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0976 - accuracy: 0.9206 - val_loss: 0.1269 - val_accuracy: 0.9054\n",
      "Epoch 599/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9206\n",
      "Epoch 599: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0975 - accuracy: 0.9206 - val_loss: 0.1268 - val_accuracy: 0.9054\n",
      "Epoch 600/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9206\n",
      "Epoch 600: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0973 - accuracy: 0.9206 - val_loss: 0.1266 - val_accuracy: 0.9054\n",
      "Epoch 601/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9206\n",
      "Epoch 601: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0972 - accuracy: 0.9206 - val_loss: 0.1265 - val_accuracy: 0.9054\n",
      "Epoch 602/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9206\n",
      "Epoch 602: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0971 - accuracy: 0.9206 - val_loss: 0.1264 - val_accuracy: 0.9054\n",
      "Epoch 603/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9206\n",
      "Epoch 603: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0970 - accuracy: 0.9206 - val_loss: 0.1262 - val_accuracy: 0.9054\n",
      "Epoch 604/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9206\n",
      "Epoch 604: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0969 - accuracy: 0.9206 - val_loss: 0.1261 - val_accuracy: 0.9054\n",
      "Epoch 605/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9216\n",
      "Epoch 605: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0968 - accuracy: 0.9216 - val_loss: 0.1260 - val_accuracy: 0.9054\n",
      "Epoch 606/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9216\n",
      "Epoch 606: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0967 - accuracy: 0.9216 - val_loss: 0.1258 - val_accuracy: 0.9054\n",
      "Epoch 607/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9216\n",
      "Epoch 607: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0966 - accuracy: 0.9216 - val_loss: 0.1257 - val_accuracy: 0.9054\n",
      "Epoch 608/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9216\n",
      "Epoch 608: val_accuracy did not improve from 0.90541\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0964 - accuracy: 0.9216 - val_loss: 0.1256 - val_accuracy: 0.9054\n",
      "Epoch 609/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9216\n",
      "Epoch 609: val_accuracy improved from 0.90541 to 0.90766, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0963 - accuracy: 0.9216 - val_loss: 0.1255 - val_accuracy: 0.9077\n",
      "Epoch 610/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9216\n",
      "Epoch 610: val_accuracy improved from 0.90766 to 0.90991, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0962 - accuracy: 0.9216 - val_loss: 0.1253 - val_accuracy: 0.9099\n",
      "Epoch 611/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9206\n",
      "Epoch 611: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0961 - accuracy: 0.9206 - val_loss: 0.1252 - val_accuracy: 0.9099\n",
      "Epoch 612/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9206\n",
      "Epoch 612: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0960 - accuracy: 0.9206 - val_loss: 0.1251 - val_accuracy: 0.9099\n",
      "Epoch 613/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9216\n",
      "Epoch 613: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0959 - accuracy: 0.9216 - val_loss: 0.1249 - val_accuracy: 0.9099\n",
      "Epoch 614/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9216\n",
      "Epoch 614: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0958 - accuracy: 0.9216 - val_loss: 0.1248 - val_accuracy: 0.9099\n",
      "Epoch 615/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9216\n",
      "Epoch 615: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0957 - accuracy: 0.9216 - val_loss: 0.1247 - val_accuracy: 0.9099\n",
      "Epoch 616/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9216\n",
      "Epoch 616: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0956 - accuracy: 0.9216 - val_loss: 0.1246 - val_accuracy: 0.9099\n",
      "Epoch 617/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9216\n",
      "Epoch 617: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0955 - accuracy: 0.9216 - val_loss: 0.1244 - val_accuracy: 0.9099\n",
      "Epoch 618/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0954 - accuracy: 0.9216\n",
      "Epoch 618: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0954 - accuracy: 0.9216 - val_loss: 0.1243 - val_accuracy: 0.9099\n",
      "Epoch 619/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9216\n",
      "Epoch 619: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0952 - accuracy: 0.9216 - val_loss: 0.1242 - val_accuracy: 0.9099\n",
      "Epoch 620/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9216\n",
      "Epoch 620: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0951 - accuracy: 0.9216 - val_loss: 0.1241 - val_accuracy: 0.9099\n",
      "Epoch 621/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9216\n",
      "Epoch 621: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0950 - accuracy: 0.9216 - val_loss: 0.1239 - val_accuracy: 0.9099\n",
      "Epoch 622/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9216\n",
      "Epoch 622: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0949 - accuracy: 0.9216 - val_loss: 0.1238 - val_accuracy: 0.9099\n",
      "Epoch 623/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9216\n",
      "Epoch 623: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0948 - accuracy: 0.9216 - val_loss: 0.1237 - val_accuracy: 0.9099\n",
      "Epoch 624/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9216\n",
      "Epoch 624: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0947 - accuracy: 0.9216 - val_loss: 0.1236 - val_accuracy: 0.9099\n",
      "Epoch 625/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9226\n",
      "Epoch 625: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0946 - accuracy: 0.9226 - val_loss: 0.1234 - val_accuracy: 0.9099\n",
      "Epoch 626/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9226\n",
      "Epoch 626: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0945 - accuracy: 0.9226 - val_loss: 0.1233 - val_accuracy: 0.9099\n",
      "Epoch 627/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9226\n",
      "Epoch 627: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0944 - accuracy: 0.9226 - val_loss: 0.1232 - val_accuracy: 0.9099\n",
      "Epoch 628/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9226\n",
      "Epoch 628: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0943 - accuracy: 0.9226 - val_loss: 0.1231 - val_accuracy: 0.9099\n",
      "Epoch 629/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0942 - accuracy: 0.9226\n",
      "Epoch 629: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0942 - accuracy: 0.9226 - val_loss: 0.1229 - val_accuracy: 0.9099\n",
      "Epoch 630/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9226\n",
      "Epoch 630: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0941 - accuracy: 0.9226 - val_loss: 0.1228 - val_accuracy: 0.9099\n",
      "Epoch 631/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9226\n",
      "Epoch 631: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0940 - accuracy: 0.9226 - val_loss: 0.1227 - val_accuracy: 0.9099\n",
      "Epoch 632/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9226\n",
      "Epoch 632: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0939 - accuracy: 0.9226 - val_loss: 0.1226 - val_accuracy: 0.9099\n",
      "Epoch 633/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9226\n",
      "Epoch 633: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0937 - accuracy: 0.9226 - val_loss: 0.1225 - val_accuracy: 0.9099\n",
      "Epoch 634/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9226\n",
      "Epoch 634: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0936 - accuracy: 0.9226 - val_loss: 0.1223 - val_accuracy: 0.9099\n",
      "Epoch 635/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9226\n",
      "Epoch 635: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0935 - accuracy: 0.9226 - val_loss: 0.1222 - val_accuracy: 0.9099\n",
      "Epoch 636/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9226\n",
      "Epoch 636: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0934 - accuracy: 0.9226 - val_loss: 0.1221 - val_accuracy: 0.9099\n",
      "Epoch 637/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9226\n",
      "Epoch 637: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0933 - accuracy: 0.9226 - val_loss: 0.1220 - val_accuracy: 0.9099\n",
      "Epoch 638/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9226\n",
      "Epoch 638: val_accuracy did not improve from 0.90991\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0932 - accuracy: 0.9226 - val_loss: 0.1218 - val_accuracy: 0.9099\n",
      "Epoch 639/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9226\n",
      "Epoch 639: val_accuracy improved from 0.90991 to 0.91216, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0931 - accuracy: 0.9226 - val_loss: 0.1217 - val_accuracy: 0.9122\n",
      "Epoch 640/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9235\n",
      "Epoch 640: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0930 - accuracy: 0.9235 - val_loss: 0.1216 - val_accuracy: 0.9122\n",
      "Epoch 641/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9235\n",
      "Epoch 641: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0929 - accuracy: 0.9235 - val_loss: 0.1215 - val_accuracy: 0.9122\n",
      "Epoch 642/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9235\n",
      "Epoch 642: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0928 - accuracy: 0.9235 - val_loss: 0.1214 - val_accuracy: 0.9122\n",
      "Epoch 643/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9235\n",
      "Epoch 643: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0927 - accuracy: 0.9235 - val_loss: 0.1212 - val_accuracy: 0.9122\n",
      "Epoch 644/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9235\n",
      "Epoch 644: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0926 - accuracy: 0.9235 - val_loss: 0.1211 - val_accuracy: 0.9122\n",
      "Epoch 645/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 0.9235\n",
      "Epoch 645: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0925 - accuracy: 0.9235 - val_loss: 0.1210 - val_accuracy: 0.9122\n",
      "Epoch 646/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9235\n",
      "Epoch 646: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0924 - accuracy: 0.9235 - val_loss: 0.1209 - val_accuracy: 0.9122\n",
      "Epoch 647/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9235\n",
      "Epoch 647: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0923 - accuracy: 0.9235 - val_loss: 0.1208 - val_accuracy: 0.9122\n",
      "Epoch 648/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9235\n",
      "Epoch 648: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0922 - accuracy: 0.9235 - val_loss: 0.1206 - val_accuracy: 0.9122\n",
      "Epoch 649/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9235\n",
      "Epoch 649: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0921 - accuracy: 0.9235 - val_loss: 0.1205 - val_accuracy: 0.9122\n",
      "Epoch 650/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9235\n",
      "Epoch 650: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0920 - accuracy: 0.9235 - val_loss: 0.1204 - val_accuracy: 0.9122\n",
      "Epoch 651/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9235\n",
      "Epoch 651: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0919 - accuracy: 0.9235 - val_loss: 0.1203 - val_accuracy: 0.9122\n",
      "Epoch 652/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9235\n",
      "Epoch 652: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0918 - accuracy: 0.9235 - val_loss: 0.1202 - val_accuracy: 0.9122\n",
      "Epoch 653/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9235\n",
      "Epoch 653: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0917 - accuracy: 0.9235 - val_loss: 0.1200 - val_accuracy: 0.9122\n",
      "Epoch 654/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9235\n",
      "Epoch 654: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0916 - accuracy: 0.9235 - val_loss: 0.1199 - val_accuracy: 0.9122\n",
      "Epoch 655/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9235\n",
      "Epoch 655: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0915 - accuracy: 0.9235 - val_loss: 0.1198 - val_accuracy: 0.9122\n",
      "Epoch 656/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9245\n",
      "Epoch 656: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0914 - accuracy: 0.9245 - val_loss: 0.1197 - val_accuracy: 0.9122\n",
      "Epoch 657/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9245\n",
      "Epoch 657: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0913 - accuracy: 0.9245 - val_loss: 0.1196 - val_accuracy: 0.9122\n",
      "Epoch 658/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9245\n",
      "Epoch 658: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0912 - accuracy: 0.9245 - val_loss: 0.1195 - val_accuracy: 0.9122\n",
      "Epoch 659/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9255\n",
      "Epoch 659: val_accuracy did not improve from 0.91216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0911 - accuracy: 0.9255 - val_loss: 0.1193 - val_accuracy: 0.9122\n",
      "Epoch 660/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9255\n",
      "Epoch 660: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0910 - accuracy: 0.9255 - val_loss: 0.1192 - val_accuracy: 0.9122\n",
      "Epoch 661/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9255\n",
      "Epoch 661: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0909 - accuracy: 0.9255 - val_loss: 0.1191 - val_accuracy: 0.9122\n",
      "Epoch 662/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9255\n",
      "Epoch 662: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0908 - accuracy: 0.9255 - val_loss: 0.1190 - val_accuracy: 0.9122\n",
      "Epoch 663/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9255\n",
      "Epoch 663: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0907 - accuracy: 0.9255 - val_loss: 0.1189 - val_accuracy: 0.9122\n",
      "Epoch 664/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9255\n",
      "Epoch 664: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0906 - accuracy: 0.9255 - val_loss: 0.1188 - val_accuracy: 0.9122\n",
      "Epoch 665/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9255\n",
      "Epoch 665: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0905 - accuracy: 0.9255 - val_loss: 0.1186 - val_accuracy: 0.9122\n",
      "Epoch 666/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9255\n",
      "Epoch 666: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0904 - accuracy: 0.9255 - val_loss: 0.1185 - val_accuracy: 0.9122\n",
      "Epoch 667/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9255\n",
      "Epoch 667: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0903 - accuracy: 0.9255 - val_loss: 0.1184 - val_accuracy: 0.9122\n",
      "Epoch 668/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.9255\n",
      "Epoch 668: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0902 - accuracy: 0.9255 - val_loss: 0.1183 - val_accuracy: 0.9122\n",
      "Epoch 669/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9255\n",
      "Epoch 669: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0901 - accuracy: 0.9255 - val_loss: 0.1182 - val_accuracy: 0.9122\n",
      "Epoch 670/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9255\n",
      "Epoch 670: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0900 - accuracy: 0.9255 - val_loss: 0.1181 - val_accuracy: 0.9122\n",
      "Epoch 671/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9255\n",
      "Epoch 671: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0899 - accuracy: 0.9255 - val_loss: 0.1180 - val_accuracy: 0.9122\n",
      "Epoch 672/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9255\n",
      "Epoch 672: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0898 - accuracy: 0.9255 - val_loss: 0.1178 - val_accuracy: 0.9122\n",
      "Epoch 673/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9255\n",
      "Epoch 673: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0897 - accuracy: 0.9255 - val_loss: 0.1177 - val_accuracy: 0.9122\n",
      "Epoch 674/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9255\n",
      "Epoch 674: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0896 - accuracy: 0.9255 - val_loss: 0.1176 - val_accuracy: 0.9122\n",
      "Epoch 675/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9255\n",
      "Epoch 675: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0895 - accuracy: 0.9255 - val_loss: 0.1175 - val_accuracy: 0.9122\n",
      "Epoch 676/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9255\n",
      "Epoch 676: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0894 - accuracy: 0.9255 - val_loss: 0.1174 - val_accuracy: 0.9122\n",
      "Epoch 677/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9255\n",
      "Epoch 677: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0893 - accuracy: 0.9255 - val_loss: 0.1173 - val_accuracy: 0.9122\n",
      "Epoch 678/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9255\n",
      "Epoch 678: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0892 - accuracy: 0.9255 - val_loss: 0.1172 - val_accuracy: 0.9122\n",
      "Epoch 679/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9255\n",
      "Epoch 679: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0891 - accuracy: 0.9255 - val_loss: 0.1170 - val_accuracy: 0.9122\n",
      "Epoch 680/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9255\n",
      "Epoch 680: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0890 - accuracy: 0.9255 - val_loss: 0.1169 - val_accuracy: 0.9122\n",
      "Epoch 681/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9255\n",
      "Epoch 681: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0889 - accuracy: 0.9255 - val_loss: 0.1168 - val_accuracy: 0.9122\n",
      "Epoch 682/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9255\n",
      "Epoch 682: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0889 - accuracy: 0.9255 - val_loss: 0.1167 - val_accuracy: 0.9122\n",
      "Epoch 683/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9255\n",
      "Epoch 683: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0888 - accuracy: 0.9255 - val_loss: 0.1166 - val_accuracy: 0.9122\n",
      "Epoch 684/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9255\n",
      "Epoch 684: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0887 - accuracy: 0.9255 - val_loss: 0.1165 - val_accuracy: 0.9122\n",
      "Epoch 685/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9255\n",
      "Epoch 685: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0886 - accuracy: 0.9255 - val_loss: 0.1164 - val_accuracy: 0.9122\n",
      "Epoch 686/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9255\n",
      "Epoch 686: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0885 - accuracy: 0.9255 - val_loss: 0.1163 - val_accuracy: 0.9122\n",
      "Epoch 687/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9255\n",
      "Epoch 687: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0884 - accuracy: 0.9255 - val_loss: 0.1161 - val_accuracy: 0.9122\n",
      "Epoch 688/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9255\n",
      "Epoch 688: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0883 - accuracy: 0.9255 - val_loss: 0.1160 - val_accuracy: 0.9122\n",
      "Epoch 689/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9255\n",
      "Epoch 689: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0882 - accuracy: 0.9255 - val_loss: 0.1159 - val_accuracy: 0.9122\n",
      "Epoch 690/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9255\n",
      "Epoch 690: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0881 - accuracy: 0.9255 - val_loss: 0.1158 - val_accuracy: 0.9122\n",
      "Epoch 691/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9255\n",
      "Epoch 691: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0880 - accuracy: 0.9255 - val_loss: 0.1157 - val_accuracy: 0.9122\n",
      "Epoch 692/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9255\n",
      "Epoch 692: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0879 - accuracy: 0.9255 - val_loss: 0.1156 - val_accuracy: 0.9122\n",
      "Epoch 693/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9255\n",
      "Epoch 693: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0878 - accuracy: 0.9255 - val_loss: 0.1155 - val_accuracy: 0.9122\n",
      "Epoch 694/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9264\n",
      "Epoch 694: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0877 - accuracy: 0.9264 - val_loss: 0.1154 - val_accuracy: 0.9122\n",
      "Epoch 695/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9264\n",
      "Epoch 695: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0876 - accuracy: 0.9264 - val_loss: 0.1153 - val_accuracy: 0.9122\n",
      "Epoch 696/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9264\n",
      "Epoch 696: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0875 - accuracy: 0.9264 - val_loss: 0.1152 - val_accuracy: 0.9122\n",
      "Epoch 697/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9264\n",
      "Epoch 697: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0874 - accuracy: 0.9264 - val_loss: 0.1150 - val_accuracy: 0.9122\n",
      "Epoch 698/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9264\n",
      "Epoch 698: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0874 - accuracy: 0.9264 - val_loss: 0.1149 - val_accuracy: 0.9122\n",
      "Epoch 699/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9264\n",
      "Epoch 699: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0873 - accuracy: 0.9264 - val_loss: 0.1148 - val_accuracy: 0.9122\n",
      "Epoch 700/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9264\n",
      "Epoch 700: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0872 - accuracy: 0.9264 - val_loss: 0.1147 - val_accuracy: 0.9122\n",
      "Epoch 701/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9274\n",
      "Epoch 701: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0871 - accuracy: 0.9274 - val_loss: 0.1146 - val_accuracy: 0.9122\n",
      "Epoch 702/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9274\n",
      "Epoch 702: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0870 - accuracy: 0.9274 - val_loss: 0.1145 - val_accuracy: 0.9122\n",
      "Epoch 703/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9274\n",
      "Epoch 703: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0869 - accuracy: 0.9274 - val_loss: 0.1144 - val_accuracy: 0.9122\n",
      "Epoch 704/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9274\n",
      "Epoch 704: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0868 - accuracy: 0.9274 - val_loss: 0.1143 - val_accuracy: 0.9122\n",
      "Epoch 705/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9274\n",
      "Epoch 705: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0867 - accuracy: 0.9274 - val_loss: 0.1142 - val_accuracy: 0.9122\n",
      "Epoch 706/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9274\n",
      "Epoch 706: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0866 - accuracy: 0.9274 - val_loss: 0.1141 - val_accuracy: 0.9122\n",
      "Epoch 707/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9274\n",
      "Epoch 707: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0865 - accuracy: 0.9274 - val_loss: 0.1140 - val_accuracy: 0.9122\n",
      "Epoch 708/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9274\n",
      "Epoch 708: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0864 - accuracy: 0.9274 - val_loss: 0.1139 - val_accuracy: 0.9122\n",
      "Epoch 709/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9274\n",
      "Epoch 709: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0864 - accuracy: 0.9274 - val_loss: 0.1137 - val_accuracy: 0.9122\n",
      "Epoch 710/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9274\n",
      "Epoch 710: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0863 - accuracy: 0.9274 - val_loss: 0.1136 - val_accuracy: 0.9122\n",
      "Epoch 711/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9274\n",
      "Epoch 711: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0862 - accuracy: 0.9274 - val_loss: 0.1135 - val_accuracy: 0.9122\n",
      "Epoch 712/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9274\n",
      "Epoch 712: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0861 - accuracy: 0.9274 - val_loss: 0.1134 - val_accuracy: 0.9122\n",
      "Epoch 713/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9274\n",
      "Epoch 713: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0860 - accuracy: 0.9274 - val_loss: 0.1133 - val_accuracy: 0.9122\n",
      "Epoch 714/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9274\n",
      "Epoch 714: val_accuracy did not improve from 0.91216\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0859 - accuracy: 0.9274 - val_loss: 0.1132 - val_accuracy: 0.9122\n",
      "Epoch 715/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9284\n",
      "Epoch 715: val_accuracy improved from 0.91216 to 0.91441, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0858 - accuracy: 0.9284 - val_loss: 0.1131 - val_accuracy: 0.9144\n",
      "Epoch 716/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9284\n",
      "Epoch 716: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0857 - accuracy: 0.9284 - val_loss: 0.1130 - val_accuracy: 0.9144\n",
      "Epoch 717/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9284\n",
      "Epoch 717: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0856 - accuracy: 0.9284 - val_loss: 0.1129 - val_accuracy: 0.9144\n",
      "Epoch 718/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9284\n",
      "Epoch 718: val_accuracy did not improve from 0.91441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0856 - accuracy: 0.9284 - val_loss: 0.1128 - val_accuracy: 0.9144\n",
      "Epoch 719/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9284\n",
      "Epoch 719: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0855 - accuracy: 0.9284 - val_loss: 0.1127 - val_accuracy: 0.9144\n",
      "Epoch 720/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9284\n",
      "Epoch 720: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0854 - accuracy: 0.9284 - val_loss: 0.1126 - val_accuracy: 0.9144\n",
      "Epoch 721/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9293\n",
      "Epoch 721: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0853 - accuracy: 0.9293 - val_loss: 0.1125 - val_accuracy: 0.9144\n",
      "Epoch 722/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9293\n",
      "Epoch 722: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0852 - accuracy: 0.9293 - val_loss: 0.1124 - val_accuracy: 0.9144\n",
      "Epoch 723/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9293\n",
      "Epoch 723: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0851 - accuracy: 0.9293 - val_loss: 0.1123 - val_accuracy: 0.9144\n",
      "Epoch 724/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9293\n",
      "Epoch 724: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0850 - accuracy: 0.9293 - val_loss: 0.1122 - val_accuracy: 0.9144\n",
      "Epoch 725/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9293\n",
      "Epoch 725: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0849 - accuracy: 0.9293 - val_loss: 0.1121 - val_accuracy: 0.9144\n",
      "Epoch 726/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9293\n",
      "Epoch 726: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0849 - accuracy: 0.9293 - val_loss: 0.1120 - val_accuracy: 0.9144\n",
      "Epoch 727/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9303\n",
      "Epoch 727: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0848 - accuracy: 0.9303 - val_loss: 0.1119 - val_accuracy: 0.9144\n",
      "Epoch 728/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9303\n",
      "Epoch 728: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0847 - accuracy: 0.9303 - val_loss: 0.1117 - val_accuracy: 0.9144\n",
      "Epoch 729/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9303\n",
      "Epoch 729: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0846 - accuracy: 0.9303 - val_loss: 0.1116 - val_accuracy: 0.9144\n",
      "Epoch 730/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9303\n",
      "Epoch 730: val_accuracy did not improve from 0.91441\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0845 - accuracy: 0.9303 - val_loss: 0.1115 - val_accuracy: 0.9144\n",
      "Epoch 731/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9303\n",
      "Epoch 731: val_accuracy improved from 0.91441 to 0.91667, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0844 - accuracy: 0.9303 - val_loss: 0.1114 - val_accuracy: 0.9167\n",
      "Epoch 732/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9303\n",
      "Epoch 732: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0843 - accuracy: 0.9303 - val_loss: 0.1113 - val_accuracy: 0.9167\n",
      "Epoch 733/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9303\n",
      "Epoch 733: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0842 - accuracy: 0.9303 - val_loss: 0.1112 - val_accuracy: 0.9167\n",
      "Epoch 734/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9303\n",
      "Epoch 734: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0842 - accuracy: 0.9303 - val_loss: 0.1111 - val_accuracy: 0.9167\n",
      "Epoch 735/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9303\n",
      "Epoch 735: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0841 - accuracy: 0.9303 - val_loss: 0.1110 - val_accuracy: 0.9167\n",
      "Epoch 736/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9303\n",
      "Epoch 736: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0840 - accuracy: 0.9303 - val_loss: 0.1109 - val_accuracy: 0.9167\n",
      "Epoch 737/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9303\n",
      "Epoch 737: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0839 - accuracy: 0.9303 - val_loss: 0.1108 - val_accuracy: 0.9167\n",
      "Epoch 738/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9303\n",
      "Epoch 738: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0838 - accuracy: 0.9303 - val_loss: 0.1107 - val_accuracy: 0.9167\n",
      "Epoch 739/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9303\n",
      "Epoch 739: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0837 - accuracy: 0.9303 - val_loss: 0.1106 - val_accuracy: 0.9167\n",
      "Epoch 740/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9303\n",
      "Epoch 740: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0836 - accuracy: 0.9303 - val_loss: 0.1105 - val_accuracy: 0.9167\n",
      "Epoch 741/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9303\n",
      "Epoch 741: val_accuracy did not improve from 0.91667\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0836 - accuracy: 0.9303 - val_loss: 0.1104 - val_accuracy: 0.9167\n",
      "Epoch 742/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9303\n",
      "Epoch 742: val_accuracy improved from 0.91667 to 0.91892, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0835 - accuracy: 0.9303 - val_loss: 0.1103 - val_accuracy: 0.9189\n",
      "Epoch 743/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9313\n",
      "Epoch 743: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0834 - accuracy: 0.9313 - val_loss: 0.1102 - val_accuracy: 0.9189\n",
      "Epoch 744/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9313\n",
      "Epoch 744: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0833 - accuracy: 0.9313 - val_loss: 0.1101 - val_accuracy: 0.9189\n",
      "Epoch 745/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9313\n",
      "Epoch 745: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0832 - accuracy: 0.9313 - val_loss: 0.1100 - val_accuracy: 0.9189\n",
      "Epoch 746/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9313\n",
      "Epoch 746: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0831 - accuracy: 0.9313 - val_loss: 0.1099 - val_accuracy: 0.9189\n",
      "Epoch 747/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9313\n",
      "Epoch 747: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0831 - accuracy: 0.9313 - val_loss: 0.1098 - val_accuracy: 0.9189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 748/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9313\n",
      "Epoch 748: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0830 - accuracy: 0.9313 - val_loss: 0.1097 - val_accuracy: 0.9189\n",
      "Epoch 749/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9313\n",
      "Epoch 749: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0829 - accuracy: 0.9313 - val_loss: 0.1096 - val_accuracy: 0.9189\n",
      "Epoch 750/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9313\n",
      "Epoch 750: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0828 - accuracy: 0.9313 - val_loss: 0.1095 - val_accuracy: 0.9189\n",
      "Epoch 751/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9313\n",
      "Epoch 751: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0827 - accuracy: 0.9313 - val_loss: 0.1094 - val_accuracy: 0.9189\n",
      "Epoch 752/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9313\n",
      "Epoch 752: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0826 - accuracy: 0.9313 - val_loss: 0.1093 - val_accuracy: 0.9189\n",
      "Epoch 753/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9313\n",
      "Epoch 753: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0826 - accuracy: 0.9313 - val_loss: 0.1092 - val_accuracy: 0.9189\n",
      "Epoch 754/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9322\n",
      "Epoch 754: val_accuracy did not improve from 0.91892\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0825 - accuracy: 0.9322 - val_loss: 0.1091 - val_accuracy: 0.9189\n",
      "Epoch 755/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9322\n",
      "Epoch 755: val_accuracy improved from 0.91892 to 0.92117, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0824 - accuracy: 0.9322 - val_loss: 0.1090 - val_accuracy: 0.9212\n",
      "Epoch 756/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9322\n",
      "Epoch 756: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0823 - accuracy: 0.9322 - val_loss: 0.1089 - val_accuracy: 0.9212\n",
      "Epoch 757/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9322\n",
      "Epoch 757: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0822 - accuracy: 0.9322 - val_loss: 0.1088 - val_accuracy: 0.9212\n",
      "Epoch 758/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9332\n",
      "Epoch 758: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0821 - accuracy: 0.9332 - val_loss: 0.1087 - val_accuracy: 0.9212\n",
      "Epoch 759/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9332\n",
      "Epoch 759: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0821 - accuracy: 0.9332 - val_loss: 0.1086 - val_accuracy: 0.9212\n",
      "Epoch 760/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9332\n",
      "Epoch 760: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0820 - accuracy: 0.9332 - val_loss: 0.1085 - val_accuracy: 0.9212\n",
      "Epoch 761/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9332\n",
      "Epoch 761: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0819 - accuracy: 0.9332 - val_loss: 0.1084 - val_accuracy: 0.9212\n",
      "Epoch 762/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9332\n",
      "Epoch 762: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0818 - accuracy: 0.9332 - val_loss: 0.1083 - val_accuracy: 0.9212\n",
      "Epoch 763/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9332\n",
      "Epoch 763: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0817 - accuracy: 0.9332 - val_loss: 0.1082 - val_accuracy: 0.9212\n",
      "Epoch 764/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9332\n",
      "Epoch 764: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0817 - accuracy: 0.9332 - val_loss: 0.1081 - val_accuracy: 0.9212\n",
      "Epoch 765/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9332\n",
      "Epoch 765: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0816 - accuracy: 0.9332 - val_loss: 0.1080 - val_accuracy: 0.9212\n",
      "Epoch 766/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9332\n",
      "Epoch 766: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0815 - accuracy: 0.9332 - val_loss: 0.1079 - val_accuracy: 0.9212\n",
      "Epoch 767/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9332\n",
      "Epoch 767: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0814 - accuracy: 0.9332 - val_loss: 0.1078 - val_accuracy: 0.9212\n",
      "Epoch 768/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9332\n",
      "Epoch 768: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0813 - accuracy: 0.9332 - val_loss: 0.1077 - val_accuracy: 0.9212\n",
      "Epoch 769/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9332\n",
      "Epoch 769: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0813 - accuracy: 0.9332 - val_loss: 0.1077 - val_accuracy: 0.9212\n",
      "Epoch 770/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9342\n",
      "Epoch 770: val_accuracy did not improve from 0.92117\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0812 - accuracy: 0.9342 - val_loss: 0.1076 - val_accuracy: 0.9212\n",
      "Epoch 771/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9351\n",
      "Epoch 771: val_accuracy improved from 0.92117 to 0.92342, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0811 - accuracy: 0.9351 - val_loss: 0.1075 - val_accuracy: 0.9234\n",
      "Epoch 772/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9351\n",
      "Epoch 772: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0810 - accuracy: 0.9351 - val_loss: 0.1074 - val_accuracy: 0.9234\n",
      "Epoch 773/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9351\n",
      "Epoch 773: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0809 - accuracy: 0.9351 - val_loss: 0.1073 - val_accuracy: 0.9234\n",
      "Epoch 774/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9351\n",
      "Epoch 774: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0809 - accuracy: 0.9351 - val_loss: 0.1072 - val_accuracy: 0.9234\n",
      "Epoch 775/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9351\n",
      "Epoch 775: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0808 - accuracy: 0.9351 - val_loss: 0.1071 - val_accuracy: 0.9234\n",
      "Epoch 776/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9351\n",
      "Epoch 776: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0807 - accuracy: 0.9351 - val_loss: 0.1070 - val_accuracy: 0.9234\n",
      "Epoch 777/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9351\n",
      "Epoch 777: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0806 - accuracy: 0.9351 - val_loss: 0.1069 - val_accuracy: 0.9234\n",
      "Epoch 778/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9351\n",
      "Epoch 778: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0805 - accuracy: 0.9351 - val_loss: 0.1068 - val_accuracy: 0.9234\n",
      "Epoch 779/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9351\n",
      "Epoch 779: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0805 - accuracy: 0.9351 - val_loss: 0.1067 - val_accuracy: 0.9234\n",
      "Epoch 780/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9351\n",
      "Epoch 780: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0804 - accuracy: 0.9351 - val_loss: 0.1066 - val_accuracy: 0.9234\n",
      "Epoch 781/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9351\n",
      "Epoch 781: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0803 - accuracy: 0.9351 - val_loss: 0.1065 - val_accuracy: 0.9234\n",
      "Epoch 782/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9351\n",
      "Epoch 782: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0802 - accuracy: 0.9351 - val_loss: 0.1064 - val_accuracy: 0.9234\n",
      "Epoch 783/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9351\n",
      "Epoch 783: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0801 - accuracy: 0.9351 - val_loss: 0.1063 - val_accuracy: 0.9234\n",
      "Epoch 784/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9351\n",
      "Epoch 784: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0801 - accuracy: 0.9351 - val_loss: 0.1062 - val_accuracy: 0.9234\n",
      "Epoch 785/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9351\n",
      "Epoch 785: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0800 - accuracy: 0.9351 - val_loss: 0.1061 - val_accuracy: 0.9234\n",
      "Epoch 786/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9351\n",
      "Epoch 786: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0799 - accuracy: 0.9351 - val_loss: 0.1060 - val_accuracy: 0.9234\n",
      "Epoch 787/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9351\n",
      "Epoch 787: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0798 - accuracy: 0.9351 - val_loss: 0.1059 - val_accuracy: 0.9234\n",
      "Epoch 788/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9351\n",
      "Epoch 788: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0798 - accuracy: 0.9351 - val_loss: 0.1058 - val_accuracy: 0.9234\n",
      "Epoch 789/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9351\n",
      "Epoch 789: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0797 - accuracy: 0.9351 - val_loss: 0.1058 - val_accuracy: 0.9234\n",
      "Epoch 790/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9351\n",
      "Epoch 790: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0796 - accuracy: 0.9351 - val_loss: 0.1057 - val_accuracy: 0.9234\n",
      "Epoch 791/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9351\n",
      "Epoch 791: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0795 - accuracy: 0.9351 - val_loss: 0.1056 - val_accuracy: 0.9234\n",
      "Epoch 792/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9351\n",
      "Epoch 792: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0795 - accuracy: 0.9351 - val_loss: 0.1055 - val_accuracy: 0.9234\n",
      "Epoch 793/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9351\n",
      "Epoch 793: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0794 - accuracy: 0.9351 - val_loss: 0.1054 - val_accuracy: 0.9234\n",
      "Epoch 794/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9351\n",
      "Epoch 794: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0793 - accuracy: 0.9351 - val_loss: 0.1053 - val_accuracy: 0.9234\n",
      "Epoch 795/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9351\n",
      "Epoch 795: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0792 - accuracy: 0.9351 - val_loss: 0.1052 - val_accuracy: 0.9234\n",
      "Epoch 796/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9351\n",
      "Epoch 796: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0791 - accuracy: 0.9351 - val_loss: 0.1051 - val_accuracy: 0.9234\n",
      "Epoch 797/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9351\n",
      "Epoch 797: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0791 - accuracy: 0.9351 - val_loss: 0.1050 - val_accuracy: 0.9234\n",
      "Epoch 798/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9351\n",
      "Epoch 798: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0790 - accuracy: 0.9351 - val_loss: 0.1049 - val_accuracy: 0.9234\n",
      "Epoch 799/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9351\n",
      "Epoch 799: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0789 - accuracy: 0.9351 - val_loss: 0.1049 - val_accuracy: 0.9234\n",
      "Epoch 800/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9351\n",
      "Epoch 800: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0788 - accuracy: 0.9351 - val_loss: 0.1047 - val_accuracy: 0.9234\n",
      "Epoch 801/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9351\n",
      "Epoch 801: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0788 - accuracy: 0.9351 - val_loss: 0.1047 - val_accuracy: 0.9234\n",
      "Epoch 802/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9361\n",
      "Epoch 802: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0787 - accuracy: 0.9361 - val_loss: 0.1045 - val_accuracy: 0.9234\n",
      "Epoch 803/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9361\n",
      "Epoch 803: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0786 - accuracy: 0.9361 - val_loss: 0.1045 - val_accuracy: 0.9234\n",
      "Epoch 804/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9361\n",
      "Epoch 804: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0785 - accuracy: 0.9361 - val_loss: 0.1043 - val_accuracy: 0.9234\n",
      "Epoch 805/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9361\n",
      "Epoch 805: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0785 - accuracy: 0.9361 - val_loss: 0.1043 - val_accuracy: 0.9234\n",
      "Epoch 806/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9361\n",
      "Epoch 806: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0784 - accuracy: 0.9361 - val_loss: 0.1042 - val_accuracy: 0.9234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 807/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9361\n",
      "Epoch 807: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0783 - accuracy: 0.9361 - val_loss: 0.1041 - val_accuracy: 0.9234\n",
      "Epoch 808/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9361\n",
      "Epoch 808: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0782 - accuracy: 0.9361 - val_loss: 0.1040 - val_accuracy: 0.9234\n",
      "Epoch 809/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9361\n",
      "Epoch 809: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0782 - accuracy: 0.9361 - val_loss: 0.1039 - val_accuracy: 0.9234\n",
      "Epoch 810/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9361\n",
      "Epoch 810: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0781 - accuracy: 0.9361 - val_loss: 0.1038 - val_accuracy: 0.9234\n",
      "Epoch 811/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9361\n",
      "Epoch 811: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0780 - accuracy: 0.9361 - val_loss: 0.1037 - val_accuracy: 0.9234\n",
      "Epoch 812/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9361\n",
      "Epoch 812: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0779 - accuracy: 0.9361 - val_loss: 0.1036 - val_accuracy: 0.9234\n",
      "Epoch 813/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9361\n",
      "Epoch 813: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0779 - accuracy: 0.9361 - val_loss: 0.1036 - val_accuracy: 0.9234\n",
      "Epoch 814/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9361\n",
      "Epoch 814: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0778 - accuracy: 0.9361 - val_loss: 0.1034 - val_accuracy: 0.9234\n",
      "Epoch 815/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9361\n",
      "Epoch 815: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0777 - accuracy: 0.9361 - val_loss: 0.1034 - val_accuracy: 0.9234\n",
      "Epoch 816/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9371\n",
      "Epoch 816: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0776 - accuracy: 0.9371 - val_loss: 0.1033 - val_accuracy: 0.9234\n",
      "Epoch 817/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9371\n",
      "Epoch 817: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0776 - accuracy: 0.9371 - val_loss: 0.1032 - val_accuracy: 0.9234\n",
      "Epoch 818/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9371\n",
      "Epoch 818: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0775 - accuracy: 0.9371 - val_loss: 0.1031 - val_accuracy: 0.9234\n",
      "Epoch 819/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9371\n",
      "Epoch 819: val_accuracy did not improve from 0.92342\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0774 - accuracy: 0.9371 - val_loss: 0.1030 - val_accuracy: 0.9234\n",
      "Epoch 820/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9371\n",
      "Epoch 820: val_accuracy improved from 0.92342 to 0.92568, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0773 - accuracy: 0.9371 - val_loss: 0.1029 - val_accuracy: 0.9257\n",
      "Epoch 821/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9371\n",
      "Epoch 821: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0773 - accuracy: 0.9371 - val_loss: 0.1028 - val_accuracy: 0.9234\n",
      "Epoch 822/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9380\n",
      "Epoch 822: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0772 - accuracy: 0.9380 - val_loss: 0.1028 - val_accuracy: 0.9234\n",
      "Epoch 823/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9380\n",
      "Epoch 823: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0771 - accuracy: 0.9380 - val_loss: 0.1027 - val_accuracy: 0.9234\n",
      "Epoch 824/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9380\n",
      "Epoch 824: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0771 - accuracy: 0.9380 - val_loss: 0.1026 - val_accuracy: 0.9234\n",
      "Epoch 825/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9380\n",
      "Epoch 825: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0770 - accuracy: 0.9380 - val_loss: 0.1025 - val_accuracy: 0.9234\n",
      "Epoch 826/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9380\n",
      "Epoch 826: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0769 - accuracy: 0.9380 - val_loss: 0.1024 - val_accuracy: 0.9234\n",
      "Epoch 827/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9380\n",
      "Epoch 827: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0768 - accuracy: 0.9380 - val_loss: 0.1023 - val_accuracy: 0.9234\n",
      "Epoch 828/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9380\n",
      "Epoch 828: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0768 - accuracy: 0.9380 - val_loss: 0.1022 - val_accuracy: 0.9234\n",
      "Epoch 829/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9380\n",
      "Epoch 829: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0767 - accuracy: 0.9380 - val_loss: 0.1021 - val_accuracy: 0.9234\n",
      "Epoch 830/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9380\n",
      "Epoch 830: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0766 - accuracy: 0.9380 - val_loss: 0.1021 - val_accuracy: 0.9234\n",
      "Epoch 831/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9380\n",
      "Epoch 831: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0766 - accuracy: 0.9380 - val_loss: 0.1020 - val_accuracy: 0.9234\n",
      "Epoch 832/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9400\n",
      "Epoch 832: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0765 - accuracy: 0.9400 - val_loss: 0.1019 - val_accuracy: 0.9234\n",
      "Epoch 833/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9400\n",
      "Epoch 833: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0764 - accuracy: 0.9400 - val_loss: 0.1018 - val_accuracy: 0.9212\n",
      "Epoch 834/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9400\n",
      "Epoch 834: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0763 - accuracy: 0.9400 - val_loss: 0.1017 - val_accuracy: 0.9212\n",
      "Epoch 835/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9400\n",
      "Epoch 835: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0763 - accuracy: 0.9400 - val_loss: 0.1016 - val_accuracy: 0.9212\n",
      "Epoch 836/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9400\n",
      "Epoch 836: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0762 - accuracy: 0.9400 - val_loss: 0.1015 - val_accuracy: 0.9212\n",
      "Epoch 837/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9400\n",
      "Epoch 837: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0761 - accuracy: 0.9400 - val_loss: 0.1014 - val_accuracy: 0.9212\n",
      "Epoch 838/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9400\n",
      "Epoch 838: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0761 - accuracy: 0.9400 - val_loss: 0.1013 - val_accuracy: 0.9212\n",
      "Epoch 839/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9400\n",
      "Epoch 839: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0760 - accuracy: 0.9400 - val_loss: 0.1013 - val_accuracy: 0.9212\n",
      "Epoch 840/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9400\n",
      "Epoch 840: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0759 - accuracy: 0.9400 - val_loss: 0.1012 - val_accuracy: 0.9212\n",
      "Epoch 841/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9400\n",
      "Epoch 841: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0758 - accuracy: 0.9400 - val_loss: 0.1011 - val_accuracy: 0.9212\n",
      "Epoch 842/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9400\n",
      "Epoch 842: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0758 - accuracy: 0.9400 - val_loss: 0.1010 - val_accuracy: 0.9212\n",
      "Epoch 843/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9400\n",
      "Epoch 843: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0757 - accuracy: 0.9400 - val_loss: 0.1009 - val_accuracy: 0.9212\n",
      "Epoch 844/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9400\n",
      "Epoch 844: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0756 - accuracy: 0.9400 - val_loss: 0.1008 - val_accuracy: 0.9212\n",
      "Epoch 845/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9400\n",
      "Epoch 845: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0756 - accuracy: 0.9400 - val_loss: 0.1008 - val_accuracy: 0.9212\n",
      "Epoch 846/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9400\n",
      "Epoch 846: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0755 - accuracy: 0.9400 - val_loss: 0.1007 - val_accuracy: 0.9212\n",
      "Epoch 847/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9400\n",
      "Epoch 847: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0754 - accuracy: 0.9400 - val_loss: 0.1006 - val_accuracy: 0.9212\n",
      "Epoch 848/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9400\n",
      "Epoch 848: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0754 - accuracy: 0.9400 - val_loss: 0.1005 - val_accuracy: 0.9212\n",
      "Epoch 849/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9400\n",
      "Epoch 849: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0753 - accuracy: 0.9400 - val_loss: 0.1004 - val_accuracy: 0.9212\n",
      "Epoch 850/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9400\n",
      "Epoch 850: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0752 - accuracy: 0.9400 - val_loss: 0.1003 - val_accuracy: 0.9212\n",
      "Epoch 851/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9400\n",
      "Epoch 851: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0751 - accuracy: 0.9400 - val_loss: 0.1002 - val_accuracy: 0.9212\n",
      "Epoch 852/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9400\n",
      "Epoch 852: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0751 - accuracy: 0.9400 - val_loss: 0.1002 - val_accuracy: 0.9212\n",
      "Epoch 853/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9400\n",
      "Epoch 853: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0750 - accuracy: 0.9400 - val_loss: 0.1001 - val_accuracy: 0.9212\n",
      "Epoch 854/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9400\n",
      "Epoch 854: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0749 - accuracy: 0.9400 - val_loss: 0.1000 - val_accuracy: 0.9212\n",
      "Epoch 855/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9400\n",
      "Epoch 855: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0749 - accuracy: 0.9400 - val_loss: 0.0999 - val_accuracy: 0.9234\n",
      "Epoch 856/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9400\n",
      "Epoch 856: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0748 - accuracy: 0.9400 - val_loss: 0.0998 - val_accuracy: 0.9234\n",
      "Epoch 857/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9400\n",
      "Epoch 857: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0747 - accuracy: 0.9400 - val_loss: 0.0997 - val_accuracy: 0.9234\n",
      "Epoch 858/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9400\n",
      "Epoch 858: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0747 - accuracy: 0.9400 - val_loss: 0.0997 - val_accuracy: 0.9234\n",
      "Epoch 859/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9400\n",
      "Epoch 859: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0746 - accuracy: 0.9400 - val_loss: 0.0996 - val_accuracy: 0.9234\n",
      "Epoch 860/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9400\n",
      "Epoch 860: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0745 - accuracy: 0.9400 - val_loss: 0.0995 - val_accuracy: 0.9212\n",
      "Epoch 861/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9400\n",
      "Epoch 861: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0745 - accuracy: 0.9400 - val_loss: 0.0994 - val_accuracy: 0.9212\n",
      "Epoch 862/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9400\n",
      "Epoch 862: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0744 - accuracy: 0.9400 - val_loss: 0.0993 - val_accuracy: 0.9212\n",
      "Epoch 863/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9400\n",
      "Epoch 863: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0743 - accuracy: 0.9400 - val_loss: 0.0992 - val_accuracy: 0.9212\n",
      "Epoch 864/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9409\n",
      "Epoch 864: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0743 - accuracy: 0.9409 - val_loss: 0.0992 - val_accuracy: 0.9212\n",
      "Epoch 865/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9409\n",
      "Epoch 865: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0742 - accuracy: 0.9409 - val_loss: 0.0991 - val_accuracy: 0.9212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 866/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9409\n",
      "Epoch 866: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0741 - accuracy: 0.9409 - val_loss: 0.0990 - val_accuracy: 0.9212\n",
      "Epoch 867/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9409\n",
      "Epoch 867: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0740 - accuracy: 0.9409 - val_loss: 0.0989 - val_accuracy: 0.9212\n",
      "Epoch 868/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9409\n",
      "Epoch 868: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0740 - accuracy: 0.9409 - val_loss: 0.0988 - val_accuracy: 0.9212\n",
      "Epoch 869/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9409\n",
      "Epoch 869: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0739 - accuracy: 0.9409 - val_loss: 0.0987 - val_accuracy: 0.9212\n",
      "Epoch 870/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9419\n",
      "Epoch 870: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0738 - accuracy: 0.9419 - val_loss: 0.0987 - val_accuracy: 0.9212\n",
      "Epoch 871/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9419\n",
      "Epoch 871: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0738 - accuracy: 0.9419 - val_loss: 0.0986 - val_accuracy: 0.9212\n",
      "Epoch 872/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9419\n",
      "Epoch 872: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0737 - accuracy: 0.9419 - val_loss: 0.0985 - val_accuracy: 0.9212\n",
      "Epoch 873/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9429\n",
      "Epoch 873: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0736 - accuracy: 0.9429 - val_loss: 0.0984 - val_accuracy: 0.9212\n",
      "Epoch 874/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9429\n",
      "Epoch 874: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0736 - accuracy: 0.9429 - val_loss: 0.0983 - val_accuracy: 0.9212\n",
      "Epoch 875/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9439\n",
      "Epoch 875: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0735 - accuracy: 0.9439 - val_loss: 0.0983 - val_accuracy: 0.9212\n",
      "Epoch 876/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9439\n",
      "Epoch 876: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0734 - accuracy: 0.9439 - val_loss: 0.0982 - val_accuracy: 0.9212\n",
      "Epoch 877/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9439\n",
      "Epoch 877: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0734 - accuracy: 0.9439 - val_loss: 0.0981 - val_accuracy: 0.9212\n",
      "Epoch 878/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9439\n",
      "Epoch 878: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0733 - accuracy: 0.9439 - val_loss: 0.0980 - val_accuracy: 0.9212\n",
      "Epoch 879/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9439\n",
      "Epoch 879: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0732 - accuracy: 0.9439 - val_loss: 0.0979 - val_accuracy: 0.9212\n",
      "Epoch 880/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9439\n",
      "Epoch 880: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0732 - accuracy: 0.9439 - val_loss: 0.0978 - val_accuracy: 0.9212\n",
      "Epoch 881/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9439\n",
      "Epoch 881: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0731 - accuracy: 0.9439 - val_loss: 0.0978 - val_accuracy: 0.9212\n",
      "Epoch 882/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9439\n",
      "Epoch 882: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0730 - accuracy: 0.9439 - val_loss: 0.0977 - val_accuracy: 0.9212\n",
      "Epoch 883/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9439\n",
      "Epoch 883: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0730 - accuracy: 0.9439 - val_loss: 0.0976 - val_accuracy: 0.9212\n",
      "Epoch 884/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9439\n",
      "Epoch 884: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0729 - accuracy: 0.9439 - val_loss: 0.0975 - val_accuracy: 0.9212\n",
      "Epoch 885/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9448\n",
      "Epoch 885: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0729 - accuracy: 0.9448 - val_loss: 0.0974 - val_accuracy: 0.9212\n",
      "Epoch 886/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9448\n",
      "Epoch 886: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0728 - accuracy: 0.9448 - val_loss: 0.0974 - val_accuracy: 0.9212\n",
      "Epoch 887/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9448\n",
      "Epoch 887: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0727 - accuracy: 0.9448 - val_loss: 0.0973 - val_accuracy: 0.9234\n",
      "Epoch 888/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9458\n",
      "Epoch 888: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0727 - accuracy: 0.9458 - val_loss: 0.0972 - val_accuracy: 0.9234\n",
      "Epoch 889/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9468\n",
      "Epoch 889: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0726 - accuracy: 0.9468 - val_loss: 0.0971 - val_accuracy: 0.9234\n",
      "Epoch 890/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9468\n",
      "Epoch 890: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0725 - accuracy: 0.9468 - val_loss: 0.0970 - val_accuracy: 0.9234\n",
      "Epoch 891/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9468\n",
      "Epoch 891: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0725 - accuracy: 0.9468 - val_loss: 0.0970 - val_accuracy: 0.9234\n",
      "Epoch 892/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9468\n",
      "Epoch 892: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0724 - accuracy: 0.9468 - val_loss: 0.0969 - val_accuracy: 0.9234\n",
      "Epoch 893/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9468\n",
      "Epoch 893: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0723 - accuracy: 0.9468 - val_loss: 0.0968 - val_accuracy: 0.9234\n",
      "Epoch 894/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9468\n",
      "Epoch 894: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0723 - accuracy: 0.9468 - val_loss: 0.0967 - val_accuracy: 0.9234\n",
      "Epoch 895/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9468\n",
      "Epoch 895: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0722 - accuracy: 0.9468 - val_loss: 0.0967 - val_accuracy: 0.9234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 896/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9468\n",
      "Epoch 896: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0721 - accuracy: 0.9468 - val_loss: 0.0966 - val_accuracy: 0.9234\n",
      "Epoch 897/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9468\n",
      "Epoch 897: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0721 - accuracy: 0.9468 - val_loss: 0.0965 - val_accuracy: 0.9234\n",
      "Epoch 898/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9468\n",
      "Epoch 898: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0720 - accuracy: 0.9468 - val_loss: 0.0964 - val_accuracy: 0.9234\n",
      "Epoch 899/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9468\n",
      "Epoch 899: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0719 - accuracy: 0.9468 - val_loss: 0.0963 - val_accuracy: 0.9234\n",
      "Epoch 900/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9468\n",
      "Epoch 900: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0719 - accuracy: 0.9468 - val_loss: 0.0963 - val_accuracy: 0.9234\n",
      "Epoch 901/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9468\n",
      "Epoch 901: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0718 - accuracy: 0.9468 - val_loss: 0.0962 - val_accuracy: 0.9234\n",
      "Epoch 902/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9468\n",
      "Epoch 902: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0718 - accuracy: 0.9468 - val_loss: 0.0961 - val_accuracy: 0.9234\n",
      "Epoch 903/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9477\n",
      "Epoch 903: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0717 - accuracy: 0.9477 - val_loss: 0.0960 - val_accuracy: 0.9234\n",
      "Epoch 904/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9477\n",
      "Epoch 904: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0716 - accuracy: 0.9477 - val_loss: 0.0959 - val_accuracy: 0.9234\n",
      "Epoch 905/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9477\n",
      "Epoch 905: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0716 - accuracy: 0.9477 - val_loss: 0.0959 - val_accuracy: 0.9234\n",
      "Epoch 906/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9477\n",
      "Epoch 906: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0715 - accuracy: 0.9477 - val_loss: 0.0958 - val_accuracy: 0.9234\n",
      "Epoch 907/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9477\n",
      "Epoch 907: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0714 - accuracy: 0.9477 - val_loss: 0.0957 - val_accuracy: 0.9234\n",
      "Epoch 908/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9477\n",
      "Epoch 908: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0714 - accuracy: 0.9477 - val_loss: 0.0956 - val_accuracy: 0.9234\n",
      "Epoch 909/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9477\n",
      "Epoch 909: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0713 - accuracy: 0.9477 - val_loss: 0.0956 - val_accuracy: 0.9234\n",
      "Epoch 910/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9477\n",
      "Epoch 910: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0712 - accuracy: 0.9477 - val_loss: 0.0955 - val_accuracy: 0.9234\n",
      "Epoch 911/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9477\n",
      "Epoch 911: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0712 - accuracy: 0.9477 - val_loss: 0.0954 - val_accuracy: 0.9234\n",
      "Epoch 912/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9477\n",
      "Epoch 912: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0711 - accuracy: 0.9477 - val_loss: 0.0953 - val_accuracy: 0.9234\n",
      "Epoch 913/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9477\n",
      "Epoch 913: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0711 - accuracy: 0.9477 - val_loss: 0.0953 - val_accuracy: 0.9234\n",
      "Epoch 914/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9477\n",
      "Epoch 914: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0710 - accuracy: 0.9477 - val_loss: 0.0952 - val_accuracy: 0.9234\n",
      "Epoch 915/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9477\n",
      "Epoch 915: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0709 - accuracy: 0.9477 - val_loss: 0.0951 - val_accuracy: 0.9234\n",
      "Epoch 916/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9477\n",
      "Epoch 916: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0709 - accuracy: 0.9477 - val_loss: 0.0950 - val_accuracy: 0.9234\n",
      "Epoch 917/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9477\n",
      "Epoch 917: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0708 - accuracy: 0.9477 - val_loss: 0.0949 - val_accuracy: 0.9234\n",
      "Epoch 918/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9477\n",
      "Epoch 918: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0707 - accuracy: 0.9477 - val_loss: 0.0949 - val_accuracy: 0.9234\n",
      "Epoch 919/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9477\n",
      "Epoch 919: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0707 - accuracy: 0.9477 - val_loss: 0.0948 - val_accuracy: 0.9234\n",
      "Epoch 920/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9477\n",
      "Epoch 920: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0706 - accuracy: 0.9477 - val_loss: 0.0947 - val_accuracy: 0.9234\n",
      "Epoch 921/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9477\n",
      "Epoch 921: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0706 - accuracy: 0.9477 - val_loss: 0.0946 - val_accuracy: 0.9234\n",
      "Epoch 922/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9477\n",
      "Epoch 922: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0705 - accuracy: 0.9477 - val_loss: 0.0946 - val_accuracy: 0.9234\n",
      "Epoch 923/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9477\n",
      "Epoch 923: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0704 - accuracy: 0.9477 - val_loss: 0.0945 - val_accuracy: 0.9234\n",
      "Epoch 924/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9477\n",
      "Epoch 924: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0704 - accuracy: 0.9477 - val_loss: 0.0944 - val_accuracy: 0.9234\n",
      "Epoch 925/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9477\n",
      "Epoch 925: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0703 - accuracy: 0.9477 - val_loss: 0.0943 - val_accuracy: 0.9234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 926/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9477\n",
      "Epoch 926: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0703 - accuracy: 0.9477 - val_loss: 0.0943 - val_accuracy: 0.9234\n",
      "Epoch 927/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9477\n",
      "Epoch 927: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0702 - accuracy: 0.9477 - val_loss: 0.0942 - val_accuracy: 0.9234\n",
      "Epoch 928/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9477\n",
      "Epoch 928: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0701 - accuracy: 0.9477 - val_loss: 0.0941 - val_accuracy: 0.9234\n",
      "Epoch 929/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9477\n",
      "Epoch 929: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0701 - accuracy: 0.9477 - val_loss: 0.0940 - val_accuracy: 0.9234\n",
      "Epoch 930/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9477\n",
      "Epoch 930: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0700 - accuracy: 0.9477 - val_loss: 0.0940 - val_accuracy: 0.9234\n",
      "Epoch 931/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9487\n",
      "Epoch 931: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0700 - accuracy: 0.9487 - val_loss: 0.0939 - val_accuracy: 0.9234\n",
      "Epoch 932/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9487\n",
      "Epoch 932: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0699 - accuracy: 0.9487 - val_loss: 0.0938 - val_accuracy: 0.9234\n",
      "Epoch 933/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9487\n",
      "Epoch 933: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0698 - accuracy: 0.9487 - val_loss: 0.0937 - val_accuracy: 0.9234\n",
      "Epoch 934/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9487\n",
      "Epoch 934: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0698 - accuracy: 0.9487 - val_loss: 0.0937 - val_accuracy: 0.9234\n",
      "Epoch 935/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9487\n",
      "Epoch 935: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0697 - accuracy: 0.9487 - val_loss: 0.0936 - val_accuracy: 0.9234\n",
      "Epoch 936/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9487\n",
      "Epoch 936: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0696 - accuracy: 0.9487 - val_loss: 0.0935 - val_accuracy: 0.9234\n",
      "Epoch 937/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9487\n",
      "Epoch 937: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0696 - accuracy: 0.9487 - val_loss: 0.0934 - val_accuracy: 0.9234\n",
      "Epoch 938/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9487\n",
      "Epoch 938: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0695 - accuracy: 0.9487 - val_loss: 0.0934 - val_accuracy: 0.9234\n",
      "Epoch 939/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9487\n",
      "Epoch 939: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0695 - accuracy: 0.9487 - val_loss: 0.0933 - val_accuracy: 0.9234\n",
      "Epoch 940/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9497\n",
      "Epoch 940: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0694 - accuracy: 0.9497 - val_loss: 0.0932 - val_accuracy: 0.9234\n",
      "Epoch 941/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9497\n",
      "Epoch 941: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0693 - accuracy: 0.9497 - val_loss: 0.0931 - val_accuracy: 0.9234\n",
      "Epoch 942/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9497\n",
      "Epoch 942: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0693 - accuracy: 0.9497 - val_loss: 0.0931 - val_accuracy: 0.9234\n",
      "Epoch 943/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9497\n",
      "Epoch 943: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0692 - accuracy: 0.9497 - val_loss: 0.0930 - val_accuracy: 0.9234\n",
      "Epoch 944/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9497\n",
      "Epoch 944: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0692 - accuracy: 0.9497 - val_loss: 0.0929 - val_accuracy: 0.9234\n",
      "Epoch 945/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9497\n",
      "Epoch 945: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0691 - accuracy: 0.9497 - val_loss: 0.0929 - val_accuracy: 0.9234\n",
      "Epoch 946/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9497\n",
      "Epoch 946: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0691 - accuracy: 0.9497 - val_loss: 0.0928 - val_accuracy: 0.9234\n",
      "Epoch 947/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9497\n",
      "Epoch 947: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0690 - accuracy: 0.9497 - val_loss: 0.0927 - val_accuracy: 0.9234\n",
      "Epoch 948/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9497\n",
      "Epoch 948: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0689 - accuracy: 0.9497 - val_loss: 0.0926 - val_accuracy: 0.9234\n",
      "Epoch 949/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9497\n",
      "Epoch 949: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0689 - accuracy: 0.9497 - val_loss: 0.0926 - val_accuracy: 0.9234\n",
      "Epoch 950/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9497\n",
      "Epoch 950: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0688 - accuracy: 0.9497 - val_loss: 0.0925 - val_accuracy: 0.9234\n",
      "Epoch 951/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9497\n",
      "Epoch 951: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0688 - accuracy: 0.9497 - val_loss: 0.0924 - val_accuracy: 0.9234\n",
      "Epoch 952/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9497\n",
      "Epoch 952: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0687 - accuracy: 0.9497 - val_loss: 0.0923 - val_accuracy: 0.9234\n",
      "Epoch 953/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9497\n",
      "Epoch 953: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0686 - accuracy: 0.9497 - val_loss: 0.0923 - val_accuracy: 0.9234\n",
      "Epoch 954/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9497\n",
      "Epoch 954: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0686 - accuracy: 0.9497 - val_loss: 0.0922 - val_accuracy: 0.9234\n",
      "Epoch 955/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9497\n",
      "Epoch 955: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0685 - accuracy: 0.9497 - val_loss: 0.0921 - val_accuracy: 0.9234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 956/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9497\n",
      "Epoch 956: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0685 - accuracy: 0.9497 - val_loss: 0.0921 - val_accuracy: 0.9234\n",
      "Epoch 957/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9497\n",
      "Epoch 957: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0684 - accuracy: 0.9497 - val_loss: 0.0920 - val_accuracy: 0.9234\n",
      "Epoch 958/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9497\n",
      "Epoch 958: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0683 - accuracy: 0.9497 - val_loss: 0.0919 - val_accuracy: 0.9234\n",
      "Epoch 959/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9497\n",
      "Epoch 959: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0683 - accuracy: 0.9497 - val_loss: 0.0918 - val_accuracy: 0.9234\n",
      "Epoch 960/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9497\n",
      "Epoch 960: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0682 - accuracy: 0.9497 - val_loss: 0.0918 - val_accuracy: 0.9257\n",
      "Epoch 961/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9497\n",
      "Epoch 961: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0682 - accuracy: 0.9497 - val_loss: 0.0917 - val_accuracy: 0.9257\n",
      "Epoch 962/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9497\n",
      "Epoch 962: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0681 - accuracy: 0.9497 - val_loss: 0.0916 - val_accuracy: 0.9257\n",
      "Epoch 963/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9497\n",
      "Epoch 963: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0681 - accuracy: 0.9497 - val_loss: 0.0916 - val_accuracy: 0.9257\n",
      "Epoch 964/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9497\n",
      "Epoch 964: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0680 - accuracy: 0.9497 - val_loss: 0.0915 - val_accuracy: 0.9257\n",
      "Epoch 965/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9497\n",
      "Epoch 965: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0679 - accuracy: 0.9497 - val_loss: 0.0914 - val_accuracy: 0.9257\n",
      "Epoch 966/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9497\n",
      "Epoch 966: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0679 - accuracy: 0.9497 - val_loss: 0.0913 - val_accuracy: 0.9257\n",
      "Epoch 967/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9497\n",
      "Epoch 967: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0678 - accuracy: 0.9497 - val_loss: 0.0913 - val_accuracy: 0.9257\n",
      "Epoch 968/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9497\n",
      "Epoch 968: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0678 - accuracy: 0.9497 - val_loss: 0.0912 - val_accuracy: 0.9257\n",
      "Epoch 969/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9497\n",
      "Epoch 969: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0677 - accuracy: 0.9497 - val_loss: 0.0911 - val_accuracy: 0.9257\n",
      "Epoch 970/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9497\n",
      "Epoch 970: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0677 - accuracy: 0.9497 - val_loss: 0.0911 - val_accuracy: 0.9257\n",
      "Epoch 971/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9497\n",
      "Epoch 971: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0676 - accuracy: 0.9497 - val_loss: 0.0910 - val_accuracy: 0.9257\n",
      "Epoch 972/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9497\n",
      "Epoch 972: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0675 - accuracy: 0.9497 - val_loss: 0.0909 - val_accuracy: 0.9257\n",
      "Epoch 973/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9497\n",
      "Epoch 973: val_accuracy did not improve from 0.92568\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0675 - accuracy: 0.9497 - val_loss: 0.0909 - val_accuracy: 0.9257\n",
      "Epoch 974/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9497\n",
      "Epoch 974: val_accuracy improved from 0.92568 to 0.92793, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0674 - accuracy: 0.9497 - val_loss: 0.0908 - val_accuracy: 0.9279\n",
      "Epoch 975/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9497\n",
      "Epoch 975: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0674 - accuracy: 0.9497 - val_loss: 0.0907 - val_accuracy: 0.9279\n",
      "Epoch 976/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9497\n",
      "Epoch 976: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0673 - accuracy: 0.9497 - val_loss: 0.0906 - val_accuracy: 0.9279\n",
      "Epoch 977/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9497\n",
      "Epoch 977: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0673 - accuracy: 0.9497 - val_loss: 0.0906 - val_accuracy: 0.9279\n",
      "Epoch 978/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9497\n",
      "Epoch 978: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0672 - accuracy: 0.9497 - val_loss: 0.0905 - val_accuracy: 0.9279\n",
      "Epoch 979/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9497\n",
      "Epoch 979: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0671 - accuracy: 0.9497 - val_loss: 0.0904 - val_accuracy: 0.9279\n",
      "Epoch 980/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9497\n",
      "Epoch 980: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0671 - accuracy: 0.9497 - val_loss: 0.0904 - val_accuracy: 0.9279\n",
      "Epoch 981/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9497\n",
      "Epoch 981: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0670 - accuracy: 0.9497 - val_loss: 0.0903 - val_accuracy: 0.9279\n",
      "Epoch 982/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9497\n",
      "Epoch 982: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0670 - accuracy: 0.9497 - val_loss: 0.0902 - val_accuracy: 0.9279\n",
      "Epoch 983/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9497\n",
      "Epoch 983: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0669 - accuracy: 0.9497 - val_loss: 0.0902 - val_accuracy: 0.9279\n",
      "Epoch 984/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9497\n",
      "Epoch 984: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0669 - accuracy: 0.9497 - val_loss: 0.0901 - val_accuracy: 0.9279\n",
      "Epoch 985/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9497\n",
      "Epoch 985: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0668 - accuracy: 0.9497 - val_loss: 0.0900 - val_accuracy: 0.9279\n",
      "Epoch 986/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9497\n",
      "Epoch 986: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0668 - accuracy: 0.9497 - val_loss: 0.0899 - val_accuracy: 0.9279\n",
      "Epoch 987/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9497\n",
      "Epoch 987: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0667 - accuracy: 0.9497 - val_loss: 0.0899 - val_accuracy: 0.9279\n",
      "Epoch 988/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9497\n",
      "Epoch 988: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0666 - accuracy: 0.9497 - val_loss: 0.0898 - val_accuracy: 0.9279\n",
      "Epoch 989/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9497\n",
      "Epoch 989: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0666 - accuracy: 0.9497 - val_loss: 0.0897 - val_accuracy: 0.9279\n",
      "Epoch 990/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9497\n",
      "Epoch 990: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0665 - accuracy: 0.9497 - val_loss: 0.0897 - val_accuracy: 0.9279\n",
      "Epoch 991/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9497\n",
      "Epoch 991: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0665 - accuracy: 0.9497 - val_loss: 0.0896 - val_accuracy: 0.9279\n",
      "Epoch 992/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9497\n",
      "Epoch 992: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0664 - accuracy: 0.9497 - val_loss: 0.0895 - val_accuracy: 0.9279\n",
      "Epoch 993/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9497\n",
      "Epoch 993: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0664 - accuracy: 0.9497 - val_loss: 0.0895 - val_accuracy: 0.9279\n",
      "Epoch 994/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9497\n",
      "Epoch 994: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0663 - accuracy: 0.9497 - val_loss: 0.0894 - val_accuracy: 0.9279\n",
      "Epoch 995/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9497\n",
      "Epoch 995: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0663 - accuracy: 0.9497 - val_loss: 0.0893 - val_accuracy: 0.9279\n",
      "Epoch 996/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9497\n",
      "Epoch 996: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0662 - accuracy: 0.9497 - val_loss: 0.0893 - val_accuracy: 0.9279\n",
      "Epoch 997/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9506\n",
      "Epoch 997: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0661 - accuracy: 0.9506 - val_loss: 0.0892 - val_accuracy: 0.9279\n",
      "Epoch 998/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9506\n",
      "Epoch 998: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0661 - accuracy: 0.9506 - val_loss: 0.0891 - val_accuracy: 0.9279\n",
      "Epoch 999/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9506\n",
      "Epoch 999: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0660 - accuracy: 0.9506 - val_loss: 0.0891 - val_accuracy: 0.9279\n",
      "Epoch 1000/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0660 - accuracy: 0.9506\n",
      "Epoch 1000: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0660 - accuracy: 0.9506 - val_loss: 0.0890 - val_accuracy: 0.9279\n",
      "Epoch 1001/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9506\n",
      "Epoch 1001: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0659 - accuracy: 0.9506 - val_loss: 0.0889 - val_accuracy: 0.9279\n",
      "Epoch 1002/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9506\n",
      "Epoch 1002: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0659 - accuracy: 0.9506 - val_loss: 0.0888 - val_accuracy: 0.9279\n",
      "Epoch 1003/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9506\n",
      "Epoch 1003: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0658 - accuracy: 0.9506 - val_loss: 0.0888 - val_accuracy: 0.9279\n",
      "Epoch 1004/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9506\n",
      "Epoch 1004: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0658 - accuracy: 0.9506 - val_loss: 0.0887 - val_accuracy: 0.9279\n",
      "Epoch 1005/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9506\n",
      "Epoch 1005: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0657 - accuracy: 0.9506 - val_loss: 0.0887 - val_accuracy: 0.9279\n",
      "Epoch 1006/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9506\n",
      "Epoch 1006: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0657 - accuracy: 0.9506 - val_loss: 0.0886 - val_accuracy: 0.9279\n",
      "Epoch 1007/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9516\n",
      "Epoch 1007: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0656 - accuracy: 0.9516 - val_loss: 0.0885 - val_accuracy: 0.9279\n",
      "Epoch 1008/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9516\n",
      "Epoch 1008: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0655 - accuracy: 0.9516 - val_loss: 0.0884 - val_accuracy: 0.9279\n",
      "Epoch 1009/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9516\n",
      "Epoch 1009: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0655 - accuracy: 0.9516 - val_loss: 0.0884 - val_accuracy: 0.9279\n",
      "Epoch 1010/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9516\n",
      "Epoch 1010: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0654 - accuracy: 0.9516 - val_loss: 0.0883 - val_accuracy: 0.9279\n",
      "Epoch 1011/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9516\n",
      "Epoch 1011: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0654 - accuracy: 0.9516 - val_loss: 0.0882 - val_accuracy: 0.9279\n",
      "Epoch 1012/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9516\n",
      "Epoch 1012: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0653 - accuracy: 0.9516 - val_loss: 0.0882 - val_accuracy: 0.9279\n",
      "Epoch 1013/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9516\n",
      "Epoch 1013: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0653 - accuracy: 0.9516 - val_loss: 0.0881 - val_accuracy: 0.9279\n",
      "Epoch 1014/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9516\n",
      "Epoch 1014: val_accuracy did not improve from 0.92793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0652 - accuracy: 0.9516 - val_loss: 0.0881 - val_accuracy: 0.9279\n",
      "Epoch 1015/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9516\n",
      "Epoch 1015: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0652 - accuracy: 0.9516 - val_loss: 0.0880 - val_accuracy: 0.9279\n",
      "Epoch 1016/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9516\n",
      "Epoch 1016: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0651 - accuracy: 0.9516 - val_loss: 0.0879 - val_accuracy: 0.9279\n",
      "Epoch 1017/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9516\n",
      "Epoch 1017: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0651 - accuracy: 0.9516 - val_loss: 0.0879 - val_accuracy: 0.9279\n",
      "Epoch 1018/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9516\n",
      "Epoch 1018: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0650 - accuracy: 0.9516 - val_loss: 0.0878 - val_accuracy: 0.9279\n",
      "Epoch 1019/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9516\n",
      "Epoch 1019: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0650 - accuracy: 0.9516 - val_loss: 0.0877 - val_accuracy: 0.9279\n",
      "Epoch 1020/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9516\n",
      "Epoch 1020: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0649 - accuracy: 0.9516 - val_loss: 0.0877 - val_accuracy: 0.9279\n",
      "Epoch 1021/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9516\n",
      "Epoch 1021: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0649 - accuracy: 0.9516 - val_loss: 0.0876 - val_accuracy: 0.9279\n",
      "Epoch 1022/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9516\n",
      "Epoch 1022: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0648 - accuracy: 0.9516 - val_loss: 0.0875 - val_accuracy: 0.9279\n",
      "Epoch 1023/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9516\n",
      "Epoch 1023: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0647 - accuracy: 0.9516 - val_loss: 0.0875 - val_accuracy: 0.9279\n",
      "Epoch 1024/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9516\n",
      "Epoch 1024: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0647 - accuracy: 0.9516 - val_loss: 0.0874 - val_accuracy: 0.9279\n",
      "Epoch 1025/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9516\n",
      "Epoch 1025: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0646 - accuracy: 0.9516 - val_loss: 0.0873 - val_accuracy: 0.9279\n",
      "Epoch 1026/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9516\n",
      "Epoch 1026: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0646 - accuracy: 0.9516 - val_loss: 0.0873 - val_accuracy: 0.9279\n",
      "Epoch 1027/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9516\n",
      "Epoch 1027: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0645 - accuracy: 0.9516 - val_loss: 0.0872 - val_accuracy: 0.9279\n",
      "Epoch 1028/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9516\n",
      "Epoch 1028: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0645 - accuracy: 0.9516 - val_loss: 0.0871 - val_accuracy: 0.9279\n",
      "Epoch 1029/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9516\n",
      "Epoch 1029: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0644 - accuracy: 0.9516 - val_loss: 0.0871 - val_accuracy: 0.9279\n",
      "Epoch 1030/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9516\n",
      "Epoch 1030: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0644 - accuracy: 0.9516 - val_loss: 0.0870 - val_accuracy: 0.9279\n",
      "Epoch 1031/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9516\n",
      "Epoch 1031: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0643 - accuracy: 0.9516 - val_loss: 0.0870 - val_accuracy: 0.9279\n",
      "Epoch 1032/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9516\n",
      "Epoch 1032: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0643 - accuracy: 0.9516 - val_loss: 0.0869 - val_accuracy: 0.9279\n",
      "Epoch 1033/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9516\n",
      "Epoch 1033: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0642 - accuracy: 0.9516 - val_loss: 0.0868 - val_accuracy: 0.9279\n",
      "Epoch 1034/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9516\n",
      "Epoch 1034: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0642 - accuracy: 0.9516 - val_loss: 0.0868 - val_accuracy: 0.9279\n",
      "Epoch 1035/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9516\n",
      "Epoch 1035: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0641 - accuracy: 0.9516 - val_loss: 0.0867 - val_accuracy: 0.9279\n",
      "Epoch 1036/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9526\n",
      "Epoch 1036: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0641 - accuracy: 0.9526 - val_loss: 0.0866 - val_accuracy: 0.9279\n",
      "Epoch 1037/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9526\n",
      "Epoch 1037: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0640 - accuracy: 0.9526 - val_loss: 0.0866 - val_accuracy: 0.9279\n",
      "Epoch 1038/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9526\n",
      "Epoch 1038: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0640 - accuracy: 0.9526 - val_loss: 0.0865 - val_accuracy: 0.9279\n",
      "Epoch 1039/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9526\n",
      "Epoch 1039: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0639 - accuracy: 0.9526 - val_loss: 0.0864 - val_accuracy: 0.9279\n",
      "Epoch 1040/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9526\n",
      "Epoch 1040: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0639 - accuracy: 0.9526 - val_loss: 0.0864 - val_accuracy: 0.9279\n",
      "Epoch 1041/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9526\n",
      "Epoch 1041: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0638 - accuracy: 0.9526 - val_loss: 0.0863 - val_accuracy: 0.9279\n",
      "Epoch 1042/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9526\n",
      "Epoch 1042: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0638 - accuracy: 0.9526 - val_loss: 0.0862 - val_accuracy: 0.9279\n",
      "Epoch 1043/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9526\n",
      "Epoch 1043: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0637 - accuracy: 0.9526 - val_loss: 0.0862 - val_accuracy: 0.9279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1044/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9526\n",
      "Epoch 1044: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0637 - accuracy: 0.9526 - val_loss: 0.0861 - val_accuracy: 0.9279\n",
      "Epoch 1045/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9526\n",
      "Epoch 1045: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0636 - accuracy: 0.9526 - val_loss: 0.0861 - val_accuracy: 0.9279\n",
      "Epoch 1046/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9526\n",
      "Epoch 1046: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0636 - accuracy: 0.9526 - val_loss: 0.0860 - val_accuracy: 0.9279\n",
      "Epoch 1047/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9526\n",
      "Epoch 1047: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0635 - accuracy: 0.9526 - val_loss: 0.0859 - val_accuracy: 0.9279\n",
      "Epoch 1048/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9526\n",
      "Epoch 1048: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0635 - accuracy: 0.9526 - val_loss: 0.0859 - val_accuracy: 0.9279\n",
      "Epoch 1049/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9526\n",
      "Epoch 1049: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0634 - accuracy: 0.9526 - val_loss: 0.0858 - val_accuracy: 0.9279\n",
      "Epoch 1050/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9526\n",
      "Epoch 1050: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0634 - accuracy: 0.9526 - val_loss: 0.0857 - val_accuracy: 0.9279\n",
      "Epoch 1051/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9526\n",
      "Epoch 1051: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0633 - accuracy: 0.9526 - val_loss: 0.0857 - val_accuracy: 0.9279\n",
      "Epoch 1052/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9526\n",
      "Epoch 1052: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0633 - accuracy: 0.9526 - val_loss: 0.0856 - val_accuracy: 0.9279\n",
      "Epoch 1053/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9526\n",
      "Epoch 1053: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0632 - accuracy: 0.9526 - val_loss: 0.0856 - val_accuracy: 0.9279\n",
      "Epoch 1054/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9526\n",
      "Epoch 1054: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0632 - accuracy: 0.9526 - val_loss: 0.0855 - val_accuracy: 0.9279\n",
      "Epoch 1055/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9526\n",
      "Epoch 1055: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0631 - accuracy: 0.9526 - val_loss: 0.0854 - val_accuracy: 0.9279\n",
      "Epoch 1056/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9526\n",
      "Epoch 1056: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0630 - accuracy: 0.9526 - val_loss: 0.0854 - val_accuracy: 0.9279\n",
      "Epoch 1057/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9526\n",
      "Epoch 1057: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0630 - accuracy: 0.9526 - val_loss: 0.0853 - val_accuracy: 0.9279\n",
      "Epoch 1058/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9535\n",
      "Epoch 1058: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0629 - accuracy: 0.9535 - val_loss: 0.0852 - val_accuracy: 0.9279\n",
      "Epoch 1059/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9535\n",
      "Epoch 1059: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0629 - accuracy: 0.9535 - val_loss: 0.0852 - val_accuracy: 0.9279\n",
      "Epoch 1060/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9535\n",
      "Epoch 1060: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0628 - accuracy: 0.9535 - val_loss: 0.0851 - val_accuracy: 0.9279\n",
      "Epoch 1061/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9535\n",
      "Epoch 1061: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0628 - accuracy: 0.9535 - val_loss: 0.0851 - val_accuracy: 0.9279\n",
      "Epoch 1062/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9535\n",
      "Epoch 1062: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0627 - accuracy: 0.9535 - val_loss: 0.0850 - val_accuracy: 0.9279\n",
      "Epoch 1063/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9535\n",
      "Epoch 1063: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0627 - accuracy: 0.9535 - val_loss: 0.0849 - val_accuracy: 0.9279\n",
      "Epoch 1064/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9535\n",
      "Epoch 1064: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0627 - accuracy: 0.9535 - val_loss: 0.0849 - val_accuracy: 0.9279\n",
      "Epoch 1065/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9535\n",
      "Epoch 1065: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0626 - accuracy: 0.9535 - val_loss: 0.0848 - val_accuracy: 0.9279\n",
      "Epoch 1066/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9535\n",
      "Epoch 1066: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0626 - accuracy: 0.9535 - val_loss: 0.0847 - val_accuracy: 0.9279\n",
      "Epoch 1067/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9535\n",
      "Epoch 1067: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0625 - accuracy: 0.9535 - val_loss: 0.0847 - val_accuracy: 0.9279\n",
      "Epoch 1068/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9535\n",
      "Epoch 1068: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0625 - accuracy: 0.9535 - val_loss: 0.0846 - val_accuracy: 0.9279\n",
      "Epoch 1069/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9535\n",
      "Epoch 1069: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0624 - accuracy: 0.9535 - val_loss: 0.0846 - val_accuracy: 0.9279\n",
      "Epoch 1070/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9535\n",
      "Epoch 1070: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0624 - accuracy: 0.9535 - val_loss: 0.0845 - val_accuracy: 0.9279\n",
      "Epoch 1071/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9535\n",
      "Epoch 1071: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0623 - accuracy: 0.9535 - val_loss: 0.0844 - val_accuracy: 0.9279\n",
      "Epoch 1072/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9535\n",
      "Epoch 1072: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0623 - accuracy: 0.9535 - val_loss: 0.0844 - val_accuracy: 0.9279\n",
      "Epoch 1073/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9535\n",
      "Epoch 1073: val_accuracy did not improve from 0.92793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0622 - accuracy: 0.9535 - val_loss: 0.0843 - val_accuracy: 0.9279\n",
      "Epoch 1074/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9535\n",
      "Epoch 1074: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0622 - accuracy: 0.9535 - val_loss: 0.0843 - val_accuracy: 0.9279\n",
      "Epoch 1075/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9535\n",
      "Epoch 1075: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0621 - accuracy: 0.9535 - val_loss: 0.0842 - val_accuracy: 0.9279\n",
      "Epoch 1076/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9535\n",
      "Epoch 1076: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0621 - accuracy: 0.9535 - val_loss: 0.0841 - val_accuracy: 0.9279\n",
      "Epoch 1077/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9535\n",
      "Epoch 1077: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0620 - accuracy: 0.9535 - val_loss: 0.0841 - val_accuracy: 0.9279\n",
      "Epoch 1078/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9535\n",
      "Epoch 1078: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0620 - accuracy: 0.9535 - val_loss: 0.0840 - val_accuracy: 0.9279\n",
      "Epoch 1079/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9535\n",
      "Epoch 1079: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0619 - accuracy: 0.9535 - val_loss: 0.0840 - val_accuracy: 0.9279\n",
      "Epoch 1080/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9535\n",
      "Epoch 1080: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0619 - accuracy: 0.9535 - val_loss: 0.0839 - val_accuracy: 0.9279\n",
      "Epoch 1081/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9535\n",
      "Epoch 1081: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0618 - accuracy: 0.9535 - val_loss: 0.0838 - val_accuracy: 0.9279\n",
      "Epoch 1082/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9535\n",
      "Epoch 1082: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0618 - accuracy: 0.9535 - val_loss: 0.0838 - val_accuracy: 0.9279\n",
      "Epoch 1083/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9535\n",
      "Epoch 1083: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0617 - accuracy: 0.9535 - val_loss: 0.0837 - val_accuracy: 0.9279\n",
      "Epoch 1084/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9535\n",
      "Epoch 1084: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0617 - accuracy: 0.9535 - val_loss: 0.0837 - val_accuracy: 0.9279\n",
      "Epoch 1085/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9535\n",
      "Epoch 1085: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0616 - accuracy: 0.9535 - val_loss: 0.0836 - val_accuracy: 0.9279\n",
      "Epoch 1086/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9535\n",
      "Epoch 1086: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0616 - accuracy: 0.9535 - val_loss: 0.0835 - val_accuracy: 0.9279\n",
      "Epoch 1087/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9535\n",
      "Epoch 1087: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0615 - accuracy: 0.9535 - val_loss: 0.0835 - val_accuracy: 0.9279\n",
      "Epoch 1088/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9535\n",
      "Epoch 1088: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0615 - accuracy: 0.9535 - val_loss: 0.0834 - val_accuracy: 0.9279\n",
      "Epoch 1089/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9535\n",
      "Epoch 1089: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0614 - accuracy: 0.9535 - val_loss: 0.0834 - val_accuracy: 0.9279\n",
      "Epoch 1090/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9535\n",
      "Epoch 1090: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0614 - accuracy: 0.9535 - val_loss: 0.0833 - val_accuracy: 0.9279\n",
      "Epoch 1091/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9535\n",
      "Epoch 1091: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0613 - accuracy: 0.9535 - val_loss: 0.0832 - val_accuracy: 0.9279\n",
      "Epoch 1092/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9535\n",
      "Epoch 1092: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0613 - accuracy: 0.9535 - val_loss: 0.0832 - val_accuracy: 0.9279\n",
      "Epoch 1093/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9535\n",
      "Epoch 1093: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0612 - accuracy: 0.9535 - val_loss: 0.0831 - val_accuracy: 0.9279\n",
      "Epoch 1094/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9535\n",
      "Epoch 1094: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0612 - accuracy: 0.9535 - val_loss: 0.0831 - val_accuracy: 0.9279\n",
      "Epoch 1095/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9535\n",
      "Epoch 1095: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0611 - accuracy: 0.9535 - val_loss: 0.0830 - val_accuracy: 0.9279\n",
      "Epoch 1096/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9535\n",
      "Epoch 1096: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0611 - accuracy: 0.9535 - val_loss: 0.0829 - val_accuracy: 0.9279\n",
      "Epoch 1097/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9535\n",
      "Epoch 1097: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0611 - accuracy: 0.9535 - val_loss: 0.0829 - val_accuracy: 0.9279\n",
      "Epoch 1098/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9535\n",
      "Epoch 1098: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0610 - accuracy: 0.9535 - val_loss: 0.0828 - val_accuracy: 0.9279\n",
      "Epoch 1099/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9535\n",
      "Epoch 1099: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0610 - accuracy: 0.9535 - val_loss: 0.0828 - val_accuracy: 0.9279\n",
      "Epoch 1100/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9535\n",
      "Epoch 1100: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0609 - accuracy: 0.9535 - val_loss: 0.0827 - val_accuracy: 0.9279\n",
      "Epoch 1101/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9535\n",
      "Epoch 1101: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0609 - accuracy: 0.9535 - val_loss: 0.0827 - val_accuracy: 0.9279\n",
      "Epoch 1102/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9535\n",
      "Epoch 1102: val_accuracy did not improve from 0.92793\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0608 - accuracy: 0.9535 - val_loss: 0.0826 - val_accuracy: 0.9279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1103/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9535\n",
      "Epoch 1103: val_accuracy improved from 0.92793 to 0.93018, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0608 - accuracy: 0.9535 - val_loss: 0.0825 - val_accuracy: 0.9302\n",
      "Epoch 1104/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.9535\n",
      "Epoch 1104: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0607 - accuracy: 0.9535 - val_loss: 0.0825 - val_accuracy: 0.9302\n",
      "Epoch 1105/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.9535\n",
      "Epoch 1105: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0607 - accuracy: 0.9535 - val_loss: 0.0824 - val_accuracy: 0.9302\n",
      "Epoch 1106/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9535\n",
      "Epoch 1106: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0606 - accuracy: 0.9535 - val_loss: 0.0824 - val_accuracy: 0.9302\n",
      "Epoch 1107/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9535\n",
      "Epoch 1107: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0606 - accuracy: 0.9535 - val_loss: 0.0823 - val_accuracy: 0.9302\n",
      "Epoch 1108/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9535\n",
      "Epoch 1108: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0605 - accuracy: 0.9535 - val_loss: 0.0822 - val_accuracy: 0.9302\n",
      "Epoch 1109/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9535\n",
      "Epoch 1109: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0605 - accuracy: 0.9535 - val_loss: 0.0822 - val_accuracy: 0.9302\n",
      "Epoch 1110/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9535\n",
      "Epoch 1110: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0604 - accuracy: 0.9535 - val_loss: 0.0821 - val_accuracy: 0.9302\n",
      "Epoch 1111/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9535\n",
      "Epoch 1111: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0604 - accuracy: 0.9535 - val_loss: 0.0821 - val_accuracy: 0.9302\n",
      "Epoch 1112/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9535\n",
      "Epoch 1112: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0603 - accuracy: 0.9535 - val_loss: 0.0820 - val_accuracy: 0.9302\n",
      "Epoch 1113/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9535\n",
      "Epoch 1113: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0603 - accuracy: 0.9535 - val_loss: 0.0820 - val_accuracy: 0.9302\n",
      "Epoch 1114/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9535\n",
      "Epoch 1114: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0603 - accuracy: 0.9535 - val_loss: 0.0819 - val_accuracy: 0.9302\n",
      "Epoch 1115/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9535\n",
      "Epoch 1115: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0602 - accuracy: 0.9535 - val_loss: 0.0818 - val_accuracy: 0.9302\n",
      "Epoch 1116/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9535\n",
      "Epoch 1116: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0602 - accuracy: 0.9535 - val_loss: 0.0818 - val_accuracy: 0.9302\n",
      "Epoch 1117/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9535\n",
      "Epoch 1117: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0601 - accuracy: 0.9535 - val_loss: 0.0817 - val_accuracy: 0.9302\n",
      "Epoch 1118/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9535\n",
      "Epoch 1118: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0601 - accuracy: 0.9535 - val_loss: 0.0817 - val_accuracy: 0.9302\n",
      "Epoch 1119/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9535\n",
      "Epoch 1119: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0600 - accuracy: 0.9535 - val_loss: 0.0816 - val_accuracy: 0.9302\n",
      "Epoch 1120/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9535\n",
      "Epoch 1120: val_accuracy did not improve from 0.93018\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0600 - accuracy: 0.9535 - val_loss: 0.0816 - val_accuracy: 0.9302\n",
      "Epoch 1121/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9555\n",
      "Epoch 1121: val_accuracy improved from 0.93018 to 0.93243, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0599 - accuracy: 0.9555 - val_loss: 0.0815 - val_accuracy: 0.9324\n",
      "Epoch 1122/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9555\n",
      "Epoch 1122: val_accuracy did not improve from 0.93243\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0599 - accuracy: 0.9555 - val_loss: 0.0814 - val_accuracy: 0.9324\n",
      "Epoch 1123/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9555\n",
      "Epoch 1123: val_accuracy did not improve from 0.93243\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0598 - accuracy: 0.9555 - val_loss: 0.0814 - val_accuracy: 0.9324\n",
      "Epoch 1124/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9555\n",
      "Epoch 1124: val_accuracy did not improve from 0.93243\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0598 - accuracy: 0.9555 - val_loss: 0.0813 - val_accuracy: 0.9324\n",
      "Epoch 1125/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9555\n",
      "Epoch 1125: val_accuracy did not improve from 0.93243\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0597 - accuracy: 0.9555 - val_loss: 0.0813 - val_accuracy: 0.9324\n",
      "Epoch 1126/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9555\n",
      "Epoch 1126: val_accuracy did not improve from 0.93243\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0597 - accuracy: 0.9555 - val_loss: 0.0812 - val_accuracy: 0.9324\n",
      "Epoch 1127/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9555\n",
      "Epoch 1127: val_accuracy did not improve from 0.93243\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0597 - accuracy: 0.9555 - val_loss: 0.0812 - val_accuracy: 0.9324\n",
      "Epoch 1128/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9555\n",
      "Epoch 1128: val_accuracy did not improve from 0.93243\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0596 - accuracy: 0.9555 - val_loss: 0.0811 - val_accuracy: 0.9324\n",
      "Epoch 1129/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9555\n",
      "Epoch 1129: val_accuracy did not improve from 0.93243\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0596 - accuracy: 0.9555 - val_loss: 0.0810 - val_accuracy: 0.9324\n",
      "Epoch 1130/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9555\n",
      "Epoch 1130: val_accuracy improved from 0.93243 to 0.93468, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0595 - accuracy: 0.9555 - val_loss: 0.0810 - val_accuracy: 0.9347\n",
      "Epoch 1131/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9564\n",
      "Epoch 1131: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0595 - accuracy: 0.9564 - val_loss: 0.0809 - val_accuracy: 0.9347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1132/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9564\n",
      "Epoch 1132: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0594 - accuracy: 0.9564 - val_loss: 0.0809 - val_accuracy: 0.9347\n",
      "Epoch 1133/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9564\n",
      "Epoch 1133: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0594 - accuracy: 0.9564 - val_loss: 0.0808 - val_accuracy: 0.9347\n",
      "Epoch 1134/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9564\n",
      "Epoch 1134: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0593 - accuracy: 0.9564 - val_loss: 0.0808 - val_accuracy: 0.9347\n",
      "Epoch 1135/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9564\n",
      "Epoch 1135: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0593 - accuracy: 0.9564 - val_loss: 0.0807 - val_accuracy: 0.9347\n",
      "Epoch 1136/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9564\n",
      "Epoch 1136: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0593 - accuracy: 0.9564 - val_loss: 0.0807 - val_accuracy: 0.9347\n",
      "Epoch 1137/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.9564\n",
      "Epoch 1137: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0592 - accuracy: 0.9564 - val_loss: 0.0806 - val_accuracy: 0.9347\n",
      "Epoch 1138/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.9564\n",
      "Epoch 1138: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0592 - accuracy: 0.9564 - val_loss: 0.0805 - val_accuracy: 0.9347\n",
      "Epoch 1139/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9564\n",
      "Epoch 1139: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0591 - accuracy: 0.9564 - val_loss: 0.0805 - val_accuracy: 0.9347\n",
      "Epoch 1140/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9564\n",
      "Epoch 1140: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0591 - accuracy: 0.9564 - val_loss: 0.0804 - val_accuracy: 0.9347\n",
      "Epoch 1141/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9564\n",
      "Epoch 1141: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0590 - accuracy: 0.9564 - val_loss: 0.0804 - val_accuracy: 0.9347\n",
      "Epoch 1142/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9574\n",
      "Epoch 1142: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0590 - accuracy: 0.9574 - val_loss: 0.0803 - val_accuracy: 0.9347\n",
      "Epoch 1143/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9574\n",
      "Epoch 1143: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0589 - accuracy: 0.9574 - val_loss: 0.0803 - val_accuracy: 0.9347\n",
      "Epoch 1144/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9574\n",
      "Epoch 1144: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0589 - accuracy: 0.9574 - val_loss: 0.0802 - val_accuracy: 0.9347\n",
      "Epoch 1145/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9574\n",
      "Epoch 1145: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0589 - accuracy: 0.9574 - val_loss: 0.0802 - val_accuracy: 0.9347\n",
      "Epoch 1146/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9574\n",
      "Epoch 1146: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0588 - accuracy: 0.9574 - val_loss: 0.0801 - val_accuracy: 0.9347\n",
      "Epoch 1147/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9574\n",
      "Epoch 1147: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0588 - accuracy: 0.9574 - val_loss: 0.0800 - val_accuracy: 0.9347\n",
      "Epoch 1148/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9574\n",
      "Epoch 1148: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0587 - accuracy: 0.9574 - val_loss: 0.0800 - val_accuracy: 0.9347\n",
      "Epoch 1149/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9574\n",
      "Epoch 1149: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0587 - accuracy: 0.9574 - val_loss: 0.0799 - val_accuracy: 0.9347\n",
      "Epoch 1150/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9574\n",
      "Epoch 1150: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0586 - accuracy: 0.9574 - val_loss: 0.0799 - val_accuracy: 0.9347\n",
      "Epoch 1151/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9574\n",
      "Epoch 1151: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0586 - accuracy: 0.9574 - val_loss: 0.0798 - val_accuracy: 0.9347\n",
      "Epoch 1152/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9574\n",
      "Epoch 1152: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0585 - accuracy: 0.9574 - val_loss: 0.0798 - val_accuracy: 0.9347\n",
      "Epoch 1153/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9574\n",
      "Epoch 1153: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0585 - accuracy: 0.9574 - val_loss: 0.0797 - val_accuracy: 0.9347\n",
      "Epoch 1154/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9574\n",
      "Epoch 1154: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0585 - accuracy: 0.9574 - val_loss: 0.0797 - val_accuracy: 0.9347\n",
      "Epoch 1155/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9574\n",
      "Epoch 1155: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0584 - accuracy: 0.9574 - val_loss: 0.0796 - val_accuracy: 0.9347\n",
      "Epoch 1156/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9574\n",
      "Epoch 1156: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0584 - accuracy: 0.9574 - val_loss: 0.0796 - val_accuracy: 0.9347\n",
      "Epoch 1157/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9574\n",
      "Epoch 1157: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0583 - accuracy: 0.9574 - val_loss: 0.0795 - val_accuracy: 0.9347\n",
      "Epoch 1158/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9574\n",
      "Epoch 1158: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0583 - accuracy: 0.9574 - val_loss: 0.0794 - val_accuracy: 0.9347\n",
      "Epoch 1159/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9574\n",
      "Epoch 1159: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0582 - accuracy: 0.9574 - val_loss: 0.0794 - val_accuracy: 0.9347\n",
      "Epoch 1160/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9574\n",
      "Epoch 1160: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0582 - accuracy: 0.9574 - val_loss: 0.0793 - val_accuracy: 0.9347\n",
      "Epoch 1161/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9574\n",
      "Epoch 1161: val_accuracy did not improve from 0.93468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0581 - accuracy: 0.9574 - val_loss: 0.0793 - val_accuracy: 0.9347\n",
      "Epoch 1162/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9574\n",
      "Epoch 1162: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0581 - accuracy: 0.9574 - val_loss: 0.0792 - val_accuracy: 0.9347\n",
      "Epoch 1163/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9574\n",
      "Epoch 1163: val_accuracy did not improve from 0.93468\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0581 - accuracy: 0.9574 - val_loss: 0.0792 - val_accuracy: 0.9347\n",
      "Epoch 1164/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9574\n",
      "Epoch 1164: val_accuracy improved from 0.93468 to 0.93694, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0580 - accuracy: 0.9574 - val_loss: 0.0791 - val_accuracy: 0.9369\n",
      "Epoch 1165/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9574\n",
      "Epoch 1165: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0580 - accuracy: 0.9574 - val_loss: 0.0791 - val_accuracy: 0.9369\n",
      "Epoch 1166/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9574\n",
      "Epoch 1166: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0579 - accuracy: 0.9574 - val_loss: 0.0790 - val_accuracy: 0.9369\n",
      "Epoch 1167/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9574\n",
      "Epoch 1167: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0579 - accuracy: 0.9574 - val_loss: 0.0790 - val_accuracy: 0.9369\n",
      "Epoch 1168/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9574\n",
      "Epoch 1168: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0578 - accuracy: 0.9574 - val_loss: 0.0789 - val_accuracy: 0.9369\n",
      "Epoch 1169/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9584\n",
      "Epoch 1169: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0578 - accuracy: 0.9584 - val_loss: 0.0789 - val_accuracy: 0.9369\n",
      "Epoch 1170/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9584\n",
      "Epoch 1170: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0578 - accuracy: 0.9584 - val_loss: 0.0788 - val_accuracy: 0.9369\n",
      "Epoch 1171/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9584\n",
      "Epoch 1171: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0577 - accuracy: 0.9584 - val_loss: 0.0788 - val_accuracy: 0.9369\n",
      "Epoch 1172/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9584\n",
      "Epoch 1172: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0577 - accuracy: 0.9584 - val_loss: 0.0787 - val_accuracy: 0.9369\n",
      "Epoch 1173/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9584\n",
      "Epoch 1173: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0576 - accuracy: 0.9584 - val_loss: 0.0786 - val_accuracy: 0.9369\n",
      "Epoch 1174/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9584\n",
      "Epoch 1174: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0576 - accuracy: 0.9584 - val_loss: 0.0786 - val_accuracy: 0.9369\n",
      "Epoch 1175/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9584\n",
      "Epoch 1175: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0575 - accuracy: 0.9584 - val_loss: 0.0785 - val_accuracy: 0.9369\n",
      "Epoch 1176/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9584\n",
      "Epoch 1176: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0575 - accuracy: 0.9584 - val_loss: 0.0785 - val_accuracy: 0.9369\n",
      "Epoch 1177/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9584\n",
      "Epoch 1177: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0575 - accuracy: 0.9584 - val_loss: 0.0784 - val_accuracy: 0.9369\n",
      "Epoch 1178/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9584\n",
      "Epoch 1178: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0574 - accuracy: 0.9584 - val_loss: 0.0784 - val_accuracy: 0.9369\n",
      "Epoch 1179/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9584\n",
      "Epoch 1179: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0574 - accuracy: 0.9584 - val_loss: 0.0783 - val_accuracy: 0.9369\n",
      "Epoch 1180/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9584\n",
      "Epoch 1180: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0573 - accuracy: 0.9584 - val_loss: 0.0783 - val_accuracy: 0.9369\n",
      "Epoch 1181/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9584\n",
      "Epoch 1181: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0573 - accuracy: 0.9584 - val_loss: 0.0782 - val_accuracy: 0.9369\n",
      "Epoch 1182/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9584\n",
      "Epoch 1182: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0573 - accuracy: 0.9584 - val_loss: 0.0782 - val_accuracy: 0.9369\n",
      "Epoch 1183/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9584\n",
      "Epoch 1183: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0572 - accuracy: 0.9584 - val_loss: 0.0781 - val_accuracy: 0.9369\n",
      "Epoch 1184/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9584\n",
      "Epoch 1184: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0572 - accuracy: 0.9584 - val_loss: 0.0781 - val_accuracy: 0.9369\n",
      "Epoch 1185/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9584\n",
      "Epoch 1185: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0571 - accuracy: 0.9584 - val_loss: 0.0780 - val_accuracy: 0.9369\n",
      "Epoch 1186/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9584\n",
      "Epoch 1186: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0571 - accuracy: 0.9584 - val_loss: 0.0780 - val_accuracy: 0.9369\n",
      "Epoch 1187/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9584\n",
      "Epoch 1187: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0570 - accuracy: 0.9584 - val_loss: 0.0779 - val_accuracy: 0.9369\n",
      "Epoch 1188/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9584\n",
      "Epoch 1188: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0570 - accuracy: 0.9584 - val_loss: 0.0779 - val_accuracy: 0.9369\n",
      "Epoch 1189/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9584\n",
      "Epoch 1189: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0570 - accuracy: 0.9584 - val_loss: 0.0778 - val_accuracy: 0.9369\n",
      "Epoch 1190/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9584\n",
      "Epoch 1190: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0569 - accuracy: 0.9584 - val_loss: 0.0778 - val_accuracy: 0.9369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1191/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9584\n",
      "Epoch 1191: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0569 - accuracy: 0.9584 - val_loss: 0.0777 - val_accuracy: 0.9369\n",
      "Epoch 1192/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9584\n",
      "Epoch 1192: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0568 - accuracy: 0.9584 - val_loss: 0.0776 - val_accuracy: 0.9369\n",
      "Epoch 1193/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9584\n",
      "Epoch 1193: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0568 - accuracy: 0.9584 - val_loss: 0.0776 - val_accuracy: 0.9369\n",
      "Epoch 1194/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9584\n",
      "Epoch 1194: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0567 - accuracy: 0.9584 - val_loss: 0.0775 - val_accuracy: 0.9369\n",
      "Epoch 1195/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9584\n",
      "Epoch 1195: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0567 - accuracy: 0.9584 - val_loss: 0.0775 - val_accuracy: 0.9369\n",
      "Epoch 1196/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9584\n",
      "Epoch 1196: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0567 - accuracy: 0.9584 - val_loss: 0.0774 - val_accuracy: 0.9369\n",
      "Epoch 1197/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9593\n",
      "Epoch 1197: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0566 - accuracy: 0.9593 - val_loss: 0.0774 - val_accuracy: 0.9369\n",
      "Epoch 1198/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9593\n",
      "Epoch 1198: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0566 - accuracy: 0.9593 - val_loss: 0.0773 - val_accuracy: 0.9369\n",
      "Epoch 1199/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9593\n",
      "Epoch 1199: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0565 - accuracy: 0.9593 - val_loss: 0.0773 - val_accuracy: 0.9369\n",
      "Epoch 1200/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9593\n",
      "Epoch 1200: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0565 - accuracy: 0.9593 - val_loss: 0.0772 - val_accuracy: 0.9369\n",
      "Epoch 1201/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9593\n",
      "Epoch 1201: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0565 - accuracy: 0.9593 - val_loss: 0.0772 - val_accuracy: 0.9369\n",
      "Epoch 1202/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9593\n",
      "Epoch 1202: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0564 - accuracy: 0.9593 - val_loss: 0.0771 - val_accuracy: 0.9369\n",
      "Epoch 1203/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9593\n",
      "Epoch 1203: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0564 - accuracy: 0.9593 - val_loss: 0.0771 - val_accuracy: 0.9369\n",
      "Epoch 1204/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9593\n",
      "Epoch 1204: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0563 - accuracy: 0.9593 - val_loss: 0.0770 - val_accuracy: 0.9369\n",
      "Epoch 1205/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9593\n",
      "Epoch 1205: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0563 - accuracy: 0.9593 - val_loss: 0.0770 - val_accuracy: 0.9369\n",
      "Epoch 1206/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9593\n",
      "Epoch 1206: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0563 - accuracy: 0.9593 - val_loss: 0.0769 - val_accuracy: 0.9369\n",
      "Epoch 1207/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9593\n",
      "Epoch 1207: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0562 - accuracy: 0.9593 - val_loss: 0.0769 - val_accuracy: 0.9369\n",
      "Epoch 1208/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9593\n",
      "Epoch 1208: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0562 - accuracy: 0.9593 - val_loss: 0.0768 - val_accuracy: 0.9369\n",
      "Epoch 1209/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9593\n",
      "Epoch 1209: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0561 - accuracy: 0.9593 - val_loss: 0.0768 - val_accuracy: 0.9369\n",
      "Epoch 1210/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9593\n",
      "Epoch 1210: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0561 - accuracy: 0.9593 - val_loss: 0.0767 - val_accuracy: 0.9369\n",
      "Epoch 1211/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9593\n",
      "Epoch 1211: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0561 - accuracy: 0.9593 - val_loss: 0.0767 - val_accuracy: 0.9369\n",
      "Epoch 1212/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9593\n",
      "Epoch 1212: val_accuracy did not improve from 0.93694\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0560 - accuracy: 0.9593 - val_loss: 0.0766 - val_accuracy: 0.9369\n",
      "Epoch 1213/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9593\n",
      "Epoch 1213: val_accuracy improved from 0.93694 to 0.93919, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0560 - accuracy: 0.9593 - val_loss: 0.0766 - val_accuracy: 0.9392\n",
      "Epoch 1214/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9593\n",
      "Epoch 1214: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0559 - accuracy: 0.9593 - val_loss: 0.0765 - val_accuracy: 0.9392\n",
      "Epoch 1215/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9593\n",
      "Epoch 1215: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0559 - accuracy: 0.9593 - val_loss: 0.0765 - val_accuracy: 0.9392\n",
      "Epoch 1216/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9593\n",
      "Epoch 1216: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0558 - accuracy: 0.9593 - val_loss: 0.0764 - val_accuracy: 0.9392\n",
      "Epoch 1217/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9593\n",
      "Epoch 1217: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0558 - accuracy: 0.9593 - val_loss: 0.0764 - val_accuracy: 0.9392\n",
      "Epoch 1218/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9593\n",
      "Epoch 1218: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0558 - accuracy: 0.9593 - val_loss: 0.0763 - val_accuracy: 0.9392\n",
      "Epoch 1219/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9593\n",
      "Epoch 1219: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0557 - accuracy: 0.9593 - val_loss: 0.0763 - val_accuracy: 0.9392\n",
      "Epoch 1220/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9593\n",
      "Epoch 1220: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0557 - accuracy: 0.9593 - val_loss: 0.0762 - val_accuracy: 0.9392\n",
      "Epoch 1221/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9593\n",
      "Epoch 1221: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0556 - accuracy: 0.9593 - val_loss: 0.0762 - val_accuracy: 0.9392\n",
      "Epoch 1222/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9593\n",
      "Epoch 1222: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0556 - accuracy: 0.9593 - val_loss: 0.0761 - val_accuracy: 0.9392\n",
      "Epoch 1223/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9593\n",
      "Epoch 1223: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0556 - accuracy: 0.9593 - val_loss: 0.0761 - val_accuracy: 0.9392\n",
      "Epoch 1224/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9593\n",
      "Epoch 1224: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0555 - accuracy: 0.9593 - val_loss: 0.0760 - val_accuracy: 0.9392\n",
      "Epoch 1225/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9593\n",
      "Epoch 1225: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0555 - accuracy: 0.9593 - val_loss: 0.0760 - val_accuracy: 0.9392\n",
      "Epoch 1226/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9593\n",
      "Epoch 1226: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0554 - accuracy: 0.9593 - val_loss: 0.0759 - val_accuracy: 0.9392\n",
      "Epoch 1227/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9593\n",
      "Epoch 1227: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0554 - accuracy: 0.9593 - val_loss: 0.0759 - val_accuracy: 0.9392\n",
      "Epoch 1228/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9593\n",
      "Epoch 1228: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0554 - accuracy: 0.9593 - val_loss: 0.0758 - val_accuracy: 0.9392\n",
      "Epoch 1229/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9593\n",
      "Epoch 1229: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0553 - accuracy: 0.9593 - val_loss: 0.0758 - val_accuracy: 0.9392\n",
      "Epoch 1230/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9593\n",
      "Epoch 1230: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0553 - accuracy: 0.9593 - val_loss: 0.0757 - val_accuracy: 0.9392\n",
      "Epoch 1231/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9593\n",
      "Epoch 1231: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0552 - accuracy: 0.9593 - val_loss: 0.0757 - val_accuracy: 0.9392\n",
      "Epoch 1232/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9603\n",
      "Epoch 1232: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0552 - accuracy: 0.9603 - val_loss: 0.0756 - val_accuracy: 0.9392\n",
      "Epoch 1233/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9603\n",
      "Epoch 1233: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0552 - accuracy: 0.9603 - val_loss: 0.0756 - val_accuracy: 0.9392\n",
      "Epoch 1234/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9603\n",
      "Epoch 1234: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0551 - accuracy: 0.9603 - val_loss: 0.0755 - val_accuracy: 0.9392\n",
      "Epoch 1235/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9603\n",
      "Epoch 1235: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0551 - accuracy: 0.9603 - val_loss: 0.0755 - val_accuracy: 0.9392\n",
      "Epoch 1236/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9603\n",
      "Epoch 1236: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0551 - accuracy: 0.9603 - val_loss: 0.0754 - val_accuracy: 0.9392\n",
      "Epoch 1237/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9603\n",
      "Epoch 1237: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0550 - accuracy: 0.9603 - val_loss: 0.0754 - val_accuracy: 0.9392\n",
      "Epoch 1238/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9603\n",
      "Epoch 1238: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0550 - accuracy: 0.9603 - val_loss: 0.0753 - val_accuracy: 0.9392\n",
      "Epoch 1239/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9603\n",
      "Epoch 1239: val_accuracy did not improve from 0.93919\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0549 - accuracy: 0.9603 - val_loss: 0.0753 - val_accuracy: 0.9392\n",
      "Epoch 1240/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9603\n",
      "Epoch 1240: val_accuracy improved from 0.93919 to 0.94144, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0549 - accuracy: 0.9603 - val_loss: 0.0752 - val_accuracy: 0.9414\n",
      "Epoch 1241/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9603\n",
      "Epoch 1241: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0549 - accuracy: 0.9603 - val_loss: 0.0752 - val_accuracy: 0.9414\n",
      "Epoch 1242/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9603\n",
      "Epoch 1242: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0548 - accuracy: 0.9603 - val_loss: 0.0751 - val_accuracy: 0.9414\n",
      "Epoch 1243/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9603\n",
      "Epoch 1243: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0548 - accuracy: 0.9603 - val_loss: 0.0751 - val_accuracy: 0.9414\n",
      "Epoch 1244/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9603\n",
      "Epoch 1244: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0547 - accuracy: 0.9603 - val_loss: 0.0750 - val_accuracy: 0.9414\n",
      "Epoch 1245/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9603\n",
      "Epoch 1245: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0547 - accuracy: 0.9603 - val_loss: 0.0750 - val_accuracy: 0.9414\n",
      "Epoch 1246/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9603\n",
      "Epoch 1246: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0547 - accuracy: 0.9603 - val_loss: 0.0749 - val_accuracy: 0.9414\n",
      "Epoch 1247/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9603\n",
      "Epoch 1247: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0546 - accuracy: 0.9603 - val_loss: 0.0749 - val_accuracy: 0.9414\n",
      "Epoch 1248/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9613\n",
      "Epoch 1248: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0546 - accuracy: 0.9613 - val_loss: 0.0749 - val_accuracy: 0.9414\n",
      "Epoch 1249/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9613\n",
      "Epoch 1249: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0545 - accuracy: 0.9613 - val_loss: 0.0748 - val_accuracy: 0.9414\n",
      "Epoch 1250/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9613\n",
      "Epoch 1250: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0545 - accuracy: 0.9613 - val_loss: 0.0748 - val_accuracy: 0.9414\n",
      "Epoch 1251/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9613\n",
      "Epoch 1251: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0545 - accuracy: 0.9613 - val_loss: 0.0747 - val_accuracy: 0.9414\n",
      "Epoch 1252/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9613\n",
      "Epoch 1252: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0544 - accuracy: 0.9613 - val_loss: 0.0747 - val_accuracy: 0.9414\n",
      "Epoch 1253/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9613\n",
      "Epoch 1253: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0544 - accuracy: 0.9613 - val_loss: 0.0746 - val_accuracy: 0.9414\n",
      "Epoch 1254/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9613\n",
      "Epoch 1254: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0544 - accuracy: 0.9613 - val_loss: 0.0746 - val_accuracy: 0.9414\n",
      "Epoch 1255/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9613\n",
      "Epoch 1255: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0543 - accuracy: 0.9613 - val_loss: 0.0745 - val_accuracy: 0.9414\n",
      "Epoch 1256/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9613\n",
      "Epoch 1256: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0543 - accuracy: 0.9613 - val_loss: 0.0745 - val_accuracy: 0.9414\n",
      "Epoch 1257/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9613\n",
      "Epoch 1257: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0542 - accuracy: 0.9613 - val_loss: 0.0744 - val_accuracy: 0.9414\n",
      "Epoch 1258/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9613\n",
      "Epoch 1258: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0542 - accuracy: 0.9613 - val_loss: 0.0744 - val_accuracy: 0.9414\n",
      "Epoch 1259/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9613\n",
      "Epoch 1259: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0542 - accuracy: 0.9613 - val_loss: 0.0743 - val_accuracy: 0.9414\n",
      "Epoch 1260/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9613\n",
      "Epoch 1260: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0541 - accuracy: 0.9613 - val_loss: 0.0743 - val_accuracy: 0.9414\n",
      "Epoch 1261/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9613\n",
      "Epoch 1261: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0541 - accuracy: 0.9613 - val_loss: 0.0742 - val_accuracy: 0.9414\n",
      "Epoch 1262/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9613\n",
      "Epoch 1262: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0540 - accuracy: 0.9613 - val_loss: 0.0742 - val_accuracy: 0.9414\n",
      "Epoch 1263/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9613\n",
      "Epoch 1263: val_accuracy did not improve from 0.94144\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0540 - accuracy: 0.9613 - val_loss: 0.0741 - val_accuracy: 0.9414\n",
      "Epoch 1264/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9613\n",
      "Epoch 1264: val_accuracy improved from 0.94144 to 0.94369, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0540 - accuracy: 0.9613 - val_loss: 0.0741 - val_accuracy: 0.9437\n",
      "Epoch 1265/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9613\n",
      "Epoch 1265: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0539 - accuracy: 0.9613 - val_loss: 0.0740 - val_accuracy: 0.9437\n",
      "Epoch 1266/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9613\n",
      "Epoch 1266: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0539 - accuracy: 0.9613 - val_loss: 0.0740 - val_accuracy: 0.9437\n",
      "Epoch 1267/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9613\n",
      "Epoch 1267: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0539 - accuracy: 0.9613 - val_loss: 0.0740 - val_accuracy: 0.9437\n",
      "Epoch 1268/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9613\n",
      "Epoch 1268: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0538 - accuracy: 0.9613 - val_loss: 0.0739 - val_accuracy: 0.9437\n",
      "Epoch 1269/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9613\n",
      "Epoch 1269: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0538 - accuracy: 0.9613 - val_loss: 0.0739 - val_accuracy: 0.9437\n",
      "Epoch 1270/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9613\n",
      "Epoch 1270: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0537 - accuracy: 0.9613 - val_loss: 0.0738 - val_accuracy: 0.9437\n",
      "Epoch 1271/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9613\n",
      "Epoch 1271: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0537 - accuracy: 0.9613 - val_loss: 0.0738 - val_accuracy: 0.9437\n",
      "Epoch 1272/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9613\n",
      "Epoch 1272: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0537 - accuracy: 0.9613 - val_loss: 0.0737 - val_accuracy: 0.9437\n",
      "Epoch 1273/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9613\n",
      "Epoch 1273: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0536 - accuracy: 0.9613 - val_loss: 0.0737 - val_accuracy: 0.9437\n",
      "Epoch 1274/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9613\n",
      "Epoch 1274: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0536 - accuracy: 0.9613 - val_loss: 0.0736 - val_accuracy: 0.9437\n",
      "Epoch 1275/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9613\n",
      "Epoch 1275: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0536 - accuracy: 0.9613 - val_loss: 0.0736 - val_accuracy: 0.9437\n",
      "Epoch 1276/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9613\n",
      "Epoch 1276: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0535 - accuracy: 0.9613 - val_loss: 0.0735 - val_accuracy: 0.9437\n",
      "Epoch 1277/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9613\n",
      "Epoch 1277: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0535 - accuracy: 0.9613 - val_loss: 0.0735 - val_accuracy: 0.9437\n",
      "Epoch 1278/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9613\n",
      "Epoch 1278: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0534 - accuracy: 0.9613 - val_loss: 0.0734 - val_accuracy: 0.9437\n",
      "Epoch 1279/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9613\n",
      "Epoch 1279: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0534 - accuracy: 0.9613 - val_loss: 0.0734 - val_accuracy: 0.9437\n",
      "Epoch 1280/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9613\n",
      "Epoch 1280: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0534 - accuracy: 0.9613 - val_loss: 0.0733 - val_accuracy: 0.9437\n",
      "Epoch 1281/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9613\n",
      "Epoch 1281: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0533 - accuracy: 0.9613 - val_loss: 0.0733 - val_accuracy: 0.9437\n",
      "Epoch 1282/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9613\n",
      "Epoch 1282: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0533 - accuracy: 0.9613 - val_loss: 0.0733 - val_accuracy: 0.9437\n",
      "Epoch 1283/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9613\n",
      "Epoch 1283: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0533 - accuracy: 0.9613 - val_loss: 0.0732 - val_accuracy: 0.9437\n",
      "Epoch 1284/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9613\n",
      "Epoch 1284: val_accuracy did not improve from 0.94369\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0532 - accuracy: 0.9613 - val_loss: 0.0732 - val_accuracy: 0.9437\n",
      "Epoch 1285/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9613\n",
      "Epoch 1285: val_accuracy improved from 0.94369 to 0.94595, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0532 - accuracy: 0.9613 - val_loss: 0.0731 - val_accuracy: 0.9459\n",
      "Epoch 1286/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9613\n",
      "Epoch 1286: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0531 - accuracy: 0.9613 - val_loss: 0.0731 - val_accuracy: 0.9459\n",
      "Epoch 1287/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9613\n",
      "Epoch 1287: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0531 - accuracy: 0.9613 - val_loss: 0.0730 - val_accuracy: 0.9459\n",
      "Epoch 1288/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9613\n",
      "Epoch 1288: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0531 - accuracy: 0.9613 - val_loss: 0.0730 - val_accuracy: 0.9459\n",
      "Epoch 1289/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9613\n",
      "Epoch 1289: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0530 - accuracy: 0.9613 - val_loss: 0.0729 - val_accuracy: 0.9459\n",
      "Epoch 1290/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9613\n",
      "Epoch 1290: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0530 - accuracy: 0.9613 - val_loss: 0.0729 - val_accuracy: 0.9459\n",
      "Epoch 1291/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9613\n",
      "Epoch 1291: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0530 - accuracy: 0.9613 - val_loss: 0.0728 - val_accuracy: 0.9459\n",
      "Epoch 1292/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9613\n",
      "Epoch 1292: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0529 - accuracy: 0.9613 - val_loss: 0.0728 - val_accuracy: 0.9459\n",
      "Epoch 1293/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9613\n",
      "Epoch 1293: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0529 - accuracy: 0.9613 - val_loss: 0.0728 - val_accuracy: 0.9459\n",
      "Epoch 1294/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9613\n",
      "Epoch 1294: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0529 - accuracy: 0.9613 - val_loss: 0.0727 - val_accuracy: 0.9459\n",
      "Epoch 1295/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9613\n",
      "Epoch 1295: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0528 - accuracy: 0.9613 - val_loss: 0.0727 - val_accuracy: 0.9459\n",
      "Epoch 1296/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9613\n",
      "Epoch 1296: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0528 - accuracy: 0.9613 - val_loss: 0.0726 - val_accuracy: 0.9459\n",
      "Epoch 1297/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9613\n",
      "Epoch 1297: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0527 - accuracy: 0.9613 - val_loss: 0.0726 - val_accuracy: 0.9459\n",
      "Epoch 1298/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9613\n",
      "Epoch 1298: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0527 - accuracy: 0.9613 - val_loss: 0.0725 - val_accuracy: 0.9459\n",
      "Epoch 1299/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9613\n",
      "Epoch 1299: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0527 - accuracy: 0.9613 - val_loss: 0.0725 - val_accuracy: 0.9459\n",
      "Epoch 1300/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9613\n",
      "Epoch 1300: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0526 - accuracy: 0.9613 - val_loss: 0.0724 - val_accuracy: 0.9459\n",
      "Epoch 1301/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9613\n",
      "Epoch 1301: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0526 - accuracy: 0.9613 - val_loss: 0.0724 - val_accuracy: 0.9459\n",
      "Epoch 1302/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9613\n",
      "Epoch 1302: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0526 - accuracy: 0.9613 - val_loss: 0.0723 - val_accuracy: 0.9459\n",
      "Epoch 1303/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9613\n",
      "Epoch 1303: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0525 - accuracy: 0.9613 - val_loss: 0.0723 - val_accuracy: 0.9459\n",
      "Epoch 1304/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9613\n",
      "Epoch 1304: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0525 - accuracy: 0.9613 - val_loss: 0.0723 - val_accuracy: 0.9459\n",
      "Epoch 1305/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9613\n",
      "Epoch 1305: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0525 - accuracy: 0.9613 - val_loss: 0.0722 - val_accuracy: 0.9459\n",
      "Epoch 1306/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9613\n",
      "Epoch 1306: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0524 - accuracy: 0.9613 - val_loss: 0.0722 - val_accuracy: 0.9459\n",
      "Epoch 1307/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9613\n",
      "Epoch 1307: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0524 - accuracy: 0.9613 - val_loss: 0.0721 - val_accuracy: 0.9459\n",
      "Epoch 1308/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9613\n",
      "Epoch 1308: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0523 - accuracy: 0.9613 - val_loss: 0.0721 - val_accuracy: 0.9459\n",
      "Epoch 1309/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9613\n",
      "Epoch 1309: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0523 - accuracy: 0.9613 - val_loss: 0.0720 - val_accuracy: 0.9459\n",
      "Epoch 1310/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9613\n",
      "Epoch 1310: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0523 - accuracy: 0.9613 - val_loss: 0.0720 - val_accuracy: 0.9459\n",
      "Epoch 1311/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9613\n",
      "Epoch 1311: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0522 - accuracy: 0.9613 - val_loss: 0.0719 - val_accuracy: 0.9459\n",
      "Epoch 1312/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9613\n",
      "Epoch 1312: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0522 - accuracy: 0.9613 - val_loss: 0.0719 - val_accuracy: 0.9459\n",
      "Epoch 1313/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9613\n",
      "Epoch 1313: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0522 - accuracy: 0.9613 - val_loss: 0.0719 - val_accuracy: 0.9459\n",
      "Epoch 1314/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9613\n",
      "Epoch 1314: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0521 - accuracy: 0.9613 - val_loss: 0.0718 - val_accuracy: 0.9459\n",
      "Epoch 1315/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9613\n",
      "Epoch 1315: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0521 - accuracy: 0.9613 - val_loss: 0.0718 - val_accuracy: 0.9459\n",
      "Epoch 1316/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9632\n",
      "Epoch 1316: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0521 - accuracy: 0.9632 - val_loss: 0.0717 - val_accuracy: 0.9459\n",
      "Epoch 1317/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9632\n",
      "Epoch 1317: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0520 - accuracy: 0.9632 - val_loss: 0.0717 - val_accuracy: 0.9459\n",
      "Epoch 1318/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9632\n",
      "Epoch 1318: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0520 - accuracy: 0.9632 - val_loss: 0.0716 - val_accuracy: 0.9459\n",
      "Epoch 1319/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9642\n",
      "Epoch 1319: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0520 - accuracy: 0.9642 - val_loss: 0.0716 - val_accuracy: 0.9459\n",
      "Epoch 1320/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9642\n",
      "Epoch 1320: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0519 - accuracy: 0.9642 - val_loss: 0.0715 - val_accuracy: 0.9459\n",
      "Epoch 1321/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9642\n",
      "Epoch 1321: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0519 - accuracy: 0.9642 - val_loss: 0.0715 - val_accuracy: 0.9459\n",
      "Epoch 1322/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9642\n",
      "Epoch 1322: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0518 - accuracy: 0.9642 - val_loss: 0.0715 - val_accuracy: 0.9459\n",
      "Epoch 1323/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9642\n",
      "Epoch 1323: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0518 - accuracy: 0.9642 - val_loss: 0.0714 - val_accuracy: 0.9459\n",
      "Epoch 1324/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9642\n",
      "Epoch 1324: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0518 - accuracy: 0.9642 - val_loss: 0.0714 - val_accuracy: 0.9459\n",
      "Epoch 1325/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9652\n",
      "Epoch 1325: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0517 - accuracy: 0.9652 - val_loss: 0.0713 - val_accuracy: 0.9459\n",
      "Epoch 1326/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9652\n",
      "Epoch 1326: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0517 - accuracy: 0.9652 - val_loss: 0.0713 - val_accuracy: 0.9459\n",
      "Epoch 1327/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9652\n",
      "Epoch 1327: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0517 - accuracy: 0.9652 - val_loss: 0.0712 - val_accuracy: 0.9459\n",
      "Epoch 1328/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9652\n",
      "Epoch 1328: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0516 - accuracy: 0.9652 - val_loss: 0.0712 - val_accuracy: 0.9459\n",
      "Epoch 1329/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9652\n",
      "Epoch 1329: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0516 - accuracy: 0.9652 - val_loss: 0.0712 - val_accuracy: 0.9459\n",
      "Epoch 1330/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9652\n",
      "Epoch 1330: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0516 - accuracy: 0.9652 - val_loss: 0.0711 - val_accuracy: 0.9459\n",
      "Epoch 1331/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9652\n",
      "Epoch 1331: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0515 - accuracy: 0.9652 - val_loss: 0.0711 - val_accuracy: 0.9459\n",
      "Epoch 1332/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9652\n",
      "Epoch 1332: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0515 - accuracy: 0.9652 - val_loss: 0.0710 - val_accuracy: 0.9459\n",
      "Epoch 1333/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9652\n",
      "Epoch 1333: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0515 - accuracy: 0.9652 - val_loss: 0.0710 - val_accuracy: 0.9459\n",
      "Epoch 1334/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9652\n",
      "Epoch 1334: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0514 - accuracy: 0.9652 - val_loss: 0.0709 - val_accuracy: 0.9459\n",
      "Epoch 1335/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9652\n",
      "Epoch 1335: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0514 - accuracy: 0.9652 - val_loss: 0.0709 - val_accuracy: 0.9459\n",
      "Epoch 1336/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9652\n",
      "Epoch 1336: val_accuracy did not improve from 0.94595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0514 - accuracy: 0.9652 - val_loss: 0.0708 - val_accuracy: 0.9459\n",
      "Epoch 1337/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9652\n",
      "Epoch 1337: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0513 - accuracy: 0.9652 - val_loss: 0.0708 - val_accuracy: 0.9459\n",
      "Epoch 1338/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9652\n",
      "Epoch 1338: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0513 - accuracy: 0.9652 - val_loss: 0.0708 - val_accuracy: 0.9459\n",
      "Epoch 1339/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9652\n",
      "Epoch 1339: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0512 - accuracy: 0.9652 - val_loss: 0.0707 - val_accuracy: 0.9459\n",
      "Epoch 1340/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9652\n",
      "Epoch 1340: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0512 - accuracy: 0.9652 - val_loss: 0.0707 - val_accuracy: 0.9459\n",
      "Epoch 1341/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9652\n",
      "Epoch 1341: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0512 - accuracy: 0.9652 - val_loss: 0.0706 - val_accuracy: 0.9459\n",
      "Epoch 1342/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9652\n",
      "Epoch 1342: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0511 - accuracy: 0.9652 - val_loss: 0.0706 - val_accuracy: 0.9459\n",
      "Epoch 1343/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9652\n",
      "Epoch 1343: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0511 - accuracy: 0.9652 - val_loss: 0.0705 - val_accuracy: 0.9459\n",
      "Epoch 1344/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9652\n",
      "Epoch 1344: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0511 - accuracy: 0.9652 - val_loss: 0.0705 - val_accuracy: 0.9459\n",
      "Epoch 1345/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9652\n",
      "Epoch 1345: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0510 - accuracy: 0.9652 - val_loss: 0.0705 - val_accuracy: 0.9459\n",
      "Epoch 1346/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9652\n",
      "Epoch 1346: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0510 - accuracy: 0.9652 - val_loss: 0.0704 - val_accuracy: 0.9459\n",
      "Epoch 1347/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9652\n",
      "Epoch 1347: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0510 - accuracy: 0.9652 - val_loss: 0.0704 - val_accuracy: 0.9459\n",
      "Epoch 1348/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9652\n",
      "Epoch 1348: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0509 - accuracy: 0.9652 - val_loss: 0.0703 - val_accuracy: 0.9459\n",
      "Epoch 1349/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9652\n",
      "Epoch 1349: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0509 - accuracy: 0.9652 - val_loss: 0.0703 - val_accuracy: 0.9459\n",
      "Epoch 1350/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9652\n",
      "Epoch 1350: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0509 - accuracy: 0.9652 - val_loss: 0.0703 - val_accuracy: 0.9459\n",
      "Epoch 1351/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9652\n",
      "Epoch 1351: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0508 - accuracy: 0.9652 - val_loss: 0.0702 - val_accuracy: 0.9459\n",
      "Epoch 1352/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9652\n",
      "Epoch 1352: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0508 - accuracy: 0.9652 - val_loss: 0.0702 - val_accuracy: 0.9459\n",
      "Epoch 1353/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9652\n",
      "Epoch 1353: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0508 - accuracy: 0.9652 - val_loss: 0.0701 - val_accuracy: 0.9459\n",
      "Epoch 1354/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9652\n",
      "Epoch 1354: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0507 - accuracy: 0.9652 - val_loss: 0.0701 - val_accuracy: 0.9459\n",
      "Epoch 1355/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9652\n",
      "Epoch 1355: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0507 - accuracy: 0.9652 - val_loss: 0.0700 - val_accuracy: 0.9459\n",
      "Epoch 1356/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9652\n",
      "Epoch 1356: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0507 - accuracy: 0.9652 - val_loss: 0.0700 - val_accuracy: 0.9459\n",
      "Epoch 1357/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9652\n",
      "Epoch 1357: val_accuracy did not improve from 0.94595\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0506 - accuracy: 0.9652 - val_loss: 0.0700 - val_accuracy: 0.9459\n",
      "Epoch 1358/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9652\n",
      "Epoch 1358: val_accuracy improved from 0.94595 to 0.94820, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0506 - accuracy: 0.9652 - val_loss: 0.0699 - val_accuracy: 0.9482\n",
      "Epoch 1359/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9652\n",
      "Epoch 1359: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0506 - accuracy: 0.9652 - val_loss: 0.0699 - val_accuracy: 0.9482\n",
      "Epoch 1360/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9652\n",
      "Epoch 1360: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0505 - accuracy: 0.9652 - val_loss: 0.0698 - val_accuracy: 0.9482\n",
      "Epoch 1361/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9652\n",
      "Epoch 1361: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0505 - accuracy: 0.9652 - val_loss: 0.0698 - val_accuracy: 0.9482\n",
      "Epoch 1362/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9652\n",
      "Epoch 1362: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0505 - accuracy: 0.9652 - val_loss: 0.0697 - val_accuracy: 0.9482\n",
      "Epoch 1363/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9652\n",
      "Epoch 1363: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0504 - accuracy: 0.9652 - val_loss: 0.0697 - val_accuracy: 0.9482\n",
      "Epoch 1364/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9652\n",
      "Epoch 1364: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0504 - accuracy: 0.9652 - val_loss: 0.0697 - val_accuracy: 0.9482\n",
      "Epoch 1365/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9652\n",
      "Epoch 1365: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0504 - accuracy: 0.9652 - val_loss: 0.0696 - val_accuracy: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1366/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9652\n",
      "Epoch 1366: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0503 - accuracy: 0.9652 - val_loss: 0.0696 - val_accuracy: 0.9482\n",
      "Epoch 1367/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9652\n",
      "Epoch 1367: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0503 - accuracy: 0.9652 - val_loss: 0.0695 - val_accuracy: 0.9482\n",
      "Epoch 1368/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9652\n",
      "Epoch 1368: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0503 - accuracy: 0.9652 - val_loss: 0.0695 - val_accuracy: 0.9482\n",
      "Epoch 1369/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9652\n",
      "Epoch 1369: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0502 - accuracy: 0.9652 - val_loss: 0.0695 - val_accuracy: 0.9482\n",
      "Epoch 1370/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9652\n",
      "Epoch 1370: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0502 - accuracy: 0.9652 - val_loss: 0.0694 - val_accuracy: 0.9482\n",
      "Epoch 1371/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9652\n",
      "Epoch 1371: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0502 - accuracy: 0.9652 - val_loss: 0.0694 - val_accuracy: 0.9482\n",
      "Epoch 1372/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9652\n",
      "Epoch 1372: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0501 - accuracy: 0.9652 - val_loss: 0.0693 - val_accuracy: 0.9482\n",
      "Epoch 1373/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9652\n",
      "Epoch 1373: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0501 - accuracy: 0.9652 - val_loss: 0.0693 - val_accuracy: 0.9482\n",
      "Epoch 1374/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9652\n",
      "Epoch 1374: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0501 - accuracy: 0.9652 - val_loss: 0.0692 - val_accuracy: 0.9482\n",
      "Epoch 1375/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9652\n",
      "Epoch 1375: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0500 - accuracy: 0.9652 - val_loss: 0.0692 - val_accuracy: 0.9482\n",
      "Epoch 1376/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9652\n",
      "Epoch 1376: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0500 - accuracy: 0.9652 - val_loss: 0.0692 - val_accuracy: 0.9482\n",
      "Epoch 1377/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9652\n",
      "Epoch 1377: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0500 - accuracy: 0.9652 - val_loss: 0.0691 - val_accuracy: 0.9482\n",
      "Epoch 1378/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9652\n",
      "Epoch 1378: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0499 - accuracy: 0.9652 - val_loss: 0.0691 - val_accuracy: 0.9482\n",
      "Epoch 1379/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9652\n",
      "Epoch 1379: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0499 - accuracy: 0.9652 - val_loss: 0.0690 - val_accuracy: 0.9482\n",
      "Epoch 1380/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9652\n",
      "Epoch 1380: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0499 - accuracy: 0.9652 - val_loss: 0.0690 - val_accuracy: 0.9482\n",
      "Epoch 1381/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9652\n",
      "Epoch 1381: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0498 - accuracy: 0.9652 - val_loss: 0.0690 - val_accuracy: 0.9482\n",
      "Epoch 1382/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9652\n",
      "Epoch 1382: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0498 - accuracy: 0.9652 - val_loss: 0.0689 - val_accuracy: 0.9482\n",
      "Epoch 1383/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9652\n",
      "Epoch 1383: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0498 - accuracy: 0.9652 - val_loss: 0.0689 - val_accuracy: 0.9482\n",
      "Epoch 1384/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9652\n",
      "Epoch 1384: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0497 - accuracy: 0.9652 - val_loss: 0.0688 - val_accuracy: 0.9482\n",
      "Epoch 1385/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9652\n",
      "Epoch 1385: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0497 - accuracy: 0.9652 - val_loss: 0.0688 - val_accuracy: 0.9482\n",
      "Epoch 1386/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9652\n",
      "Epoch 1386: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0497 - accuracy: 0.9652 - val_loss: 0.0688 - val_accuracy: 0.9482\n",
      "Epoch 1387/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9652\n",
      "Epoch 1387: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0496 - accuracy: 0.9652 - val_loss: 0.0687 - val_accuracy: 0.9482\n",
      "Epoch 1388/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9652\n",
      "Epoch 1388: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0496 - accuracy: 0.9652 - val_loss: 0.0687 - val_accuracy: 0.9482\n",
      "Epoch 1389/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9652\n",
      "Epoch 1389: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0496 - accuracy: 0.9652 - val_loss: 0.0686 - val_accuracy: 0.9482\n",
      "Epoch 1390/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9652\n",
      "Epoch 1390: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0495 - accuracy: 0.9652 - val_loss: 0.0686 - val_accuracy: 0.9482\n",
      "Epoch 1391/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9652\n",
      "Epoch 1391: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0495 - accuracy: 0.9652 - val_loss: 0.0686 - val_accuracy: 0.9482\n",
      "Epoch 1392/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9652\n",
      "Epoch 1392: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0495 - accuracy: 0.9652 - val_loss: 0.0685 - val_accuracy: 0.9482\n",
      "Epoch 1393/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9652\n",
      "Epoch 1393: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0494 - accuracy: 0.9652 - val_loss: 0.0685 - val_accuracy: 0.9482\n",
      "Epoch 1394/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9661\n",
      "Epoch 1394: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0494 - accuracy: 0.9661 - val_loss: 0.0684 - val_accuracy: 0.9482\n",
      "Epoch 1395/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9661\n",
      "Epoch 1395: val_accuracy did not improve from 0.94820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0494 - accuracy: 0.9661 - val_loss: 0.0684 - val_accuracy: 0.9482\n",
      "Epoch 1396/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9661\n",
      "Epoch 1396: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0493 - accuracy: 0.9661 - val_loss: 0.0684 - val_accuracy: 0.9482\n",
      "Epoch 1397/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9661\n",
      "Epoch 1397: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0493 - accuracy: 0.9661 - val_loss: 0.0683 - val_accuracy: 0.9482\n",
      "Epoch 1398/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9661\n",
      "Epoch 1398: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0493 - accuracy: 0.9661 - val_loss: 0.0683 - val_accuracy: 0.9482\n",
      "Epoch 1399/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9661\n",
      "Epoch 1399: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0492 - accuracy: 0.9661 - val_loss: 0.0682 - val_accuracy: 0.9482\n",
      "Epoch 1400/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9661\n",
      "Epoch 1400: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0492 - accuracy: 0.9661 - val_loss: 0.0682 - val_accuracy: 0.9482\n",
      "Epoch 1401/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9661\n",
      "Epoch 1401: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0492 - accuracy: 0.9661 - val_loss: 0.0682 - val_accuracy: 0.9482\n",
      "Epoch 1402/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9661\n",
      "Epoch 1402: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0491 - accuracy: 0.9661 - val_loss: 0.0681 - val_accuracy: 0.9482\n",
      "Epoch 1403/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9661\n",
      "Epoch 1403: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0491 - accuracy: 0.9661 - val_loss: 0.0681 - val_accuracy: 0.9482\n",
      "Epoch 1404/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9661\n",
      "Epoch 1404: val_accuracy did not improve from 0.94820\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0491 - accuracy: 0.9661 - val_loss: 0.0680 - val_accuracy: 0.9482\n",
      "Epoch 1405/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9661\n",
      "Epoch 1405: val_accuracy improved from 0.94820 to 0.95045, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0490 - accuracy: 0.9661 - val_loss: 0.0680 - val_accuracy: 0.9505\n",
      "Epoch 1406/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9661\n",
      "Epoch 1406: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0490 - accuracy: 0.9661 - val_loss: 0.0680 - val_accuracy: 0.9505\n",
      "Epoch 1407/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9661\n",
      "Epoch 1407: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0490 - accuracy: 0.9661 - val_loss: 0.0679 - val_accuracy: 0.9505\n",
      "Epoch 1408/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9661\n",
      "Epoch 1408: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0490 - accuracy: 0.9661 - val_loss: 0.0679 - val_accuracy: 0.9505\n",
      "Epoch 1409/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9661\n",
      "Epoch 1409: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0489 - accuracy: 0.9661 - val_loss: 0.0678 - val_accuracy: 0.9505\n",
      "Epoch 1410/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9661\n",
      "Epoch 1410: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0489 - accuracy: 0.9661 - val_loss: 0.0678 - val_accuracy: 0.9505\n",
      "Epoch 1411/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9661\n",
      "Epoch 1411: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0489 - accuracy: 0.9661 - val_loss: 0.0678 - val_accuracy: 0.9505\n",
      "Epoch 1412/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9661\n",
      "Epoch 1412: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0488 - accuracy: 0.9661 - val_loss: 0.0677 - val_accuracy: 0.9505\n",
      "Epoch 1413/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9671\n",
      "Epoch 1413: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0488 - accuracy: 0.9671 - val_loss: 0.0677 - val_accuracy: 0.9505\n",
      "Epoch 1414/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9671\n",
      "Epoch 1414: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0488 - accuracy: 0.9671 - val_loss: 0.0676 - val_accuracy: 0.9505\n",
      "Epoch 1415/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9671\n",
      "Epoch 1415: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0487 - accuracy: 0.9671 - val_loss: 0.0676 - val_accuracy: 0.9505\n",
      "Epoch 1416/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9671\n",
      "Epoch 1416: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0487 - accuracy: 0.9671 - val_loss: 0.0676 - val_accuracy: 0.9505\n",
      "Epoch 1417/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9671\n",
      "Epoch 1417: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0487 - accuracy: 0.9671 - val_loss: 0.0675 - val_accuracy: 0.9505\n",
      "Epoch 1418/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9671\n",
      "Epoch 1418: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0486 - accuracy: 0.9671 - val_loss: 0.0675 - val_accuracy: 0.9505\n",
      "Epoch 1419/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9671\n",
      "Epoch 1419: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0486 - accuracy: 0.9671 - val_loss: 0.0674 - val_accuracy: 0.9505\n",
      "Epoch 1420/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9671\n",
      "Epoch 1420: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0486 - accuracy: 0.9671 - val_loss: 0.0674 - val_accuracy: 0.9505\n",
      "Epoch 1421/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9671\n",
      "Epoch 1421: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0485 - accuracy: 0.9671 - val_loss: 0.0674 - val_accuracy: 0.9505\n",
      "Epoch 1422/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9671\n",
      "Epoch 1422: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0485 - accuracy: 0.9671 - val_loss: 0.0673 - val_accuracy: 0.9505\n",
      "Epoch 1423/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9671\n",
      "Epoch 1423: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0485 - accuracy: 0.9671 - val_loss: 0.0673 - val_accuracy: 0.9505\n",
      "Epoch 1424/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9671\n",
      "Epoch 1424: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0484 - accuracy: 0.9671 - val_loss: 0.0673 - val_accuracy: 0.9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1425/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9671\n",
      "Epoch 1425: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0484 - accuracy: 0.9671 - val_loss: 0.0672 - val_accuracy: 0.9505\n",
      "Epoch 1426/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9671\n",
      "Epoch 1426: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0484 - accuracy: 0.9671 - val_loss: 0.0672 - val_accuracy: 0.9505\n",
      "Epoch 1427/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9681\n",
      "Epoch 1427: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0484 - accuracy: 0.9681 - val_loss: 0.0671 - val_accuracy: 0.9505\n",
      "Epoch 1428/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9690\n",
      "Epoch 1428: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0483 - accuracy: 0.9690 - val_loss: 0.0671 - val_accuracy: 0.9505\n",
      "Epoch 1429/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9690\n",
      "Epoch 1429: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0483 - accuracy: 0.9690 - val_loss: 0.0671 - val_accuracy: 0.9505\n",
      "Epoch 1430/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9690\n",
      "Epoch 1430: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0483 - accuracy: 0.9690 - val_loss: 0.0670 - val_accuracy: 0.9505\n",
      "Epoch 1431/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9690\n",
      "Epoch 1431: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0482 - accuracy: 0.9690 - val_loss: 0.0670 - val_accuracy: 0.9505\n",
      "Epoch 1432/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9690\n",
      "Epoch 1432: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0482 - accuracy: 0.9690 - val_loss: 0.0669 - val_accuracy: 0.9505\n",
      "Epoch 1433/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9690\n",
      "Epoch 1433: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0482 - accuracy: 0.9690 - val_loss: 0.0669 - val_accuracy: 0.9505\n",
      "Epoch 1434/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9690\n",
      "Epoch 1434: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0481 - accuracy: 0.9690 - val_loss: 0.0669 - val_accuracy: 0.9505\n",
      "Epoch 1435/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9690\n",
      "Epoch 1435: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0481 - accuracy: 0.9690 - val_loss: 0.0668 - val_accuracy: 0.9505\n",
      "Epoch 1436/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9690\n",
      "Epoch 1436: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0481 - accuracy: 0.9690 - val_loss: 0.0668 - val_accuracy: 0.9505\n",
      "Epoch 1437/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9690\n",
      "Epoch 1437: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0480 - accuracy: 0.9690 - val_loss: 0.0668 - val_accuracy: 0.9505\n",
      "Epoch 1438/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9690\n",
      "Epoch 1438: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0480 - accuracy: 0.9690 - val_loss: 0.0667 - val_accuracy: 0.9505\n",
      "Epoch 1439/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9690\n",
      "Epoch 1439: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0480 - accuracy: 0.9690 - val_loss: 0.0667 - val_accuracy: 0.9505\n",
      "Epoch 1440/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9690\n",
      "Epoch 1440: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0479 - accuracy: 0.9690 - val_loss: 0.0666 - val_accuracy: 0.9505\n",
      "Epoch 1441/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9690\n",
      "Epoch 1441: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0479 - accuracy: 0.9690 - val_loss: 0.0666 - val_accuracy: 0.9505\n",
      "Epoch 1442/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9690\n",
      "Epoch 1442: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0479 - accuracy: 0.9690 - val_loss: 0.0665 - val_accuracy: 0.9505\n",
      "Epoch 1443/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9690\n",
      "Epoch 1443: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0479 - accuracy: 0.9690 - val_loss: 0.0665 - val_accuracy: 0.9505\n",
      "Epoch 1444/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9690\n",
      "Epoch 1444: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0478 - accuracy: 0.9690 - val_loss: 0.0665 - val_accuracy: 0.9505\n",
      "Epoch 1445/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9690\n",
      "Epoch 1445: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0478 - accuracy: 0.9690 - val_loss: 0.0665 - val_accuracy: 0.9505\n",
      "Epoch 1446/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9690\n",
      "Epoch 1446: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0478 - accuracy: 0.9690 - val_loss: 0.0664 - val_accuracy: 0.9505\n",
      "Epoch 1447/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9690\n",
      "Epoch 1447: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0477 - accuracy: 0.9690 - val_loss: 0.0664 - val_accuracy: 0.9505\n",
      "Epoch 1448/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9690\n",
      "Epoch 1448: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0477 - accuracy: 0.9690 - val_loss: 0.0663 - val_accuracy: 0.9505\n",
      "Epoch 1449/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9690\n",
      "Epoch 1449: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0477 - accuracy: 0.9690 - val_loss: 0.0663 - val_accuracy: 0.9505\n",
      "Epoch 1450/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9690\n",
      "Epoch 1450: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0476 - accuracy: 0.9690 - val_loss: 0.0663 - val_accuracy: 0.9505\n",
      "Epoch 1451/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9690\n",
      "Epoch 1451: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0476 - accuracy: 0.9690 - val_loss: 0.0662 - val_accuracy: 0.9505\n",
      "Epoch 1452/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9690\n",
      "Epoch 1452: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0476 - accuracy: 0.9690 - val_loss: 0.0662 - val_accuracy: 0.9505\n",
      "Epoch 1453/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9690\n",
      "Epoch 1453: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0476 - accuracy: 0.9690 - val_loss: 0.0661 - val_accuracy: 0.9505\n",
      "Epoch 1454/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9690\n",
      "Epoch 1454: val_accuracy did not improve from 0.95045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0475 - accuracy: 0.9690 - val_loss: 0.0661 - val_accuracy: 0.9505\n",
      "Epoch 1455/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9690\n",
      "Epoch 1455: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0475 - accuracy: 0.9690 - val_loss: 0.0661 - val_accuracy: 0.9505\n",
      "Epoch 1456/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9690\n",
      "Epoch 1456: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0475 - accuracy: 0.9690 - val_loss: 0.0660 - val_accuracy: 0.9505\n",
      "Epoch 1457/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9690\n",
      "Epoch 1457: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0474 - accuracy: 0.9690 - val_loss: 0.0660 - val_accuracy: 0.9505\n",
      "Epoch 1458/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9690\n",
      "Epoch 1458: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0474 - accuracy: 0.9690 - val_loss: 0.0660 - val_accuracy: 0.9505\n",
      "Epoch 1459/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9690\n",
      "Epoch 1459: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0474 - accuracy: 0.9690 - val_loss: 0.0659 - val_accuracy: 0.9505\n",
      "Epoch 1460/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9690\n",
      "Epoch 1460: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0473 - accuracy: 0.9690 - val_loss: 0.0659 - val_accuracy: 0.9505\n",
      "Epoch 1461/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9690\n",
      "Epoch 1461: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0473 - accuracy: 0.9690 - val_loss: 0.0659 - val_accuracy: 0.9505\n",
      "Epoch 1462/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9690\n",
      "Epoch 1462: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0473 - accuracy: 0.9690 - val_loss: 0.0658 - val_accuracy: 0.9505\n",
      "Epoch 1463/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9690\n",
      "Epoch 1463: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0472 - accuracy: 0.9690 - val_loss: 0.0658 - val_accuracy: 0.9505\n",
      "Epoch 1464/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9690\n",
      "Epoch 1464: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0472 - accuracy: 0.9690 - val_loss: 0.0657 - val_accuracy: 0.9505\n",
      "Epoch 1465/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9690\n",
      "Epoch 1465: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0472 - accuracy: 0.9690 - val_loss: 0.0657 - val_accuracy: 0.9505\n",
      "Epoch 1466/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9690\n",
      "Epoch 1466: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0472 - accuracy: 0.9690 - val_loss: 0.0657 - val_accuracy: 0.9505\n",
      "Epoch 1467/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9690\n",
      "Epoch 1467: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0471 - accuracy: 0.9690 - val_loss: 0.0656 - val_accuracy: 0.9505\n",
      "Epoch 1468/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9690\n",
      "Epoch 1468: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0471 - accuracy: 0.9690 - val_loss: 0.0656 - val_accuracy: 0.9505\n",
      "Epoch 1469/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9690\n",
      "Epoch 1469: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0471 - accuracy: 0.9690 - val_loss: 0.0656 - val_accuracy: 0.9505\n",
      "Epoch 1470/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9690\n",
      "Epoch 1470: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0470 - accuracy: 0.9690 - val_loss: 0.0655 - val_accuracy: 0.9505\n",
      "Epoch 1471/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9690\n",
      "Epoch 1471: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0470 - accuracy: 0.9690 - val_loss: 0.0655 - val_accuracy: 0.9505\n",
      "Epoch 1472/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9690\n",
      "Epoch 1472: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0470 - accuracy: 0.9690 - val_loss: 0.0654 - val_accuracy: 0.9505\n",
      "Epoch 1473/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9690\n",
      "Epoch 1473: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0470 - accuracy: 0.9690 - val_loss: 0.0654 - val_accuracy: 0.9505\n",
      "Epoch 1474/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9690\n",
      "Epoch 1474: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0469 - accuracy: 0.9690 - val_loss: 0.0654 - val_accuracy: 0.9505\n",
      "Epoch 1475/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9700\n",
      "Epoch 1475: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0469 - accuracy: 0.9700 - val_loss: 0.0653 - val_accuracy: 0.9505\n",
      "Epoch 1476/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9700\n",
      "Epoch 1476: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0469 - accuracy: 0.9700 - val_loss: 0.0653 - val_accuracy: 0.9505\n",
      "Epoch 1477/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9700\n",
      "Epoch 1477: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0468 - accuracy: 0.9700 - val_loss: 0.0653 - val_accuracy: 0.9505\n",
      "Epoch 1478/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9700\n",
      "Epoch 1478: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0468 - accuracy: 0.9700 - val_loss: 0.0652 - val_accuracy: 0.9505\n",
      "Epoch 1479/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9700\n",
      "Epoch 1479: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0468 - accuracy: 0.9700 - val_loss: 0.0652 - val_accuracy: 0.9505\n",
      "Epoch 1480/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9700\n",
      "Epoch 1480: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0467 - accuracy: 0.9700 - val_loss: 0.0652 - val_accuracy: 0.9505\n",
      "Epoch 1481/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9700\n",
      "Epoch 1481: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0467 - accuracy: 0.9700 - val_loss: 0.0651 - val_accuracy: 0.9505\n",
      "Epoch 1482/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9700\n",
      "Epoch 1482: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0467 - accuracy: 0.9700 - val_loss: 0.0651 - val_accuracy: 0.9505\n",
      "Epoch 1483/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9700\n",
      "Epoch 1483: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0467 - accuracy: 0.9700 - val_loss: 0.0650 - val_accuracy: 0.9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1484/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9700\n",
      "Epoch 1484: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0466 - accuracy: 0.9700 - val_loss: 0.0650 - val_accuracy: 0.9505\n",
      "Epoch 1485/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9700\n",
      "Epoch 1485: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0466 - accuracy: 0.9700 - val_loss: 0.0650 - val_accuracy: 0.9505\n",
      "Epoch 1486/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9700\n",
      "Epoch 1486: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0466 - accuracy: 0.9700 - val_loss: 0.0649 - val_accuracy: 0.9505\n",
      "Epoch 1487/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9700\n",
      "Epoch 1487: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0465 - accuracy: 0.9700 - val_loss: 0.0649 - val_accuracy: 0.9505\n",
      "Epoch 1488/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9700\n",
      "Epoch 1488: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0465 - accuracy: 0.9700 - val_loss: 0.0649 - val_accuracy: 0.9505\n",
      "Epoch 1489/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9700\n",
      "Epoch 1489: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0465 - accuracy: 0.9700 - val_loss: 0.0648 - val_accuracy: 0.9505\n",
      "Epoch 1490/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9700\n",
      "Epoch 1490: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0465 - accuracy: 0.9700 - val_loss: 0.0648 - val_accuracy: 0.9505\n",
      "Epoch 1491/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9700\n",
      "Epoch 1491: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0464 - accuracy: 0.9700 - val_loss: 0.0648 - val_accuracy: 0.9505\n",
      "Epoch 1492/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9700\n",
      "Epoch 1492: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0464 - accuracy: 0.9700 - val_loss: 0.0647 - val_accuracy: 0.9505\n",
      "Epoch 1493/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9700\n",
      "Epoch 1493: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0464 - accuracy: 0.9700 - val_loss: 0.0647 - val_accuracy: 0.9505\n",
      "Epoch 1494/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9700\n",
      "Epoch 1494: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0463 - accuracy: 0.9700 - val_loss: 0.0647 - val_accuracy: 0.9505\n",
      "Epoch 1495/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9700\n",
      "Epoch 1495: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0463 - accuracy: 0.9700 - val_loss: 0.0646 - val_accuracy: 0.9505\n",
      "Epoch 1496/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9700\n",
      "Epoch 1496: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0463 - accuracy: 0.9700 - val_loss: 0.0646 - val_accuracy: 0.9505\n",
      "Epoch 1497/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9700\n",
      "Epoch 1497: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0462 - accuracy: 0.9700 - val_loss: 0.0645 - val_accuracy: 0.9505\n",
      "Epoch 1498/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9700\n",
      "Epoch 1498: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0462 - accuracy: 0.9700 - val_loss: 0.0645 - val_accuracy: 0.9505\n",
      "Epoch 1499/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9700\n",
      "Epoch 1499: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0462 - accuracy: 0.9700 - val_loss: 0.0645 - val_accuracy: 0.9505\n",
      "Epoch 1500/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9700\n",
      "Epoch 1500: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0462 - accuracy: 0.9700 - val_loss: 0.0644 - val_accuracy: 0.9505\n",
      "Epoch 1501/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9700\n",
      "Epoch 1501: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0461 - accuracy: 0.9700 - val_loss: 0.0644 - val_accuracy: 0.9505\n",
      "Epoch 1502/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9700\n",
      "Epoch 1502: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0461 - accuracy: 0.9700 - val_loss: 0.0644 - val_accuracy: 0.9505\n",
      "Epoch 1503/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9700\n",
      "Epoch 1503: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0461 - accuracy: 0.9700 - val_loss: 0.0643 - val_accuracy: 0.9505\n",
      "Epoch 1504/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9700\n",
      "Epoch 1504: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0460 - accuracy: 0.9700 - val_loss: 0.0643 - val_accuracy: 0.9505\n",
      "Epoch 1505/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9700\n",
      "Epoch 1505: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0460 - accuracy: 0.9700 - val_loss: 0.0643 - val_accuracy: 0.9505\n",
      "Epoch 1506/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9700\n",
      "Epoch 1506: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0460 - accuracy: 0.9700 - val_loss: 0.0642 - val_accuracy: 0.9505\n",
      "Epoch 1507/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9700\n",
      "Epoch 1507: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0460 - accuracy: 0.9700 - val_loss: 0.0642 - val_accuracy: 0.9505\n",
      "Epoch 1508/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9700\n",
      "Epoch 1508: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0459 - accuracy: 0.9700 - val_loss: 0.0642 - val_accuracy: 0.9505\n",
      "Epoch 1509/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9700\n",
      "Epoch 1509: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0459 - accuracy: 0.9700 - val_loss: 0.0641 - val_accuracy: 0.9505\n",
      "Epoch 1510/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9700\n",
      "Epoch 1510: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0459 - accuracy: 0.9700 - val_loss: 0.0641 - val_accuracy: 0.9505\n",
      "Epoch 1511/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9700\n",
      "Epoch 1511: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0458 - accuracy: 0.9700 - val_loss: 0.0640 - val_accuracy: 0.9505\n",
      "Epoch 1512/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9700\n",
      "Epoch 1512: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0458 - accuracy: 0.9700 - val_loss: 0.0640 - val_accuracy: 0.9505\n",
      "Epoch 1513/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9700\n",
      "Epoch 1513: val_accuracy did not improve from 0.95045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0458 - accuracy: 0.9700 - val_loss: 0.0640 - val_accuracy: 0.9482\n",
      "Epoch 1514/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9700\n",
      "Epoch 1514: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0458 - accuracy: 0.9700 - val_loss: 0.0639 - val_accuracy: 0.9482\n",
      "Epoch 1515/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9700\n",
      "Epoch 1515: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0457 - accuracy: 0.9700 - val_loss: 0.0639 - val_accuracy: 0.9482\n",
      "Epoch 1516/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9700\n",
      "Epoch 1516: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0457 - accuracy: 0.9700 - val_loss: 0.0639 - val_accuracy: 0.9482\n",
      "Epoch 1517/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9700\n",
      "Epoch 1517: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0457 - accuracy: 0.9700 - val_loss: 0.0638 - val_accuracy: 0.9482\n",
      "Epoch 1518/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9700\n",
      "Epoch 1518: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0456 - accuracy: 0.9700 - val_loss: 0.0638 - val_accuracy: 0.9482\n",
      "Epoch 1519/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9700\n",
      "Epoch 1519: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0456 - accuracy: 0.9700 - val_loss: 0.0638 - val_accuracy: 0.9482\n",
      "Epoch 1520/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9700\n",
      "Epoch 1520: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0456 - accuracy: 0.9700 - val_loss: 0.0637 - val_accuracy: 0.9482\n",
      "Epoch 1521/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9700\n",
      "Epoch 1521: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0456 - accuracy: 0.9700 - val_loss: 0.0637 - val_accuracy: 0.9482\n",
      "Epoch 1522/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9700\n",
      "Epoch 1522: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0455 - accuracy: 0.9700 - val_loss: 0.0637 - val_accuracy: 0.9482\n",
      "Epoch 1523/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9700\n",
      "Epoch 1523: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0455 - accuracy: 0.9700 - val_loss: 0.0636 - val_accuracy: 0.9482\n",
      "Epoch 1524/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9700\n",
      "Epoch 1524: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0455 - accuracy: 0.9700 - val_loss: 0.0636 - val_accuracy: 0.9482\n",
      "Epoch 1525/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9700\n",
      "Epoch 1525: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0454 - accuracy: 0.9700 - val_loss: 0.0636 - val_accuracy: 0.9482\n",
      "Epoch 1526/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9700\n",
      "Epoch 1526: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0454 - accuracy: 0.9700 - val_loss: 0.0635 - val_accuracy: 0.9482\n",
      "Epoch 1527/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9700\n",
      "Epoch 1527: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0454 - accuracy: 0.9700 - val_loss: 0.0635 - val_accuracy: 0.9482\n",
      "Epoch 1528/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9700\n",
      "Epoch 1528: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0454 - accuracy: 0.9700 - val_loss: 0.0635 - val_accuracy: 0.9482\n",
      "Epoch 1529/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9700\n",
      "Epoch 1529: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0453 - accuracy: 0.9700 - val_loss: 0.0634 - val_accuracy: 0.9482\n",
      "Epoch 1530/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9700\n",
      "Epoch 1530: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0453 - accuracy: 0.9700 - val_loss: 0.0634 - val_accuracy: 0.9482\n",
      "Epoch 1531/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9700\n",
      "Epoch 1531: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0453 - accuracy: 0.9700 - val_loss: 0.0634 - val_accuracy: 0.9482\n",
      "Epoch 1532/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9700\n",
      "Epoch 1532: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0453 - accuracy: 0.9700 - val_loss: 0.0633 - val_accuracy: 0.9482\n",
      "Epoch 1533/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9700\n",
      "Epoch 1533: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0452 - accuracy: 0.9700 - val_loss: 0.0633 - val_accuracy: 0.9482\n",
      "Epoch 1534/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9700\n",
      "Epoch 1534: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0452 - accuracy: 0.9700 - val_loss: 0.0633 - val_accuracy: 0.9482\n",
      "Epoch 1535/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9700\n",
      "Epoch 1535: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0452 - accuracy: 0.9700 - val_loss: 0.0632 - val_accuracy: 0.9482\n",
      "Epoch 1536/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9700\n",
      "Epoch 1536: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0451 - accuracy: 0.9700 - val_loss: 0.0632 - val_accuracy: 0.9482\n",
      "Epoch 1537/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9700\n",
      "Epoch 1537: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0451 - accuracy: 0.9700 - val_loss: 0.0632 - val_accuracy: 0.9482\n",
      "Epoch 1538/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9700\n",
      "Epoch 1538: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0451 - accuracy: 0.9700 - val_loss: 0.0631 - val_accuracy: 0.9482\n",
      "Epoch 1539/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9700\n",
      "Epoch 1539: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0451 - accuracy: 0.9700 - val_loss: 0.0631 - val_accuracy: 0.9482\n",
      "Epoch 1540/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9700\n",
      "Epoch 1540: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0450 - accuracy: 0.9700 - val_loss: 0.0631 - val_accuracy: 0.9482\n",
      "Epoch 1541/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9700\n",
      "Epoch 1541: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0450 - accuracy: 0.9700 - val_loss: 0.0630 - val_accuracy: 0.9482\n",
      "Epoch 1542/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9700\n",
      "Epoch 1542: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0450 - accuracy: 0.9700 - val_loss: 0.0630 - val_accuracy: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1543/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9700\n",
      "Epoch 1543: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0449 - accuracy: 0.9700 - val_loss: 0.0629 - val_accuracy: 0.9482\n",
      "Epoch 1544/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9700\n",
      "Epoch 1544: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0449 - accuracy: 0.9700 - val_loss: 0.0629 - val_accuracy: 0.9482\n",
      "Epoch 1545/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9700\n",
      "Epoch 1545: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0449 - accuracy: 0.9700 - val_loss: 0.0629 - val_accuracy: 0.9482\n",
      "Epoch 1546/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9700\n",
      "Epoch 1546: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0449 - accuracy: 0.9700 - val_loss: 0.0628 - val_accuracy: 0.9482\n",
      "Epoch 1547/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9700\n",
      "Epoch 1547: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0448 - accuracy: 0.9700 - val_loss: 0.0628 - val_accuracy: 0.9482\n",
      "Epoch 1548/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9700\n",
      "Epoch 1548: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0448 - accuracy: 0.9700 - val_loss: 0.0628 - val_accuracy: 0.9482\n",
      "Epoch 1549/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9700\n",
      "Epoch 1549: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0448 - accuracy: 0.9700 - val_loss: 0.0627 - val_accuracy: 0.9482\n",
      "Epoch 1550/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9700\n",
      "Epoch 1550: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0448 - accuracy: 0.9700 - val_loss: 0.0627 - val_accuracy: 0.9482\n",
      "Epoch 1551/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9700\n",
      "Epoch 1551: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0447 - accuracy: 0.9700 - val_loss: 0.0627 - val_accuracy: 0.9482\n",
      "Epoch 1552/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9700\n",
      "Epoch 1552: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0447 - accuracy: 0.9700 - val_loss: 0.0626 - val_accuracy: 0.9482\n",
      "Epoch 1553/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9700\n",
      "Epoch 1553: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0447 - accuracy: 0.9700 - val_loss: 0.0626 - val_accuracy: 0.9482\n",
      "Epoch 1554/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9700\n",
      "Epoch 1554: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0446 - accuracy: 0.9700 - val_loss: 0.0626 - val_accuracy: 0.9482\n",
      "Epoch 1555/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9700\n",
      "Epoch 1555: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0446 - accuracy: 0.9700 - val_loss: 0.0625 - val_accuracy: 0.9482\n",
      "Epoch 1556/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9700\n",
      "Epoch 1556: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0446 - accuracy: 0.9700 - val_loss: 0.0625 - val_accuracy: 0.9482\n",
      "Epoch 1557/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9700\n",
      "Epoch 1557: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0446 - accuracy: 0.9700 - val_loss: 0.0625 - val_accuracy: 0.9482\n",
      "Epoch 1558/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9700\n",
      "Epoch 1558: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0445 - accuracy: 0.9700 - val_loss: 0.0624 - val_accuracy: 0.9482\n",
      "Epoch 1559/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9700\n",
      "Epoch 1559: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0445 - accuracy: 0.9700 - val_loss: 0.0624 - val_accuracy: 0.9482\n",
      "Epoch 1560/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9700\n",
      "Epoch 1560: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0445 - accuracy: 0.9700 - val_loss: 0.0624 - val_accuracy: 0.9482\n",
      "Epoch 1561/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9700\n",
      "Epoch 1561: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0445 - accuracy: 0.9700 - val_loss: 0.0623 - val_accuracy: 0.9482\n",
      "Epoch 1562/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9700\n",
      "Epoch 1562: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0444 - accuracy: 0.9700 - val_loss: 0.0623 - val_accuracy: 0.9482\n",
      "Epoch 1563/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9700\n",
      "Epoch 1563: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0444 - accuracy: 0.9700 - val_loss: 0.0623 - val_accuracy: 0.9482\n",
      "Epoch 1564/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9700\n",
      "Epoch 1564: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0444 - accuracy: 0.9700 - val_loss: 0.0622 - val_accuracy: 0.9482\n",
      "Epoch 1565/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9700\n",
      "Epoch 1565: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0444 - accuracy: 0.9700 - val_loss: 0.0622 - val_accuracy: 0.9482\n",
      "Epoch 1566/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9700\n",
      "Epoch 1566: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0443 - accuracy: 0.9700 - val_loss: 0.0622 - val_accuracy: 0.9482\n",
      "Epoch 1567/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9700\n",
      "Epoch 1567: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0443 - accuracy: 0.9700 - val_loss: 0.0622 - val_accuracy: 0.9482\n",
      "Epoch 1568/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9700\n",
      "Epoch 1568: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0443 - accuracy: 0.9700 - val_loss: 0.0621 - val_accuracy: 0.9482\n",
      "Epoch 1569/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9700\n",
      "Epoch 1569: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0442 - accuracy: 0.9700 - val_loss: 0.0621 - val_accuracy: 0.9482\n",
      "Epoch 1570/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9700\n",
      "Epoch 1570: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0442 - accuracy: 0.9700 - val_loss: 0.0621 - val_accuracy: 0.9482\n",
      "Epoch 1571/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9700\n",
      "Epoch 1571: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0442 - accuracy: 0.9700 - val_loss: 0.0620 - val_accuracy: 0.9482\n",
      "Epoch 1572/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9700\n",
      "Epoch 1572: val_accuracy did not improve from 0.95045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0442 - accuracy: 0.9700 - val_loss: 0.0620 - val_accuracy: 0.9482\n",
      "Epoch 1573/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9700\n",
      "Epoch 1573: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0441 - accuracy: 0.9700 - val_loss: 0.0620 - val_accuracy: 0.9482\n",
      "Epoch 1574/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9700\n",
      "Epoch 1574: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0441 - accuracy: 0.9700 - val_loss: 0.0619 - val_accuracy: 0.9482\n",
      "Epoch 1575/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9700\n",
      "Epoch 1575: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0441 - accuracy: 0.9700 - val_loss: 0.0619 - val_accuracy: 0.9482\n",
      "Epoch 1576/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9710\n",
      "Epoch 1576: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0441 - accuracy: 0.9710 - val_loss: 0.0619 - val_accuracy: 0.9482\n",
      "Epoch 1577/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9710\n",
      "Epoch 1577: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0440 - accuracy: 0.9710 - val_loss: 0.0618 - val_accuracy: 0.9482\n",
      "Epoch 1578/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9710\n",
      "Epoch 1578: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0440 - accuracy: 0.9710 - val_loss: 0.0618 - val_accuracy: 0.9482\n",
      "Epoch 1579/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9710\n",
      "Epoch 1579: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0440 - accuracy: 0.9710 - val_loss: 0.0618 - val_accuracy: 0.9482\n",
      "Epoch 1580/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9710\n",
      "Epoch 1580: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0439 - accuracy: 0.9710 - val_loss: 0.0617 - val_accuracy: 0.9482\n",
      "Epoch 1581/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9710\n",
      "Epoch 1581: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0439 - accuracy: 0.9710 - val_loss: 0.0617 - val_accuracy: 0.9482\n",
      "Epoch 1582/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9710\n",
      "Epoch 1582: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0439 - accuracy: 0.9710 - val_loss: 0.0617 - val_accuracy: 0.9482\n",
      "Epoch 1583/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9710\n",
      "Epoch 1583: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0439 - accuracy: 0.9710 - val_loss: 0.0616 - val_accuracy: 0.9482\n",
      "Epoch 1584/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9710\n",
      "Epoch 1584: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0438 - accuracy: 0.9710 - val_loss: 0.0616 - val_accuracy: 0.9482\n",
      "Epoch 1585/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9719\n",
      "Epoch 1585: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0438 - accuracy: 0.9719 - val_loss: 0.0616 - val_accuracy: 0.9482\n",
      "Epoch 1586/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9719\n",
      "Epoch 1586: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0438 - accuracy: 0.9719 - val_loss: 0.0615 - val_accuracy: 0.9482\n",
      "Epoch 1587/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9719\n",
      "Epoch 1587: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0438 - accuracy: 0.9719 - val_loss: 0.0615 - val_accuracy: 0.9482\n",
      "Epoch 1588/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9719\n",
      "Epoch 1588: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0437 - accuracy: 0.9719 - val_loss: 0.0615 - val_accuracy: 0.9482\n",
      "Epoch 1589/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9719\n",
      "Epoch 1589: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0437 - accuracy: 0.9719 - val_loss: 0.0614 - val_accuracy: 0.9482\n",
      "Epoch 1590/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9719\n",
      "Epoch 1590: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0437 - accuracy: 0.9719 - val_loss: 0.0614 - val_accuracy: 0.9482\n",
      "Epoch 1591/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9719\n",
      "Epoch 1591: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0437 - accuracy: 0.9719 - val_loss: 0.0614 - val_accuracy: 0.9482\n",
      "Epoch 1592/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9719\n",
      "Epoch 1592: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0436 - accuracy: 0.9719 - val_loss: 0.0613 - val_accuracy: 0.9482\n",
      "Epoch 1593/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9719\n",
      "Epoch 1593: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0436 - accuracy: 0.9719 - val_loss: 0.0613 - val_accuracy: 0.9482\n",
      "Epoch 1594/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9719\n",
      "Epoch 1594: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0436 - accuracy: 0.9719 - val_loss: 0.0613 - val_accuracy: 0.9482\n",
      "Epoch 1595/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9719\n",
      "Epoch 1595: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0436 - accuracy: 0.9719 - val_loss: 0.0612 - val_accuracy: 0.9482\n",
      "Epoch 1596/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9719\n",
      "Epoch 1596: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0435 - accuracy: 0.9719 - val_loss: 0.0612 - val_accuracy: 0.9482\n",
      "Epoch 1597/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9719\n",
      "Epoch 1597: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0435 - accuracy: 0.9719 - val_loss: 0.0612 - val_accuracy: 0.9482\n",
      "Epoch 1598/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9719\n",
      "Epoch 1598: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0435 - accuracy: 0.9719 - val_loss: 0.0611 - val_accuracy: 0.9482\n",
      "Epoch 1599/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9719\n",
      "Epoch 1599: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0435 - accuracy: 0.9719 - val_loss: 0.0611 - val_accuracy: 0.9482\n",
      "Epoch 1600/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9719\n",
      "Epoch 1600: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0434 - accuracy: 0.9719 - val_loss: 0.0611 - val_accuracy: 0.9482\n",
      "Epoch 1601/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9719\n",
      "Epoch 1601: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0434 - accuracy: 0.9719 - val_loss: 0.0611 - val_accuracy: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1602/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9719\n",
      "Epoch 1602: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0434 - accuracy: 0.9719 - val_loss: 0.0610 - val_accuracy: 0.9482\n",
      "Epoch 1603/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9719\n",
      "Epoch 1603: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0433 - accuracy: 0.9719 - val_loss: 0.0610 - val_accuracy: 0.9482\n",
      "Epoch 1604/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9719\n",
      "Epoch 1604: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0433 - accuracy: 0.9719 - val_loss: 0.0610 - val_accuracy: 0.9482\n",
      "Epoch 1605/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9719\n",
      "Epoch 1605: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0433 - accuracy: 0.9719 - val_loss: 0.0609 - val_accuracy: 0.9482\n",
      "Epoch 1606/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9719\n",
      "Epoch 1606: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0433 - accuracy: 0.9719 - val_loss: 0.0609 - val_accuracy: 0.9482\n",
      "Epoch 1607/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9719\n",
      "Epoch 1607: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0432 - accuracy: 0.9719 - val_loss: 0.0609 - val_accuracy: 0.9482\n",
      "Epoch 1608/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9719\n",
      "Epoch 1608: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0432 - accuracy: 0.9719 - val_loss: 0.0608 - val_accuracy: 0.9482\n",
      "Epoch 1609/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9719\n",
      "Epoch 1609: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0432 - accuracy: 0.9719 - val_loss: 0.0608 - val_accuracy: 0.9482\n",
      "Epoch 1610/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9719\n",
      "Epoch 1610: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0432 - accuracy: 0.9719 - val_loss: 0.0608 - val_accuracy: 0.9482\n",
      "Epoch 1611/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9719\n",
      "Epoch 1611: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0431 - accuracy: 0.9719 - val_loss: 0.0607 - val_accuracy: 0.9482\n",
      "Epoch 1612/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9719\n",
      "Epoch 1612: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0431 - accuracy: 0.9719 - val_loss: 0.0607 - val_accuracy: 0.9482\n",
      "Epoch 1613/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9719\n",
      "Epoch 1613: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0431 - accuracy: 0.9719 - val_loss: 0.0607 - val_accuracy: 0.9482\n",
      "Epoch 1614/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9719\n",
      "Epoch 1614: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0431 - accuracy: 0.9719 - val_loss: 0.0606 - val_accuracy: 0.9482\n",
      "Epoch 1615/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9719\n",
      "Epoch 1615: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0430 - accuracy: 0.9719 - val_loss: 0.0606 - val_accuracy: 0.9482\n",
      "Epoch 1616/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9719\n",
      "Epoch 1616: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0430 - accuracy: 0.9719 - val_loss: 0.0606 - val_accuracy: 0.9482\n",
      "Epoch 1617/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9719\n",
      "Epoch 1617: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0430 - accuracy: 0.9719 - val_loss: 0.0606 - val_accuracy: 0.9482\n",
      "Epoch 1618/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9719\n",
      "Epoch 1618: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0430 - accuracy: 0.9719 - val_loss: 0.0605 - val_accuracy: 0.9482\n",
      "Epoch 1619/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9719\n",
      "Epoch 1619: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0429 - accuracy: 0.9719 - val_loss: 0.0605 - val_accuracy: 0.9482\n",
      "Epoch 1620/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9719\n",
      "Epoch 1620: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0429 - accuracy: 0.9719 - val_loss: 0.0605 - val_accuracy: 0.9482\n",
      "Epoch 1621/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9719\n",
      "Epoch 1621: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0429 - accuracy: 0.9719 - val_loss: 0.0604 - val_accuracy: 0.9482\n",
      "Epoch 1622/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9719\n",
      "Epoch 1622: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0429 - accuracy: 0.9719 - val_loss: 0.0604 - val_accuracy: 0.9482\n",
      "Epoch 1623/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9719\n",
      "Epoch 1623: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0428 - accuracy: 0.9719 - val_loss: 0.0604 - val_accuracy: 0.9482\n",
      "Epoch 1624/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9719\n",
      "Epoch 1624: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0428 - accuracy: 0.9719 - val_loss: 0.0603 - val_accuracy: 0.9482\n",
      "Epoch 1625/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9719\n",
      "Epoch 1625: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0428 - accuracy: 0.9719 - val_loss: 0.0603 - val_accuracy: 0.9482\n",
      "Epoch 1626/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9719\n",
      "Epoch 1626: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0428 - accuracy: 0.9719 - val_loss: 0.0603 - val_accuracy: 0.9482\n",
      "Epoch 1627/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9719\n",
      "Epoch 1627: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0427 - accuracy: 0.9719 - val_loss: 0.0602 - val_accuracy: 0.9482\n",
      "Epoch 1628/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9719\n",
      "Epoch 1628: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0427 - accuracy: 0.9719 - val_loss: 0.0602 - val_accuracy: 0.9482\n",
      "Epoch 1629/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9719\n",
      "Epoch 1629: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0427 - accuracy: 0.9719 - val_loss: 0.0602 - val_accuracy: 0.9482\n",
      "Epoch 1630/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9719\n",
      "Epoch 1630: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0427 - accuracy: 0.9719 - val_loss: 0.0602 - val_accuracy: 0.9482\n",
      "Epoch 1631/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9729\n",
      "Epoch 1631: val_accuracy did not improve from 0.95045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0426 - accuracy: 0.9729 - val_loss: 0.0601 - val_accuracy: 0.9482\n",
      "Epoch 1632/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9729\n",
      "Epoch 1632: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0426 - accuracy: 0.9729 - val_loss: 0.0601 - val_accuracy: 0.9482\n",
      "Epoch 1633/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9729\n",
      "Epoch 1633: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0426 - accuracy: 0.9729 - val_loss: 0.0601 - val_accuracy: 0.9482\n",
      "Epoch 1634/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9729\n",
      "Epoch 1634: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0426 - accuracy: 0.9729 - val_loss: 0.0600 - val_accuracy: 0.9482\n",
      "Epoch 1635/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9729\n",
      "Epoch 1635: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0425 - accuracy: 0.9729 - val_loss: 0.0600 - val_accuracy: 0.9482\n",
      "Epoch 1636/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9729\n",
      "Epoch 1636: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0425 - accuracy: 0.9729 - val_loss: 0.0600 - val_accuracy: 0.9482\n",
      "Epoch 1637/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9729\n",
      "Epoch 1637: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0425 - accuracy: 0.9729 - val_loss: 0.0599 - val_accuracy: 0.9482\n",
      "Epoch 1638/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9729\n",
      "Epoch 1638: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0425 - accuracy: 0.9729 - val_loss: 0.0599 - val_accuracy: 0.9482\n",
      "Epoch 1639/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9729\n",
      "Epoch 1639: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0424 - accuracy: 0.9729 - val_loss: 0.0599 - val_accuracy: 0.9482\n",
      "Epoch 1640/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9729\n",
      "Epoch 1640: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0424 - accuracy: 0.9729 - val_loss: 0.0598 - val_accuracy: 0.9482\n",
      "Epoch 1641/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9729\n",
      "Epoch 1641: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0424 - accuracy: 0.9729 - val_loss: 0.0598 - val_accuracy: 0.9482\n",
      "Epoch 1642/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9729\n",
      "Epoch 1642: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0424 - accuracy: 0.9729 - val_loss: 0.0598 - val_accuracy: 0.9482\n",
      "Epoch 1643/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9729\n",
      "Epoch 1643: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0423 - accuracy: 0.9729 - val_loss: 0.0598 - val_accuracy: 0.9482\n",
      "Epoch 1644/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9729\n",
      "Epoch 1644: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0423 - accuracy: 0.9729 - val_loss: 0.0597 - val_accuracy: 0.9482\n",
      "Epoch 1645/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9729\n",
      "Epoch 1645: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0423 - accuracy: 0.9729 - val_loss: 0.0597 - val_accuracy: 0.9482\n",
      "Epoch 1646/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9729\n",
      "Epoch 1646: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0423 - accuracy: 0.9729 - val_loss: 0.0597 - val_accuracy: 0.9482\n",
      "Epoch 1647/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9729\n",
      "Epoch 1647: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0422 - accuracy: 0.9729 - val_loss: 0.0596 - val_accuracy: 0.9482\n",
      "Epoch 1648/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9729\n",
      "Epoch 1648: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0422 - accuracy: 0.9729 - val_loss: 0.0596 - val_accuracy: 0.9482\n",
      "Epoch 1649/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9729\n",
      "Epoch 1649: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0422 - accuracy: 0.9729 - val_loss: 0.0596 - val_accuracy: 0.9482\n",
      "Epoch 1650/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9729\n",
      "Epoch 1650: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0422 - accuracy: 0.9729 - val_loss: 0.0595 - val_accuracy: 0.9482\n",
      "Epoch 1651/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9729\n",
      "Epoch 1651: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0421 - accuracy: 0.9729 - val_loss: 0.0595 - val_accuracy: 0.9482\n",
      "Epoch 1652/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9729\n",
      "Epoch 1652: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0421 - accuracy: 0.9729 - val_loss: 0.0595 - val_accuracy: 0.9482\n",
      "Epoch 1653/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9729\n",
      "Epoch 1653: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0421 - accuracy: 0.9729 - val_loss: 0.0595 - val_accuracy: 0.9482\n",
      "Epoch 1654/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9729\n",
      "Epoch 1654: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0421 - accuracy: 0.9729 - val_loss: 0.0594 - val_accuracy: 0.9482\n",
      "Epoch 1655/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9729\n",
      "Epoch 1655: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0420 - accuracy: 0.9729 - val_loss: 0.0594 - val_accuracy: 0.9482\n",
      "Epoch 1656/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9729\n",
      "Epoch 1656: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0420 - accuracy: 0.9729 - val_loss: 0.0594 - val_accuracy: 0.9482\n",
      "Epoch 1657/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9729\n",
      "Epoch 1657: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0420 - accuracy: 0.9729 - val_loss: 0.0593 - val_accuracy: 0.9482\n",
      "Epoch 1658/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9729\n",
      "Epoch 1658: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0420 - accuracy: 0.9729 - val_loss: 0.0593 - val_accuracy: 0.9482\n",
      "Epoch 1659/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9729\n",
      "Epoch 1659: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0419 - accuracy: 0.9729 - val_loss: 0.0593 - val_accuracy: 0.9482\n",
      "Epoch 1660/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9729\n",
      "Epoch 1660: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0419 - accuracy: 0.9729 - val_loss: 0.0593 - val_accuracy: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1661/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9729\n",
      "Epoch 1661: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0419 - accuracy: 0.9729 - val_loss: 0.0592 - val_accuracy: 0.9482\n",
      "Epoch 1662/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9729\n",
      "Epoch 1662: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0419 - accuracy: 0.9729 - val_loss: 0.0592 - val_accuracy: 0.9482\n",
      "Epoch 1663/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9729\n",
      "Epoch 1663: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0418 - accuracy: 0.9729 - val_loss: 0.0591 - val_accuracy: 0.9482\n",
      "Epoch 1664/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9729\n",
      "Epoch 1664: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0418 - accuracy: 0.9729 - val_loss: 0.0591 - val_accuracy: 0.9482\n",
      "Epoch 1665/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9729\n",
      "Epoch 1665: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0418 - accuracy: 0.9729 - val_loss: 0.0591 - val_accuracy: 0.9482\n",
      "Epoch 1666/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9729\n",
      "Epoch 1666: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0418 - accuracy: 0.9729 - val_loss: 0.0591 - val_accuracy: 0.9482\n",
      "Epoch 1667/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9729\n",
      "Epoch 1667: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0417 - accuracy: 0.9729 - val_loss: 0.0590 - val_accuracy: 0.9482\n",
      "Epoch 1668/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9729\n",
      "Epoch 1668: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0417 - accuracy: 0.9729 - val_loss: 0.0590 - val_accuracy: 0.9482\n",
      "Epoch 1669/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9729\n",
      "Epoch 1669: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0417 - accuracy: 0.9729 - val_loss: 0.0590 - val_accuracy: 0.9505\n",
      "Epoch 1670/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9729\n",
      "Epoch 1670: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0417 - accuracy: 0.9729 - val_loss: 0.0590 - val_accuracy: 0.9482\n",
      "Epoch 1671/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9729\n",
      "Epoch 1671: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0416 - accuracy: 0.9729 - val_loss: 0.0589 - val_accuracy: 0.9482\n",
      "Epoch 1672/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9729\n",
      "Epoch 1672: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0416 - accuracy: 0.9729 - val_loss: 0.0589 - val_accuracy: 0.9482\n",
      "Epoch 1673/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9729\n",
      "Epoch 1673: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0416 - accuracy: 0.9729 - val_loss: 0.0589 - val_accuracy: 0.9482\n",
      "Epoch 1674/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9729\n",
      "Epoch 1674: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0416 - accuracy: 0.9729 - val_loss: 0.0588 - val_accuracy: 0.9482\n",
      "Epoch 1675/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9729\n",
      "Epoch 1675: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0416 - accuracy: 0.9729 - val_loss: 0.0588 - val_accuracy: 0.9482\n",
      "Epoch 1676/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9729\n",
      "Epoch 1676: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0415 - accuracy: 0.9729 - val_loss: 0.0588 - val_accuracy: 0.9505\n",
      "Epoch 1677/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9729\n",
      "Epoch 1677: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0415 - accuracy: 0.9729 - val_loss: 0.0588 - val_accuracy: 0.9482\n",
      "Epoch 1678/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9729\n",
      "Epoch 1678: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0415 - accuracy: 0.9729 - val_loss: 0.0587 - val_accuracy: 0.9505\n",
      "Epoch 1679/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9729\n",
      "Epoch 1679: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0415 - accuracy: 0.9729 - val_loss: 0.0587 - val_accuracy: 0.9482\n",
      "Epoch 1680/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9729\n",
      "Epoch 1680: val_accuracy did not improve from 0.95045\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0414 - accuracy: 0.9729 - val_loss: 0.0587 - val_accuracy: 0.9482\n",
      "Epoch 1681/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9729\n",
      "Epoch 1681: val_accuracy improved from 0.95045 to 0.95270, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0414 - accuracy: 0.9729 - val_loss: 0.0586 - val_accuracy: 0.9527\n",
      "Epoch 1682/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9729\n",
      "Epoch 1682: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0414 - accuracy: 0.9729 - val_loss: 0.0586 - val_accuracy: 0.9505\n",
      "Epoch 1683/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9729\n",
      "Epoch 1683: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0414 - accuracy: 0.9729 - val_loss: 0.0586 - val_accuracy: 0.9527\n",
      "Epoch 1684/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9729\n",
      "Epoch 1684: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0413 - accuracy: 0.9729 - val_loss: 0.0586 - val_accuracy: 0.9505\n",
      "Epoch 1685/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9729\n",
      "Epoch 1685: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0413 - accuracy: 0.9729 - val_loss: 0.0585 - val_accuracy: 0.9527\n",
      "Epoch 1686/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9729\n",
      "Epoch 1686: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0413 - accuracy: 0.9729 - val_loss: 0.0585 - val_accuracy: 0.9527\n",
      "Epoch 1687/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9729\n",
      "Epoch 1687: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0413 - accuracy: 0.9729 - val_loss: 0.0585 - val_accuracy: 0.9527\n",
      "Epoch 1688/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9729\n",
      "Epoch 1688: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0412 - accuracy: 0.9729 - val_loss: 0.0584 - val_accuracy: 0.9527\n",
      "Epoch 1689/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9729\n",
      "Epoch 1689: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0412 - accuracy: 0.9729 - val_loss: 0.0584 - val_accuracy: 0.9527\n",
      "Epoch 1690/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9729\n",
      "Epoch 1690: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0412 - accuracy: 0.9729 - val_loss: 0.0584 - val_accuracy: 0.9527\n",
      "Epoch 1691/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9729\n",
      "Epoch 1691: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0412 - accuracy: 0.9729 - val_loss: 0.0583 - val_accuracy: 0.9527\n",
      "Epoch 1692/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9729\n",
      "Epoch 1692: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0411 - accuracy: 0.9729 - val_loss: 0.0583 - val_accuracy: 0.9527\n",
      "Epoch 1693/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9729\n",
      "Epoch 1693: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0411 - accuracy: 0.9729 - val_loss: 0.0583 - val_accuracy: 0.9527\n",
      "Epoch 1694/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9729\n",
      "Epoch 1694: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0411 - accuracy: 0.9729 - val_loss: 0.0583 - val_accuracy: 0.9527\n",
      "Epoch 1695/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9729\n",
      "Epoch 1695: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0411 - accuracy: 0.9729 - val_loss: 0.0582 - val_accuracy: 0.9527\n",
      "Epoch 1696/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9729\n",
      "Epoch 1696: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0411 - accuracy: 0.9729 - val_loss: 0.0582 - val_accuracy: 0.9527\n",
      "Epoch 1697/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9729\n",
      "Epoch 1697: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0410 - accuracy: 0.9729 - val_loss: 0.0582 - val_accuracy: 0.9527\n",
      "Epoch 1698/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9729\n",
      "Epoch 1698: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0410 - accuracy: 0.9729 - val_loss: 0.0581 - val_accuracy: 0.9527\n",
      "Epoch 1699/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9729\n",
      "Epoch 1699: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0410 - accuracy: 0.9729 - val_loss: 0.0581 - val_accuracy: 0.9527\n",
      "Epoch 1700/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9729\n",
      "Epoch 1700: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0410 - accuracy: 0.9729 - val_loss: 0.0581 - val_accuracy: 0.9527\n",
      "Epoch 1701/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9729\n",
      "Epoch 1701: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0409 - accuracy: 0.9729 - val_loss: 0.0581 - val_accuracy: 0.9527\n",
      "Epoch 1702/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9729\n",
      "Epoch 1702: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0409 - accuracy: 0.9729 - val_loss: 0.0580 - val_accuracy: 0.9527\n",
      "Epoch 1703/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9729\n",
      "Epoch 1703: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0409 - accuracy: 0.9729 - val_loss: 0.0580 - val_accuracy: 0.9527\n",
      "Epoch 1704/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9729\n",
      "Epoch 1704: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0409 - accuracy: 0.9729 - val_loss: 0.0580 - val_accuracy: 0.9527\n",
      "Epoch 1705/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9729\n",
      "Epoch 1705: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0408 - accuracy: 0.9729 - val_loss: 0.0579 - val_accuracy: 0.9527\n",
      "Epoch 1706/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9729\n",
      "Epoch 1706: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0408 - accuracy: 0.9729 - val_loss: 0.0579 - val_accuracy: 0.9527\n",
      "Epoch 1707/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9729\n",
      "Epoch 1707: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0408 - accuracy: 0.9729 - val_loss: 0.0579 - val_accuracy: 0.9527\n",
      "Epoch 1708/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9729\n",
      "Epoch 1708: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0408 - accuracy: 0.9729 - val_loss: 0.0579 - val_accuracy: 0.9527\n",
      "Epoch 1709/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9729\n",
      "Epoch 1709: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0407 - accuracy: 0.9729 - val_loss: 0.0578 - val_accuracy: 0.9527\n",
      "Epoch 1710/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9729\n",
      "Epoch 1710: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0407 - accuracy: 0.9729 - val_loss: 0.0578 - val_accuracy: 0.9527\n",
      "Epoch 1711/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9729\n",
      "Epoch 1711: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0407 - accuracy: 0.9729 - val_loss: 0.0578 - val_accuracy: 0.9527\n",
      "Epoch 1712/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9729\n",
      "Epoch 1712: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0407 - accuracy: 0.9729 - val_loss: 0.0577 - val_accuracy: 0.9527\n",
      "Epoch 1713/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9729\n",
      "Epoch 1713: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0407 - accuracy: 0.9729 - val_loss: 0.0577 - val_accuracy: 0.9527\n",
      "Epoch 1714/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9729\n",
      "Epoch 1714: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0406 - accuracy: 0.9729 - val_loss: 0.0577 - val_accuracy: 0.9527\n",
      "Epoch 1715/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9729\n",
      "Epoch 1715: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0406 - accuracy: 0.9729 - val_loss: 0.0577 - val_accuracy: 0.9527\n",
      "Epoch 1716/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9729\n",
      "Epoch 1716: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0406 - accuracy: 0.9729 - val_loss: 0.0576 - val_accuracy: 0.9527\n",
      "Epoch 1717/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9729\n",
      "Epoch 1717: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0406 - accuracy: 0.9729 - val_loss: 0.0576 - val_accuracy: 0.9527\n",
      "Epoch 1718/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9729\n",
      "Epoch 1718: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0405 - accuracy: 0.9729 - val_loss: 0.0576 - val_accuracy: 0.9527\n",
      "Epoch 1719/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9729\n",
      "Epoch 1719: val_accuracy did not improve from 0.95270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0405 - accuracy: 0.9729 - val_loss: 0.0576 - val_accuracy: 0.9527\n",
      "Epoch 1720/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9729\n",
      "Epoch 1720: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0405 - accuracy: 0.9729 - val_loss: 0.0575 - val_accuracy: 0.9527\n",
      "Epoch 1721/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9729\n",
      "Epoch 1721: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0405 - accuracy: 0.9729 - val_loss: 0.0575 - val_accuracy: 0.9527\n",
      "Epoch 1722/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9729\n",
      "Epoch 1722: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0404 - accuracy: 0.9729 - val_loss: 0.0575 - val_accuracy: 0.9527\n",
      "Epoch 1723/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9729\n",
      "Epoch 1723: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0404 - accuracy: 0.9729 - val_loss: 0.0574 - val_accuracy: 0.9527\n",
      "Epoch 1724/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9729\n",
      "Epoch 1724: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0404 - accuracy: 0.9729 - val_loss: 0.0574 - val_accuracy: 0.9527\n",
      "Epoch 1725/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9729\n",
      "Epoch 1725: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0404 - accuracy: 0.9729 - val_loss: 0.0574 - val_accuracy: 0.9527\n",
      "Epoch 1726/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9729\n",
      "Epoch 1726: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0404 - accuracy: 0.9729 - val_loss: 0.0574 - val_accuracy: 0.9527\n",
      "Epoch 1727/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9729\n",
      "Epoch 1727: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0403 - accuracy: 0.9729 - val_loss: 0.0573 - val_accuracy: 0.9527\n",
      "Epoch 1728/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9729\n",
      "Epoch 1728: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0403 - accuracy: 0.9729 - val_loss: 0.0573 - val_accuracy: 0.9527\n",
      "Epoch 1729/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9729\n",
      "Epoch 1729: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0403 - accuracy: 0.9729 - val_loss: 0.0573 - val_accuracy: 0.9527\n",
      "Epoch 1730/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9729\n",
      "Epoch 1730: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0403 - accuracy: 0.9729 - val_loss: 0.0573 - val_accuracy: 0.9527\n",
      "Epoch 1731/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9729\n",
      "Epoch 1731: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0402 - accuracy: 0.9729 - val_loss: 0.0572 - val_accuracy: 0.9527\n",
      "Epoch 1732/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9729\n",
      "Epoch 1732: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0402 - accuracy: 0.9729 - val_loss: 0.0572 - val_accuracy: 0.9527\n",
      "Epoch 1733/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9729\n",
      "Epoch 1733: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0402 - accuracy: 0.9729 - val_loss: 0.0572 - val_accuracy: 0.9527\n",
      "Epoch 1734/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9729\n",
      "Epoch 1734: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0402 - accuracy: 0.9729 - val_loss: 0.0571 - val_accuracy: 0.9527\n",
      "Epoch 1735/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9729\n",
      "Epoch 1735: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0402 - accuracy: 0.9729 - val_loss: 0.0571 - val_accuracy: 0.9527\n",
      "Epoch 1736/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9729\n",
      "Epoch 1736: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0401 - accuracy: 0.9729 - val_loss: 0.0571 - val_accuracy: 0.9527\n",
      "Epoch 1737/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9729\n",
      "Epoch 1737: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0401 - accuracy: 0.9729 - val_loss: 0.0571 - val_accuracy: 0.9527\n",
      "Epoch 1738/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9729\n",
      "Epoch 1738: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0401 - accuracy: 0.9729 - val_loss: 0.0570 - val_accuracy: 0.9527\n",
      "Epoch 1739/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9729\n",
      "Epoch 1739: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0401 - accuracy: 0.9729 - val_loss: 0.0570 - val_accuracy: 0.9527\n",
      "Epoch 1740/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9729\n",
      "Epoch 1740: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0400 - accuracy: 0.9729 - val_loss: 0.0570 - val_accuracy: 0.9527\n",
      "Epoch 1741/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9729\n",
      "Epoch 1741: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0400 - accuracy: 0.9729 - val_loss: 0.0570 - val_accuracy: 0.9527\n",
      "Epoch 1742/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9729\n",
      "Epoch 1742: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0400 - accuracy: 0.9729 - val_loss: 0.0569 - val_accuracy: 0.9527\n",
      "Epoch 1743/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9729\n",
      "Epoch 1743: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0400 - accuracy: 0.9729 - val_loss: 0.0569 - val_accuracy: 0.9527\n",
      "Epoch 1744/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9729\n",
      "Epoch 1744: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0399 - accuracy: 0.9729 - val_loss: 0.0569 - val_accuracy: 0.9527\n",
      "Epoch 1745/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9729\n",
      "Epoch 1745: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0399 - accuracy: 0.9729 - val_loss: 0.0568 - val_accuracy: 0.9527\n",
      "Epoch 1746/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9729\n",
      "Epoch 1746: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0399 - accuracy: 0.9729 - val_loss: 0.0568 - val_accuracy: 0.9527\n",
      "Epoch 1747/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9729\n",
      "Epoch 1747: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0399 - accuracy: 0.9729 - val_loss: 0.0568 - val_accuracy: 0.9527\n",
      "Epoch 1748/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9729\n",
      "Epoch 1748: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0399 - accuracy: 0.9729 - val_loss: 0.0568 - val_accuracy: 0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1749/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9729\n",
      "Epoch 1749: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0398 - accuracy: 0.9729 - val_loss: 0.0567 - val_accuracy: 0.9527\n",
      "Epoch 1750/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9739\n",
      "Epoch 1750: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0398 - accuracy: 0.9739 - val_loss: 0.0567 - val_accuracy: 0.9527\n",
      "Epoch 1751/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9739\n",
      "Epoch 1751: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0398 - accuracy: 0.9739 - val_loss: 0.0567 - val_accuracy: 0.9527\n",
      "Epoch 1752/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9739\n",
      "Epoch 1752: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0398 - accuracy: 0.9739 - val_loss: 0.0567 - val_accuracy: 0.9527\n",
      "Epoch 1753/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9739\n",
      "Epoch 1753: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0397 - accuracy: 0.9739 - val_loss: 0.0566 - val_accuracy: 0.9527\n",
      "Epoch 1754/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9739\n",
      "Epoch 1754: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0397 - accuracy: 0.9739 - val_loss: 0.0566 - val_accuracy: 0.9527\n",
      "Epoch 1755/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9739\n",
      "Epoch 1755: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0397 - accuracy: 0.9739 - val_loss: 0.0566 - val_accuracy: 0.9527\n",
      "Epoch 1756/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9739\n",
      "Epoch 1756: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0397 - accuracy: 0.9739 - val_loss: 0.0565 - val_accuracy: 0.9527\n",
      "Epoch 1757/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9739\n",
      "Epoch 1757: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0397 - accuracy: 0.9739 - val_loss: 0.0565 - val_accuracy: 0.9527\n",
      "Epoch 1758/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9739\n",
      "Epoch 1758: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0396 - accuracy: 0.9739 - val_loss: 0.0565 - val_accuracy: 0.9527\n",
      "Epoch 1759/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9739\n",
      "Epoch 1759: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0396 - accuracy: 0.9739 - val_loss: 0.0565 - val_accuracy: 0.9527\n",
      "Epoch 1760/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9739\n",
      "Epoch 1760: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0396 - accuracy: 0.9739 - val_loss: 0.0564 - val_accuracy: 0.9527\n",
      "Epoch 1761/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9739\n",
      "Epoch 1761: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0396 - accuracy: 0.9739 - val_loss: 0.0564 - val_accuracy: 0.9527\n",
      "Epoch 1762/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9739\n",
      "Epoch 1762: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0395 - accuracy: 0.9739 - val_loss: 0.0564 - val_accuracy: 0.9527\n",
      "Epoch 1763/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9739\n",
      "Epoch 1763: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0395 - accuracy: 0.9739 - val_loss: 0.0564 - val_accuracy: 0.9527\n",
      "Epoch 1764/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9739\n",
      "Epoch 1764: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0395 - accuracy: 0.9739 - val_loss: 0.0563 - val_accuracy: 0.9527\n",
      "Epoch 1765/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9739\n",
      "Epoch 1765: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0395 - accuracy: 0.9739 - val_loss: 0.0563 - val_accuracy: 0.9527\n",
      "Epoch 1766/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9739\n",
      "Epoch 1766: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0395 - accuracy: 0.9739 - val_loss: 0.0563 - val_accuracy: 0.9527\n",
      "Epoch 1767/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9739\n",
      "Epoch 1767: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0394 - accuracy: 0.9739 - val_loss: 0.0563 - val_accuracy: 0.9527\n",
      "Epoch 1768/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9739\n",
      "Epoch 1768: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0394 - accuracy: 0.9739 - val_loss: 0.0562 - val_accuracy: 0.9527\n",
      "Epoch 1769/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9739\n",
      "Epoch 1769: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0394 - accuracy: 0.9739 - val_loss: 0.0562 - val_accuracy: 0.9527\n",
      "Epoch 1770/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9739\n",
      "Epoch 1770: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0394 - accuracy: 0.9739 - val_loss: 0.0562 - val_accuracy: 0.9527\n",
      "Epoch 1771/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9739\n",
      "Epoch 1771: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0393 - accuracy: 0.9739 - val_loss: 0.0561 - val_accuracy: 0.9527\n",
      "Epoch 1772/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9739\n",
      "Epoch 1772: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0393 - accuracy: 0.9739 - val_loss: 0.0561 - val_accuracy: 0.9527\n",
      "Epoch 1773/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9739\n",
      "Epoch 1773: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0393 - accuracy: 0.9739 - val_loss: 0.0561 - val_accuracy: 0.9527\n",
      "Epoch 1774/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9739\n",
      "Epoch 1774: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0393 - accuracy: 0.9739 - val_loss: 0.0561 - val_accuracy: 0.9527\n",
      "Epoch 1775/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9739\n",
      "Epoch 1775: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0393 - accuracy: 0.9739 - val_loss: 0.0560 - val_accuracy: 0.9527\n",
      "Epoch 1776/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9739\n",
      "Epoch 1776: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0392 - accuracy: 0.9739 - val_loss: 0.0560 - val_accuracy: 0.9527\n",
      "Epoch 1777/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9739\n",
      "Epoch 1777: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0392 - accuracy: 0.9739 - val_loss: 0.0560 - val_accuracy: 0.9527\n",
      "Epoch 1778/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9739\n",
      "Epoch 1778: val_accuracy did not improve from 0.95270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0392 - accuracy: 0.9739 - val_loss: 0.0560 - val_accuracy: 0.9527\n",
      "Epoch 1779/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9739\n",
      "Epoch 1779: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0392 - accuracy: 0.9739 - val_loss: 0.0559 - val_accuracy: 0.9527\n",
      "Epoch 1780/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9739\n",
      "Epoch 1780: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0392 - accuracy: 0.9739 - val_loss: 0.0559 - val_accuracy: 0.9527\n",
      "Epoch 1781/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9739\n",
      "Epoch 1781: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0391 - accuracy: 0.9739 - val_loss: 0.0559 - val_accuracy: 0.9527\n",
      "Epoch 1782/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9739\n",
      "Epoch 1782: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0391 - accuracy: 0.9739 - val_loss: 0.0559 - val_accuracy: 0.9527\n",
      "Epoch 1783/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9739\n",
      "Epoch 1783: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0391 - accuracy: 0.9739 - val_loss: 0.0558 - val_accuracy: 0.9527\n",
      "Epoch 1784/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9739\n",
      "Epoch 1784: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0391 - accuracy: 0.9739 - val_loss: 0.0558 - val_accuracy: 0.9527\n",
      "Epoch 1785/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9739\n",
      "Epoch 1785: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0390 - accuracy: 0.9739 - val_loss: 0.0558 - val_accuracy: 0.9527\n",
      "Epoch 1786/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9739\n",
      "Epoch 1786: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0390 - accuracy: 0.9739 - val_loss: 0.0558 - val_accuracy: 0.9527\n",
      "Epoch 1787/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9739\n",
      "Epoch 1787: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0390 - accuracy: 0.9739 - val_loss: 0.0557 - val_accuracy: 0.9527\n",
      "Epoch 1788/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9739\n",
      "Epoch 1788: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0390 - accuracy: 0.9739 - val_loss: 0.0557 - val_accuracy: 0.9527\n",
      "Epoch 1789/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9739\n",
      "Epoch 1789: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0390 - accuracy: 0.9739 - val_loss: 0.0557 - val_accuracy: 0.9527\n",
      "Epoch 1790/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9739\n",
      "Epoch 1790: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0389 - accuracy: 0.9739 - val_loss: 0.0557 - val_accuracy: 0.9527\n",
      "Epoch 1791/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9739\n",
      "Epoch 1791: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0389 - accuracy: 0.9739 - val_loss: 0.0556 - val_accuracy: 0.9527\n",
      "Epoch 1792/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9739\n",
      "Epoch 1792: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0389 - accuracy: 0.9739 - val_loss: 0.0556 - val_accuracy: 0.9527\n",
      "Epoch 1793/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9739\n",
      "Epoch 1793: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0389 - accuracy: 0.9739 - val_loss: 0.0556 - val_accuracy: 0.9527\n",
      "Epoch 1794/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9739\n",
      "Epoch 1794: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0389 - accuracy: 0.9739 - val_loss: 0.0555 - val_accuracy: 0.9527\n",
      "Epoch 1795/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9739\n",
      "Epoch 1795: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0388 - accuracy: 0.9739 - val_loss: 0.0555 - val_accuracy: 0.9527\n",
      "Epoch 1796/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9739\n",
      "Epoch 1796: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0388 - accuracy: 0.9739 - val_loss: 0.0555 - val_accuracy: 0.9527\n",
      "Epoch 1797/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9739\n",
      "Epoch 1797: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0388 - accuracy: 0.9739 - val_loss: 0.0555 - val_accuracy: 0.9527\n",
      "Epoch 1798/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9739\n",
      "Epoch 1798: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0388 - accuracy: 0.9739 - val_loss: 0.0554 - val_accuracy: 0.9527\n",
      "Epoch 1799/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9739\n",
      "Epoch 1799: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0387 - accuracy: 0.9739 - val_loss: 0.0554 - val_accuracy: 0.9527\n",
      "Epoch 1800/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9739\n",
      "Epoch 1800: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0387 - accuracy: 0.9739 - val_loss: 0.0554 - val_accuracy: 0.9527\n",
      "Epoch 1801/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9739\n",
      "Epoch 1801: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0387 - accuracy: 0.9739 - val_loss: 0.0554 - val_accuracy: 0.9527\n",
      "Epoch 1802/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9739\n",
      "Epoch 1802: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0387 - accuracy: 0.9739 - val_loss: 0.0553 - val_accuracy: 0.9527\n",
      "Epoch 1803/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9739\n",
      "Epoch 1803: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0387 - accuracy: 0.9739 - val_loss: 0.0553 - val_accuracy: 0.9527\n",
      "Epoch 1804/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9739\n",
      "Epoch 1804: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0386 - accuracy: 0.9739 - val_loss: 0.0553 - val_accuracy: 0.9527\n",
      "Epoch 1805/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9739\n",
      "Epoch 1805: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0386 - accuracy: 0.9739 - val_loss: 0.0553 - val_accuracy: 0.9527\n",
      "Epoch 1806/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9739\n",
      "Epoch 1806: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0386 - accuracy: 0.9739 - val_loss: 0.0552 - val_accuracy: 0.9527\n",
      "Epoch 1807/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9739\n",
      "Epoch 1807: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0386 - accuracy: 0.9739 - val_loss: 0.0552 - val_accuracy: 0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1808/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9739\n",
      "Epoch 1808: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0386 - accuracy: 0.9739 - val_loss: 0.0552 - val_accuracy: 0.9527\n",
      "Epoch 1809/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9739\n",
      "Epoch 1809: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0385 - accuracy: 0.9739 - val_loss: 0.0552 - val_accuracy: 0.9527\n",
      "Epoch 1810/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9739\n",
      "Epoch 1810: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0385 - accuracy: 0.9739 - val_loss: 0.0551 - val_accuracy: 0.9527\n",
      "Epoch 1811/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9739\n",
      "Epoch 1811: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0385 - accuracy: 0.9739 - val_loss: 0.0551 - val_accuracy: 0.9527\n",
      "Epoch 1812/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9739\n",
      "Epoch 1812: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0385 - accuracy: 0.9739 - val_loss: 0.0551 - val_accuracy: 0.9527\n",
      "Epoch 1813/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9739\n",
      "Epoch 1813: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0384 - accuracy: 0.9739 - val_loss: 0.0551 - val_accuracy: 0.9527\n",
      "Epoch 1814/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9739\n",
      "Epoch 1814: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0384 - accuracy: 0.9739 - val_loss: 0.0550 - val_accuracy: 0.9527\n",
      "Epoch 1815/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9739\n",
      "Epoch 1815: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0384 - accuracy: 0.9739 - val_loss: 0.0550 - val_accuracy: 0.9527\n",
      "Epoch 1816/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9739\n",
      "Epoch 1816: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0384 - accuracy: 0.9739 - val_loss: 0.0550 - val_accuracy: 0.9527\n",
      "Epoch 1817/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9739\n",
      "Epoch 1817: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0384 - accuracy: 0.9739 - val_loss: 0.0550 - val_accuracy: 0.9527\n",
      "Epoch 1818/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9739\n",
      "Epoch 1818: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0383 - accuracy: 0.9739 - val_loss: 0.0549 - val_accuracy: 0.9527\n",
      "Epoch 1819/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9739\n",
      "Epoch 1819: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0383 - accuracy: 0.9739 - val_loss: 0.0549 - val_accuracy: 0.9527\n",
      "Epoch 1820/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9739\n",
      "Epoch 1820: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0383 - accuracy: 0.9739 - val_loss: 0.0549 - val_accuracy: 0.9527\n",
      "Epoch 1821/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9739\n",
      "Epoch 1821: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0383 - accuracy: 0.9739 - val_loss: 0.0549 - val_accuracy: 0.9527\n",
      "Epoch 1822/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9739\n",
      "Epoch 1822: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0383 - accuracy: 0.9739 - val_loss: 0.0548 - val_accuracy: 0.9527\n",
      "Epoch 1823/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9739\n",
      "Epoch 1823: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0382 - accuracy: 0.9739 - val_loss: 0.0548 - val_accuracy: 0.9527\n",
      "Epoch 1824/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9739\n",
      "Epoch 1824: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0382 - accuracy: 0.9739 - val_loss: 0.0548 - val_accuracy: 0.9527\n",
      "Epoch 1825/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9739\n",
      "Epoch 1825: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0382 - accuracy: 0.9739 - val_loss: 0.0548 - val_accuracy: 0.9527\n",
      "Epoch 1826/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9739\n",
      "Epoch 1826: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0382 - accuracy: 0.9739 - val_loss: 0.0547 - val_accuracy: 0.9527\n",
      "Epoch 1827/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9739\n",
      "Epoch 1827: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0382 - accuracy: 0.9739 - val_loss: 0.0547 - val_accuracy: 0.9527\n",
      "Epoch 1828/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9739\n",
      "Epoch 1828: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0381 - accuracy: 0.9739 - val_loss: 0.0547 - val_accuracy: 0.9527\n",
      "Epoch 1829/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9739\n",
      "Epoch 1829: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0381 - accuracy: 0.9739 - val_loss: 0.0547 - val_accuracy: 0.9527\n",
      "Epoch 1830/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9739\n",
      "Epoch 1830: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0381 - accuracy: 0.9739 - val_loss: 0.0546 - val_accuracy: 0.9527\n",
      "Epoch 1831/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9739\n",
      "Epoch 1831: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0381 - accuracy: 0.9739 - val_loss: 0.0546 - val_accuracy: 0.9527\n",
      "Epoch 1832/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9739\n",
      "Epoch 1832: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0380 - accuracy: 0.9739 - val_loss: 0.0546 - val_accuracy: 0.9527\n",
      "Epoch 1833/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9739\n",
      "Epoch 1833: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0380 - accuracy: 0.9739 - val_loss: 0.0546 - val_accuracy: 0.9527\n",
      "Epoch 1834/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9739\n",
      "Epoch 1834: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0380 - accuracy: 0.9739 - val_loss: 0.0545 - val_accuracy: 0.9527\n",
      "Epoch 1835/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9739\n",
      "Epoch 1835: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0380 - accuracy: 0.9739 - val_loss: 0.0545 - val_accuracy: 0.9527\n",
      "Epoch 1836/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9739\n",
      "Epoch 1836: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0380 - accuracy: 0.9739 - val_loss: 0.0545 - val_accuracy: 0.9527\n",
      "Epoch 1837/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9739\n",
      "Epoch 1837: val_accuracy did not improve from 0.95270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0379 - accuracy: 0.9739 - val_loss: 0.0545 - val_accuracy: 0.9527\n",
      "Epoch 1838/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9739\n",
      "Epoch 1838: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0379 - accuracy: 0.9739 - val_loss: 0.0544 - val_accuracy: 0.9527\n",
      "Epoch 1839/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9739\n",
      "Epoch 1839: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0379 - accuracy: 0.9739 - val_loss: 0.0544 - val_accuracy: 0.9527\n",
      "Epoch 1840/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9748\n",
      "Epoch 1840: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0379 - accuracy: 0.9748 - val_loss: 0.0544 - val_accuracy: 0.9527\n",
      "Epoch 1841/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9748\n",
      "Epoch 1841: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0379 - accuracy: 0.9748 - val_loss: 0.0544 - val_accuracy: 0.9527\n",
      "Epoch 1842/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9748\n",
      "Epoch 1842: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0378 - accuracy: 0.9748 - val_loss: 0.0543 - val_accuracy: 0.9527\n",
      "Epoch 1843/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9748\n",
      "Epoch 1843: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0378 - accuracy: 0.9748 - val_loss: 0.0543 - val_accuracy: 0.9527\n",
      "Epoch 1844/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9748\n",
      "Epoch 1844: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0378 - accuracy: 0.9748 - val_loss: 0.0543 - val_accuracy: 0.9527\n",
      "Epoch 1845/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9748\n",
      "Epoch 1845: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0378 - accuracy: 0.9748 - val_loss: 0.0543 - val_accuracy: 0.9527\n",
      "Epoch 1846/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9748\n",
      "Epoch 1846: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0378 - accuracy: 0.9748 - val_loss: 0.0542 - val_accuracy: 0.9527\n",
      "Epoch 1847/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9748\n",
      "Epoch 1847: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0377 - accuracy: 0.9748 - val_loss: 0.0542 - val_accuracy: 0.9527\n",
      "Epoch 1848/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9748\n",
      "Epoch 1848: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0377 - accuracy: 0.9748 - val_loss: 0.0542 - val_accuracy: 0.9527\n",
      "Epoch 1849/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9748\n",
      "Epoch 1849: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0377 - accuracy: 0.9748 - val_loss: 0.0542 - val_accuracy: 0.9527\n",
      "Epoch 1850/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9748\n",
      "Epoch 1850: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0377 - accuracy: 0.9748 - val_loss: 0.0542 - val_accuracy: 0.9527\n",
      "Epoch 1851/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9748\n",
      "Epoch 1851: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0377 - accuracy: 0.9748 - val_loss: 0.0541 - val_accuracy: 0.9527\n",
      "Epoch 1852/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9748\n",
      "Epoch 1852: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0376 - accuracy: 0.9748 - val_loss: 0.0541 - val_accuracy: 0.9527\n",
      "Epoch 1853/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9748\n",
      "Epoch 1853: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0376 - accuracy: 0.9748 - val_loss: 0.0541 - val_accuracy: 0.9527\n",
      "Epoch 1854/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9748\n",
      "Epoch 1854: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0376 - accuracy: 0.9748 - val_loss: 0.0541 - val_accuracy: 0.9527\n",
      "Epoch 1855/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9748\n",
      "Epoch 1855: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0376 - accuracy: 0.9748 - val_loss: 0.0540 - val_accuracy: 0.9527\n",
      "Epoch 1856/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9748\n",
      "Epoch 1856: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0376 - accuracy: 0.9748 - val_loss: 0.0540 - val_accuracy: 0.9527\n",
      "Epoch 1857/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9748\n",
      "Epoch 1857: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0375 - accuracy: 0.9748 - val_loss: 0.0540 - val_accuracy: 0.9527\n",
      "Epoch 1858/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9748\n",
      "Epoch 1858: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0375 - accuracy: 0.9748 - val_loss: 0.0540 - val_accuracy: 0.9527\n",
      "Epoch 1859/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9748\n",
      "Epoch 1859: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0375 - accuracy: 0.9748 - val_loss: 0.0539 - val_accuracy: 0.9527\n",
      "Epoch 1860/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9748\n",
      "Epoch 1860: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0375 - accuracy: 0.9748 - val_loss: 0.0539 - val_accuracy: 0.9527\n",
      "Epoch 1861/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9748\n",
      "Epoch 1861: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0375 - accuracy: 0.9748 - val_loss: 0.0539 - val_accuracy: 0.9527\n",
      "Epoch 1862/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9748\n",
      "Epoch 1862: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0374 - accuracy: 0.9748 - val_loss: 0.0539 - val_accuracy: 0.9527\n",
      "Epoch 1863/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9748\n",
      "Epoch 1863: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0374 - accuracy: 0.9748 - val_loss: 0.0538 - val_accuracy: 0.9527\n",
      "Epoch 1864/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9748\n",
      "Epoch 1864: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0374 - accuracy: 0.9748 - val_loss: 0.0538 - val_accuracy: 0.9527\n",
      "Epoch 1865/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9748\n",
      "Epoch 1865: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0374 - accuracy: 0.9748 - val_loss: 0.0538 - val_accuracy: 0.9527\n",
      "Epoch 1866/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9748\n",
      "Epoch 1866: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0374 - accuracy: 0.9748 - val_loss: 0.0538 - val_accuracy: 0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1867/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9748\n",
      "Epoch 1867: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0373 - accuracy: 0.9748 - val_loss: 0.0537 - val_accuracy: 0.9527\n",
      "Epoch 1868/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9748\n",
      "Epoch 1868: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0373 - accuracy: 0.9748 - val_loss: 0.0537 - val_accuracy: 0.9527\n",
      "Epoch 1869/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9748\n",
      "Epoch 1869: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0373 - accuracy: 0.9748 - val_loss: 0.0537 - val_accuracy: 0.9527\n",
      "Epoch 1870/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9748\n",
      "Epoch 1870: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0373 - accuracy: 0.9748 - val_loss: 0.0537 - val_accuracy: 0.9527\n",
      "Epoch 1871/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9748\n",
      "Epoch 1871: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0373 - accuracy: 0.9748 - val_loss: 0.0536 - val_accuracy: 0.9527\n",
      "Epoch 1872/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9748\n",
      "Epoch 1872: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0372 - accuracy: 0.9748 - val_loss: 0.0536 - val_accuracy: 0.9527\n",
      "Epoch 1873/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9748\n",
      "Epoch 1873: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0372 - accuracy: 0.9748 - val_loss: 0.0536 - val_accuracy: 0.9527\n",
      "Epoch 1874/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9748\n",
      "Epoch 1874: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0372 - accuracy: 0.9748 - val_loss: 0.0536 - val_accuracy: 0.9527\n",
      "Epoch 1875/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9748\n",
      "Epoch 1875: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0372 - accuracy: 0.9748 - val_loss: 0.0536 - val_accuracy: 0.9527\n",
      "Epoch 1876/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9748\n",
      "Epoch 1876: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0372 - accuracy: 0.9748 - val_loss: 0.0535 - val_accuracy: 0.9527\n",
      "Epoch 1877/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9748\n",
      "Epoch 1877: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0371 - accuracy: 0.9748 - val_loss: 0.0535 - val_accuracy: 0.9527\n",
      "Epoch 1878/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9748\n",
      "Epoch 1878: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0371 - accuracy: 0.9748 - val_loss: 0.0535 - val_accuracy: 0.9527\n",
      "Epoch 1879/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9748\n",
      "Epoch 1879: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0371 - accuracy: 0.9748 - val_loss: 0.0535 - val_accuracy: 0.9527\n",
      "Epoch 1880/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9748\n",
      "Epoch 1880: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0371 - accuracy: 0.9748 - val_loss: 0.0534 - val_accuracy: 0.9527\n",
      "Epoch 1881/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9748\n",
      "Epoch 1881: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0371 - accuracy: 0.9748 - val_loss: 0.0534 - val_accuracy: 0.9527\n",
      "Epoch 1882/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9748\n",
      "Epoch 1882: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0370 - accuracy: 0.9748 - val_loss: 0.0534 - val_accuracy: 0.9527\n",
      "Epoch 1883/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9748\n",
      "Epoch 1883: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0370 - accuracy: 0.9748 - val_loss: 0.0534 - val_accuracy: 0.9527\n",
      "Epoch 1884/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9748\n",
      "Epoch 1884: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0370 - accuracy: 0.9748 - val_loss: 0.0533 - val_accuracy: 0.9527\n",
      "Epoch 1885/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9748\n",
      "Epoch 1885: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0370 - accuracy: 0.9748 - val_loss: 0.0533 - val_accuracy: 0.9527\n",
      "Epoch 1886/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9748\n",
      "Epoch 1886: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0370 - accuracy: 0.9748 - val_loss: 0.0533 - val_accuracy: 0.9527\n",
      "Epoch 1887/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9748\n",
      "Epoch 1887: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0369 - accuracy: 0.9748 - val_loss: 0.0533 - val_accuracy: 0.9527\n",
      "Epoch 1888/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9748\n",
      "Epoch 1888: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0369 - accuracy: 0.9748 - val_loss: 0.0532 - val_accuracy: 0.9527\n",
      "Epoch 1889/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9758\n",
      "Epoch 1889: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0369 - accuracy: 0.9758 - val_loss: 0.0532 - val_accuracy: 0.9527\n",
      "Epoch 1890/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9758\n",
      "Epoch 1890: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0369 - accuracy: 0.9758 - val_loss: 0.0532 - val_accuracy: 0.9527\n",
      "Epoch 1891/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9758\n",
      "Epoch 1891: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0369 - accuracy: 0.9758 - val_loss: 0.0532 - val_accuracy: 0.9527\n",
      "Epoch 1892/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9758\n",
      "Epoch 1892: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0368 - accuracy: 0.9758 - val_loss: 0.0532 - val_accuracy: 0.9527\n",
      "Epoch 1893/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9758\n",
      "Epoch 1893: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0368 - accuracy: 0.9758 - val_loss: 0.0531 - val_accuracy: 0.9527\n",
      "Epoch 1894/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9758\n",
      "Epoch 1894: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0368 - accuracy: 0.9758 - val_loss: 0.0531 - val_accuracy: 0.9527\n",
      "Epoch 1895/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9758\n",
      "Epoch 1895: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0368 - accuracy: 0.9758 - val_loss: 0.0531 - val_accuracy: 0.9527\n",
      "Epoch 1896/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9758\n",
      "Epoch 1896: val_accuracy did not improve from 0.95270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0368 - accuracy: 0.9758 - val_loss: 0.0531 - val_accuracy: 0.9527\n",
      "Epoch 1897/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9758\n",
      "Epoch 1897: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0367 - accuracy: 0.9758 - val_loss: 0.0530 - val_accuracy: 0.9527\n",
      "Epoch 1898/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9758\n",
      "Epoch 1898: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0367 - accuracy: 0.9758 - val_loss: 0.0530 - val_accuracy: 0.9527\n",
      "Epoch 1899/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9758\n",
      "Epoch 1899: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0367 - accuracy: 0.9758 - val_loss: 0.0530 - val_accuracy: 0.9527\n",
      "Epoch 1900/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9758\n",
      "Epoch 1900: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0367 - accuracy: 0.9758 - val_loss: 0.0530 - val_accuracy: 0.9527\n",
      "Epoch 1901/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9758\n",
      "Epoch 1901: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0367 - accuracy: 0.9758 - val_loss: 0.0529 - val_accuracy: 0.9527\n",
      "Epoch 1902/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9758\n",
      "Epoch 1902: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0366 - accuracy: 0.9758 - val_loss: 0.0529 - val_accuracy: 0.9527\n",
      "Epoch 1903/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9758\n",
      "Epoch 1903: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0366 - accuracy: 0.9758 - val_loss: 0.0529 - val_accuracy: 0.9527\n",
      "Epoch 1904/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9758\n",
      "Epoch 1904: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0366 - accuracy: 0.9758 - val_loss: 0.0529 - val_accuracy: 0.9527\n",
      "Epoch 1905/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9758\n",
      "Epoch 1905: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0366 - accuracy: 0.9758 - val_loss: 0.0529 - val_accuracy: 0.9527\n",
      "Epoch 1906/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9758\n",
      "Epoch 1906: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0366 - accuracy: 0.9758 - val_loss: 0.0528 - val_accuracy: 0.9527\n",
      "Epoch 1907/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9758\n",
      "Epoch 1907: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0365 - accuracy: 0.9758 - val_loss: 0.0528 - val_accuracy: 0.9527\n",
      "Epoch 1908/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9758\n",
      "Epoch 1908: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0365 - accuracy: 0.9758 - val_loss: 0.0528 - val_accuracy: 0.9527\n",
      "Epoch 1909/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9758\n",
      "Epoch 1909: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0365 - accuracy: 0.9758 - val_loss: 0.0528 - val_accuracy: 0.9527\n",
      "Epoch 1910/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9758\n",
      "Epoch 1910: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0365 - accuracy: 0.9758 - val_loss: 0.0527 - val_accuracy: 0.9527\n",
      "Epoch 1911/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9758\n",
      "Epoch 1911: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0365 - accuracy: 0.9758 - val_loss: 0.0527 - val_accuracy: 0.9527\n",
      "Epoch 1912/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9758\n",
      "Epoch 1912: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0365 - accuracy: 0.9758 - val_loss: 0.0527 - val_accuracy: 0.9527\n",
      "Epoch 1913/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9758\n",
      "Epoch 1913: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0364 - accuracy: 0.9758 - val_loss: 0.0527 - val_accuracy: 0.9527\n",
      "Epoch 1914/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9758\n",
      "Epoch 1914: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0364 - accuracy: 0.9758 - val_loss: 0.0526 - val_accuracy: 0.9527\n",
      "Epoch 1915/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9758\n",
      "Epoch 1915: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0364 - accuracy: 0.9758 - val_loss: 0.0526 - val_accuracy: 0.9527\n",
      "Epoch 1916/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9758\n",
      "Epoch 1916: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0364 - accuracy: 0.9758 - val_loss: 0.0526 - val_accuracy: 0.9527\n",
      "Epoch 1917/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9758\n",
      "Epoch 1917: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0364 - accuracy: 0.9758 - val_loss: 0.0526 - val_accuracy: 0.9527\n",
      "Epoch 1918/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9758\n",
      "Epoch 1918: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0363 - accuracy: 0.9758 - val_loss: 0.0526 - val_accuracy: 0.9527\n",
      "Epoch 1919/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9758\n",
      "Epoch 1919: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0363 - accuracy: 0.9758 - val_loss: 0.0525 - val_accuracy: 0.9527\n",
      "Epoch 1920/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9758\n",
      "Epoch 1920: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0363 - accuracy: 0.9758 - val_loss: 0.0525 - val_accuracy: 0.9527\n",
      "Epoch 1921/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9758\n",
      "Epoch 1921: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0363 - accuracy: 0.9758 - val_loss: 0.0525 - val_accuracy: 0.9527\n",
      "Epoch 1922/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9758\n",
      "Epoch 1922: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0363 - accuracy: 0.9758 - val_loss: 0.0525 - val_accuracy: 0.9527\n",
      "Epoch 1923/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9758\n",
      "Epoch 1923: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0362 - accuracy: 0.9758 - val_loss: 0.0524 - val_accuracy: 0.9527\n",
      "Epoch 1924/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9758\n",
      "Epoch 1924: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0362 - accuracy: 0.9758 - val_loss: 0.0524 - val_accuracy: 0.9527\n",
      "Epoch 1925/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9758\n",
      "Epoch 1925: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0362 - accuracy: 0.9758 - val_loss: 0.0524 - val_accuracy: 0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1926/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9758\n",
      "Epoch 1926: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0362 - accuracy: 0.9758 - val_loss: 0.0524 - val_accuracy: 0.9527\n",
      "Epoch 1927/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9758\n",
      "Epoch 1927: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0362 - accuracy: 0.9758 - val_loss: 0.0524 - val_accuracy: 0.9527\n",
      "Epoch 1928/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9758\n",
      "Epoch 1928: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0361 - accuracy: 0.9758 - val_loss: 0.0523 - val_accuracy: 0.9527\n",
      "Epoch 1929/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9758\n",
      "Epoch 1929: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0361 - accuracy: 0.9758 - val_loss: 0.0523 - val_accuracy: 0.9527\n",
      "Epoch 1930/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9758\n",
      "Epoch 1930: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0361 - accuracy: 0.9758 - val_loss: 0.0523 - val_accuracy: 0.9527\n",
      "Epoch 1931/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9758\n",
      "Epoch 1931: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0361 - accuracy: 0.9758 - val_loss: 0.0523 - val_accuracy: 0.9527\n",
      "Epoch 1932/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9758\n",
      "Epoch 1932: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0361 - accuracy: 0.9758 - val_loss: 0.0522 - val_accuracy: 0.9527\n",
      "Epoch 1933/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9758\n",
      "Epoch 1933: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0361 - accuracy: 0.9758 - val_loss: 0.0522 - val_accuracy: 0.9527\n",
      "Epoch 1934/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9758\n",
      "Epoch 1934: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0360 - accuracy: 0.9758 - val_loss: 0.0522 - val_accuracy: 0.9527\n",
      "Epoch 1935/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9758\n",
      "Epoch 1935: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0360 - accuracy: 0.9758 - val_loss: 0.0522 - val_accuracy: 0.9527\n",
      "Epoch 1936/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9758\n",
      "Epoch 1936: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0360 - accuracy: 0.9758 - val_loss: 0.0521 - val_accuracy: 0.9527\n",
      "Epoch 1937/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9758\n",
      "Epoch 1937: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0360 - accuracy: 0.9758 - val_loss: 0.0521 - val_accuracy: 0.9527\n",
      "Epoch 1938/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9758\n",
      "Epoch 1938: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0360 - accuracy: 0.9758 - val_loss: 0.0521 - val_accuracy: 0.9527\n",
      "Epoch 1939/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9758\n",
      "Epoch 1939: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0359 - accuracy: 0.9758 - val_loss: 0.0521 - val_accuracy: 0.9527\n",
      "Epoch 1940/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9758\n",
      "Epoch 1940: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0359 - accuracy: 0.9758 - val_loss: 0.0521 - val_accuracy: 0.9527\n",
      "Epoch 1941/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9758\n",
      "Epoch 1941: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0359 - accuracy: 0.9758 - val_loss: 0.0520 - val_accuracy: 0.9527\n",
      "Epoch 1942/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9758\n",
      "Epoch 1942: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0359 - accuracy: 0.9758 - val_loss: 0.0520 - val_accuracy: 0.9527\n",
      "Epoch 1943/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9758\n",
      "Epoch 1943: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0359 - accuracy: 0.9758 - val_loss: 0.0520 - val_accuracy: 0.9527\n",
      "Epoch 1944/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9758\n",
      "Epoch 1944: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0358 - accuracy: 0.9758 - val_loss: 0.0520 - val_accuracy: 0.9527\n",
      "Epoch 1945/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9758\n",
      "Epoch 1945: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0358 - accuracy: 0.9758 - val_loss: 0.0520 - val_accuracy: 0.9527\n",
      "Epoch 1946/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9758\n",
      "Epoch 1946: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0358 - accuracy: 0.9758 - val_loss: 0.0519 - val_accuracy: 0.9527\n",
      "Epoch 1947/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9758\n",
      "Epoch 1947: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0358 - accuracy: 0.9758 - val_loss: 0.0519 - val_accuracy: 0.9527\n",
      "Epoch 1948/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9758\n",
      "Epoch 1948: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0358 - accuracy: 0.9758 - val_loss: 0.0519 - val_accuracy: 0.9527\n",
      "Epoch 1949/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9758\n",
      "Epoch 1949: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0358 - accuracy: 0.9758 - val_loss: 0.0519 - val_accuracy: 0.9527\n",
      "Epoch 1950/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9758\n",
      "Epoch 1950: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0357 - accuracy: 0.9758 - val_loss: 0.0518 - val_accuracy: 0.9527\n",
      "Epoch 1951/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9758\n",
      "Epoch 1951: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0357 - accuracy: 0.9758 - val_loss: 0.0518 - val_accuracy: 0.9527\n",
      "Epoch 1952/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9758\n",
      "Epoch 1952: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0357 - accuracy: 0.9758 - val_loss: 0.0518 - val_accuracy: 0.9527\n",
      "Epoch 1953/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9758\n",
      "Epoch 1953: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0357 - accuracy: 0.9758 - val_loss: 0.0518 - val_accuracy: 0.9527\n",
      "Epoch 1954/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9758\n",
      "Epoch 1954: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0357 - accuracy: 0.9758 - val_loss: 0.0518 - val_accuracy: 0.9527\n",
      "Epoch 1955/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9758\n",
      "Epoch 1955: val_accuracy did not improve from 0.95270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0356 - accuracy: 0.9758 - val_loss: 0.0517 - val_accuracy: 0.9527\n",
      "Epoch 1956/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9758\n",
      "Epoch 1956: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0356 - accuracy: 0.9758 - val_loss: 0.0517 - val_accuracy: 0.9527\n",
      "Epoch 1957/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9758\n",
      "Epoch 1957: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0356 - accuracy: 0.9758 - val_loss: 0.0517 - val_accuracy: 0.9527\n",
      "Epoch 1958/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9758\n",
      "Epoch 1958: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0356 - accuracy: 0.9758 - val_loss: 0.0517 - val_accuracy: 0.9527\n",
      "Epoch 1959/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9758\n",
      "Epoch 1959: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0356 - accuracy: 0.9758 - val_loss: 0.0516 - val_accuracy: 0.9527\n",
      "Epoch 1960/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9758\n",
      "Epoch 1960: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0355 - accuracy: 0.9758 - val_loss: 0.0516 - val_accuracy: 0.9527\n",
      "Epoch 1961/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9758\n",
      "Epoch 1961: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0355 - accuracy: 0.9758 - val_loss: 0.0516 - val_accuracy: 0.9527\n",
      "Epoch 1962/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9758\n",
      "Epoch 1962: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0355 - accuracy: 0.9758 - val_loss: 0.0516 - val_accuracy: 0.9527\n",
      "Epoch 1963/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9758\n",
      "Epoch 1963: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0355 - accuracy: 0.9758 - val_loss: 0.0516 - val_accuracy: 0.9527\n",
      "Epoch 1964/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9758\n",
      "Epoch 1964: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0355 - accuracy: 0.9758 - val_loss: 0.0515 - val_accuracy: 0.9527\n",
      "Epoch 1965/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9758\n",
      "Epoch 1965: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0355 - accuracy: 0.9758 - val_loss: 0.0515 - val_accuracy: 0.9527\n",
      "Epoch 1966/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9758\n",
      "Epoch 1966: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0354 - accuracy: 0.9758 - val_loss: 0.0515 - val_accuracy: 0.9527\n",
      "Epoch 1967/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9758\n",
      "Epoch 1967: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0354 - accuracy: 0.9758 - val_loss: 0.0515 - val_accuracy: 0.9527\n",
      "Epoch 1968/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9758\n",
      "Epoch 1968: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0354 - accuracy: 0.9758 - val_loss: 0.0515 - val_accuracy: 0.9527\n",
      "Epoch 1969/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9758\n",
      "Epoch 1969: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0354 - accuracy: 0.9758 - val_loss: 0.0514 - val_accuracy: 0.9527\n",
      "Epoch 1970/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9758\n",
      "Epoch 1970: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0354 - accuracy: 0.9758 - val_loss: 0.0514 - val_accuracy: 0.9527\n",
      "Epoch 1971/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9758\n",
      "Epoch 1971: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0353 - accuracy: 0.9758 - val_loss: 0.0514 - val_accuracy: 0.9527\n",
      "Epoch 1972/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9758\n",
      "Epoch 1972: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0353 - accuracy: 0.9758 - val_loss: 0.0514 - val_accuracy: 0.9527\n",
      "Epoch 1973/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9758\n",
      "Epoch 1973: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0353 - accuracy: 0.9758 - val_loss: 0.0513 - val_accuracy: 0.9527\n",
      "Epoch 1974/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9758\n",
      "Epoch 1974: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0353 - accuracy: 0.9758 - val_loss: 0.0513 - val_accuracy: 0.9527\n",
      "Epoch 1975/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9758\n",
      "Epoch 1975: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0353 - accuracy: 0.9758 - val_loss: 0.0513 - val_accuracy: 0.9527\n",
      "Epoch 1976/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9758\n",
      "Epoch 1976: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0353 - accuracy: 0.9758 - val_loss: 0.0513 - val_accuracy: 0.9527\n",
      "Epoch 1977/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9768\n",
      "Epoch 1977: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0352 - accuracy: 0.9768 - val_loss: 0.0513 - val_accuracy: 0.9527\n",
      "Epoch 1978/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9768\n",
      "Epoch 1978: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0352 - accuracy: 0.9768 - val_loss: 0.0512 - val_accuracy: 0.9527\n",
      "Epoch 1979/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9768\n",
      "Epoch 1979: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0352 - accuracy: 0.9768 - val_loss: 0.0512 - val_accuracy: 0.9527\n",
      "Epoch 1980/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9768\n",
      "Epoch 1980: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0352 - accuracy: 0.9768 - val_loss: 0.0512 - val_accuracy: 0.9527\n",
      "Epoch 1981/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9768\n",
      "Epoch 1981: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0352 - accuracy: 0.9768 - val_loss: 0.0512 - val_accuracy: 0.9527\n",
      "Epoch 1982/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9768\n",
      "Epoch 1982: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0351 - accuracy: 0.9768 - val_loss: 0.0512 - val_accuracy: 0.9527\n",
      "Epoch 1983/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9768\n",
      "Epoch 1983: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0351 - accuracy: 0.9768 - val_loss: 0.0511 - val_accuracy: 0.9527\n",
      "Epoch 1984/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9768\n",
      "Epoch 1984: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0351 - accuracy: 0.9768 - val_loss: 0.0511 - val_accuracy: 0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1985/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9768\n",
      "Epoch 1985: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0351 - accuracy: 0.9768 - val_loss: 0.0511 - val_accuracy: 0.9527\n",
      "Epoch 1986/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9768\n",
      "Epoch 1986: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0351 - accuracy: 0.9768 - val_loss: 0.0511 - val_accuracy: 0.9527\n",
      "Epoch 1987/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9768\n",
      "Epoch 1987: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0351 - accuracy: 0.9768 - val_loss: 0.0510 - val_accuracy: 0.9527\n",
      "Epoch 1988/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9768\n",
      "Epoch 1988: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0350 - accuracy: 0.9768 - val_loss: 0.0510 - val_accuracy: 0.9527\n",
      "Epoch 1989/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9768\n",
      "Epoch 1989: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0350 - accuracy: 0.9768 - val_loss: 0.0510 - val_accuracy: 0.9527\n",
      "Epoch 1990/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9768\n",
      "Epoch 1990: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0350 - accuracy: 0.9768 - val_loss: 0.0510 - val_accuracy: 0.9527\n",
      "Epoch 1991/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9768\n",
      "Epoch 1991: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0350 - accuracy: 0.9768 - val_loss: 0.0510 - val_accuracy: 0.9527\n",
      "Epoch 1992/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9768\n",
      "Epoch 1992: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0350 - accuracy: 0.9768 - val_loss: 0.0509 - val_accuracy: 0.9527\n",
      "Epoch 1993/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9768\n",
      "Epoch 1993: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0349 - accuracy: 0.9768 - val_loss: 0.0509 - val_accuracy: 0.9527\n",
      "Epoch 1994/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9768\n",
      "Epoch 1994: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0349 - accuracy: 0.9768 - val_loss: 0.0509 - val_accuracy: 0.9527\n",
      "Epoch 1995/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9768\n",
      "Epoch 1995: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0349 - accuracy: 0.9768 - val_loss: 0.0509 - val_accuracy: 0.9527\n",
      "Epoch 1996/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9768\n",
      "Epoch 1996: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0349 - accuracy: 0.9768 - val_loss: 0.0509 - val_accuracy: 0.9527\n",
      "Epoch 1997/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9768\n",
      "Epoch 1997: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0349 - accuracy: 0.9768 - val_loss: 0.0508 - val_accuracy: 0.9527\n",
      "Epoch 1998/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9768\n",
      "Epoch 1998: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0349 - accuracy: 0.9768 - val_loss: 0.0508 - val_accuracy: 0.9527\n",
      "Epoch 1999/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9768\n",
      "Epoch 1999: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0348 - accuracy: 0.9768 - val_loss: 0.0508 - val_accuracy: 0.9527\n",
      "Epoch 2000/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9768\n",
      "Epoch 2000: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0348 - accuracy: 0.9768 - val_loss: 0.0508 - val_accuracy: 0.9527\n",
      "Epoch 2001/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9768\n",
      "Epoch 2001: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0348 - accuracy: 0.9768 - val_loss: 0.0508 - val_accuracy: 0.9527\n",
      "Epoch 2002/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9768\n",
      "Epoch 2002: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0348 - accuracy: 0.9768 - val_loss: 0.0507 - val_accuracy: 0.9527\n",
      "Epoch 2003/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9768\n",
      "Epoch 2003: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0348 - accuracy: 0.9768 - val_loss: 0.0507 - val_accuracy: 0.9527\n",
      "Epoch 2004/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9768\n",
      "Epoch 2004: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0348 - accuracy: 0.9768 - val_loss: 0.0507 - val_accuracy: 0.9527\n",
      "Epoch 2005/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9768\n",
      "Epoch 2005: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0347 - accuracy: 0.9768 - val_loss: 0.0507 - val_accuracy: 0.9527\n",
      "Epoch 2006/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9768\n",
      "Epoch 2006: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0347 - accuracy: 0.9768 - val_loss: 0.0506 - val_accuracy: 0.9527\n",
      "Epoch 2007/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9768\n",
      "Epoch 2007: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0347 - accuracy: 0.9768 - val_loss: 0.0506 - val_accuracy: 0.9527\n",
      "Epoch 2008/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9768\n",
      "Epoch 2008: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0347 - accuracy: 0.9768 - val_loss: 0.0506 - val_accuracy: 0.9527\n",
      "Epoch 2009/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9768\n",
      "Epoch 2009: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0347 - accuracy: 0.9768 - val_loss: 0.0506 - val_accuracy: 0.9527\n",
      "Epoch 2010/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9768\n",
      "Epoch 2010: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0346 - accuracy: 0.9768 - val_loss: 0.0506 - val_accuracy: 0.9527\n",
      "Epoch 2011/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9768\n",
      "Epoch 2011: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0346 - accuracy: 0.9768 - val_loss: 0.0505 - val_accuracy: 0.9527\n",
      "Epoch 2012/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9768\n",
      "Epoch 2012: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0346 - accuracy: 0.9768 - val_loss: 0.0505 - val_accuracy: 0.9527\n",
      "Epoch 2013/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9768\n",
      "Epoch 2013: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0346 - accuracy: 0.9768 - val_loss: 0.0505 - val_accuracy: 0.9527\n",
      "Epoch 2014/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9768\n",
      "Epoch 2014: val_accuracy did not improve from 0.95270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0346 - accuracy: 0.9768 - val_loss: 0.0505 - val_accuracy: 0.9527\n",
      "Epoch 2015/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9768\n",
      "Epoch 2015: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0346 - accuracy: 0.9768 - val_loss: 0.0505 - val_accuracy: 0.9527\n",
      "Epoch 2016/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9768\n",
      "Epoch 2016: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0345 - accuracy: 0.9768 - val_loss: 0.0504 - val_accuracy: 0.9527\n",
      "Epoch 2017/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9768\n",
      "Epoch 2017: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0345 - accuracy: 0.9768 - val_loss: 0.0504 - val_accuracy: 0.9527\n",
      "Epoch 2018/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9768\n",
      "Epoch 2018: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0345 - accuracy: 0.9768 - val_loss: 0.0504 - val_accuracy: 0.9527\n",
      "Epoch 2019/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9768\n",
      "Epoch 2019: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0345 - accuracy: 0.9768 - val_loss: 0.0504 - val_accuracy: 0.9527\n",
      "Epoch 2020/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9768\n",
      "Epoch 2020: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0345 - accuracy: 0.9768 - val_loss: 0.0504 - val_accuracy: 0.9527\n",
      "Epoch 2021/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9768\n",
      "Epoch 2021: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0345 - accuracy: 0.9768 - val_loss: 0.0503 - val_accuracy: 0.9527\n",
      "Epoch 2022/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9768\n",
      "Epoch 2022: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0344 - accuracy: 0.9768 - val_loss: 0.0503 - val_accuracy: 0.9527\n",
      "Epoch 2023/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9768\n",
      "Epoch 2023: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0344 - accuracy: 0.9768 - val_loss: 0.0503 - val_accuracy: 0.9527\n",
      "Epoch 2024/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9768\n",
      "Epoch 2024: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0344 - accuracy: 0.9768 - val_loss: 0.0503 - val_accuracy: 0.9527\n",
      "Epoch 2025/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9768\n",
      "Epoch 2025: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0344 - accuracy: 0.9768 - val_loss: 0.0503 - val_accuracy: 0.9527\n",
      "Epoch 2026/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9768\n",
      "Epoch 2026: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0344 - accuracy: 0.9768 - val_loss: 0.0502 - val_accuracy: 0.9527\n",
      "Epoch 2027/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9768\n",
      "Epoch 2027: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0343 - accuracy: 0.9768 - val_loss: 0.0502 - val_accuracy: 0.9527\n",
      "Epoch 2028/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9768\n",
      "Epoch 2028: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0343 - accuracy: 0.9768 - val_loss: 0.0502 - val_accuracy: 0.9527\n",
      "Epoch 2029/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9768\n",
      "Epoch 2029: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0343 - accuracy: 0.9768 - val_loss: 0.0502 - val_accuracy: 0.9527\n",
      "Epoch 2030/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9768\n",
      "Epoch 2030: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0343 - accuracy: 0.9768 - val_loss: 0.0502 - val_accuracy: 0.9527\n",
      "Epoch 2031/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9768\n",
      "Epoch 2031: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0343 - accuracy: 0.9768 - val_loss: 0.0501 - val_accuracy: 0.9527\n",
      "Epoch 2032/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9768\n",
      "Epoch 2032: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0343 - accuracy: 0.9768 - val_loss: 0.0501 - val_accuracy: 0.9527\n",
      "Epoch 2033/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9768\n",
      "Epoch 2033: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0342 - accuracy: 0.9768 - val_loss: 0.0501 - val_accuracy: 0.9527\n",
      "Epoch 2034/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9758\n",
      "Epoch 2034: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0342 - accuracy: 0.9758 - val_loss: 0.0501 - val_accuracy: 0.9527\n",
      "Epoch 2035/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9758\n",
      "Epoch 2035: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0342 - accuracy: 0.9758 - val_loss: 0.0501 - val_accuracy: 0.9527\n",
      "Epoch 2036/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9758\n",
      "Epoch 2036: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0342 - accuracy: 0.9758 - val_loss: 0.0500 - val_accuracy: 0.9527\n",
      "Epoch 2037/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9758\n",
      "Epoch 2037: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0342 - accuracy: 0.9758 - val_loss: 0.0500 - val_accuracy: 0.9527\n",
      "Epoch 2038/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9758\n",
      "Epoch 2038: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0342 - accuracy: 0.9758 - val_loss: 0.0500 - val_accuracy: 0.9527\n",
      "Epoch 2039/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9758\n",
      "Epoch 2039: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0341 - accuracy: 0.9758 - val_loss: 0.0500 - val_accuracy: 0.9527\n",
      "Epoch 2040/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9758\n",
      "Epoch 2040: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0341 - accuracy: 0.9758 - val_loss: 0.0500 - val_accuracy: 0.9527\n",
      "Epoch 2041/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9758\n",
      "Epoch 2041: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0341 - accuracy: 0.9758 - val_loss: 0.0499 - val_accuracy: 0.9527\n",
      "Epoch 2042/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9758\n",
      "Epoch 2042: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0341 - accuracy: 0.9758 - val_loss: 0.0499 - val_accuracy: 0.9527\n",
      "Epoch 2043/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9758\n",
      "Epoch 2043: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0341 - accuracy: 0.9758 - val_loss: 0.0499 - val_accuracy: 0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2044/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9758\n",
      "Epoch 2044: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0341 - accuracy: 0.9758 - val_loss: 0.0499 - val_accuracy: 0.9527\n",
      "Epoch 2045/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9758\n",
      "Epoch 2045: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0340 - accuracy: 0.9758 - val_loss: 0.0499 - val_accuracy: 0.9527\n",
      "Epoch 2046/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9758\n",
      "Epoch 2046: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0340 - accuracy: 0.9758 - val_loss: 0.0498 - val_accuracy: 0.9527\n",
      "Epoch 2047/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9758\n",
      "Epoch 2047: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0340 - accuracy: 0.9758 - val_loss: 0.0498 - val_accuracy: 0.9527\n",
      "Epoch 2048/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9758\n",
      "Epoch 2048: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0340 - accuracy: 0.9758 - val_loss: 0.0498 - val_accuracy: 0.9527\n",
      "Epoch 2049/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9758\n",
      "Epoch 2049: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0340 - accuracy: 0.9758 - val_loss: 0.0498 - val_accuracy: 0.9527\n",
      "Epoch 2050/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9758\n",
      "Epoch 2050: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0340 - accuracy: 0.9758 - val_loss: 0.0498 - val_accuracy: 0.9527\n",
      "Epoch 2051/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9758\n",
      "Epoch 2051: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0339 - accuracy: 0.9758 - val_loss: 0.0497 - val_accuracy: 0.9527\n",
      "Epoch 2052/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9758\n",
      "Epoch 2052: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0339 - accuracy: 0.9758 - val_loss: 0.0497 - val_accuracy: 0.9527\n",
      "Epoch 2053/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9768\n",
      "Epoch 2053: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0339 - accuracy: 0.9768 - val_loss: 0.0497 - val_accuracy: 0.9527\n",
      "Epoch 2054/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9768\n",
      "Epoch 2054: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0339 - accuracy: 0.9768 - val_loss: 0.0497 - val_accuracy: 0.9527\n",
      "Epoch 2055/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9768\n",
      "Epoch 2055: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0339 - accuracy: 0.9768 - val_loss: 0.0497 - val_accuracy: 0.9527\n",
      "Epoch 2056/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9768\n",
      "Epoch 2056: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0338 - accuracy: 0.9768 - val_loss: 0.0496 - val_accuracy: 0.9527\n",
      "Epoch 2057/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9768\n",
      "Epoch 2057: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0338 - accuracy: 0.9768 - val_loss: 0.0496 - val_accuracy: 0.9527\n",
      "Epoch 2058/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9768\n",
      "Epoch 2058: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0338 - accuracy: 0.9768 - val_loss: 0.0496 - val_accuracy: 0.9527\n",
      "Epoch 2059/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9768\n",
      "Epoch 2059: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0338 - accuracy: 0.9768 - val_loss: 0.0496 - val_accuracy: 0.9527\n",
      "Epoch 2060/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9768\n",
      "Epoch 2060: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0338 - accuracy: 0.9768 - val_loss: 0.0496 - val_accuracy: 0.9527\n",
      "Epoch 2061/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9768\n",
      "Epoch 2061: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0338 - accuracy: 0.9768 - val_loss: 0.0495 - val_accuracy: 0.9527\n",
      "Epoch 2062/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9768\n",
      "Epoch 2062: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0337 - accuracy: 0.9768 - val_loss: 0.0495 - val_accuracy: 0.9527\n",
      "Epoch 2063/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9768\n",
      "Epoch 2063: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0337 - accuracy: 0.9768 - val_loss: 0.0495 - val_accuracy: 0.9527\n",
      "Epoch 2064/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9768\n",
      "Epoch 2064: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0337 - accuracy: 0.9768 - val_loss: 0.0495 - val_accuracy: 0.9527\n",
      "Epoch 2065/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9768\n",
      "Epoch 2065: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0337 - accuracy: 0.9768 - val_loss: 0.0495 - val_accuracy: 0.9527\n",
      "Epoch 2066/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9768\n",
      "Epoch 2066: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0337 - accuracy: 0.9768 - val_loss: 0.0494 - val_accuracy: 0.9527\n",
      "Epoch 2067/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9768\n",
      "Epoch 2067: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0337 - accuracy: 0.9768 - val_loss: 0.0494 - val_accuracy: 0.9527\n",
      "Epoch 2068/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9768\n",
      "Epoch 2068: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0336 - accuracy: 0.9768 - val_loss: 0.0494 - val_accuracy: 0.9527\n",
      "Epoch 2069/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9768\n",
      "Epoch 2069: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0336 - accuracy: 0.9768 - val_loss: 0.0494 - val_accuracy: 0.9527\n",
      "Epoch 2070/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9768\n",
      "Epoch 2070: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0336 - accuracy: 0.9768 - val_loss: 0.0494 - val_accuracy: 0.9527\n",
      "Epoch 2071/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9768\n",
      "Epoch 2071: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0336 - accuracy: 0.9768 - val_loss: 0.0493 - val_accuracy: 0.9527\n",
      "Epoch 2072/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9768\n",
      "Epoch 2072: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0336 - accuracy: 0.9768 - val_loss: 0.0493 - val_accuracy: 0.9527\n",
      "Epoch 2073/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9768\n",
      "Epoch 2073: val_accuracy did not improve from 0.95270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0336 - accuracy: 0.9768 - val_loss: 0.0493 - val_accuracy: 0.9527\n",
      "Epoch 2074/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9768\n",
      "Epoch 2074: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0335 - accuracy: 0.9768 - val_loss: 0.0493 - val_accuracy: 0.9527\n",
      "Epoch 2075/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9768\n",
      "Epoch 2075: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0335 - accuracy: 0.9768 - val_loss: 0.0493 - val_accuracy: 0.9527\n",
      "Epoch 2076/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9768\n",
      "Epoch 2076: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0335 - accuracy: 0.9768 - val_loss: 0.0492 - val_accuracy: 0.9527\n",
      "Epoch 2077/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9768\n",
      "Epoch 2077: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0335 - accuracy: 0.9768 - val_loss: 0.0492 - val_accuracy: 0.9527\n",
      "Epoch 2078/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9768\n",
      "Epoch 2078: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0335 - accuracy: 0.9768 - val_loss: 0.0492 - val_accuracy: 0.9527\n",
      "Epoch 2079/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9768\n",
      "Epoch 2079: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0335 - accuracy: 0.9768 - val_loss: 0.0492 - val_accuracy: 0.9527\n",
      "Epoch 2080/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9768\n",
      "Epoch 2080: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0334 - accuracy: 0.9768 - val_loss: 0.0492 - val_accuracy: 0.9527\n",
      "Epoch 2081/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9768\n",
      "Epoch 2081: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0334 - accuracy: 0.9768 - val_loss: 0.0491 - val_accuracy: 0.9527\n",
      "Epoch 2082/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9768\n",
      "Epoch 2082: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0334 - accuracy: 0.9768 - val_loss: 0.0491 - val_accuracy: 0.9527\n",
      "Epoch 2083/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9768\n",
      "Epoch 2083: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0334 - accuracy: 0.9768 - val_loss: 0.0491 - val_accuracy: 0.9527\n",
      "Epoch 2084/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9768\n",
      "Epoch 2084: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0334 - accuracy: 0.9768 - val_loss: 0.0491 - val_accuracy: 0.9527\n",
      "Epoch 2085/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9768\n",
      "Epoch 2085: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0334 - accuracy: 0.9768 - val_loss: 0.0491 - val_accuracy: 0.9527\n",
      "Epoch 2086/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9768\n",
      "Epoch 2086: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0333 - accuracy: 0.9768 - val_loss: 0.0490 - val_accuracy: 0.9527\n",
      "Epoch 2087/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9768\n",
      "Epoch 2087: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0333 - accuracy: 0.9768 - val_loss: 0.0490 - val_accuracy: 0.9527\n",
      "Epoch 2088/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9768\n",
      "Epoch 2088: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0333 - accuracy: 0.9768 - val_loss: 0.0490 - val_accuracy: 0.9527\n",
      "Epoch 2089/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9768\n",
      "Epoch 2089: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0333 - accuracy: 0.9768 - val_loss: 0.0490 - val_accuracy: 0.9527\n",
      "Epoch 2090/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9768\n",
      "Epoch 2090: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0333 - accuracy: 0.9768 - val_loss: 0.0490 - val_accuracy: 0.9527\n",
      "Epoch 2091/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9768\n",
      "Epoch 2091: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0333 - accuracy: 0.9768 - val_loss: 0.0489 - val_accuracy: 0.9527\n",
      "Epoch 2092/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9768\n",
      "Epoch 2092: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0332 - accuracy: 0.9768 - val_loss: 0.0489 - val_accuracy: 0.9527\n",
      "Epoch 2093/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9768\n",
      "Epoch 2093: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0332 - accuracy: 0.9768 - val_loss: 0.0489 - val_accuracy: 0.9527\n",
      "Epoch 2094/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9768\n",
      "Epoch 2094: val_accuracy did not improve from 0.95270\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0332 - accuracy: 0.9768 - val_loss: 0.0489 - val_accuracy: 0.9527\n",
      "Epoch 2095/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9768\n",
      "Epoch 2095: val_accuracy improved from 0.95270 to 0.95495, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0332 - accuracy: 0.9768 - val_loss: 0.0489 - val_accuracy: 0.9550\n",
      "Epoch 2096/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9768\n",
      "Epoch 2096: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0332 - accuracy: 0.9768 - val_loss: 0.0489 - val_accuracy: 0.9550\n",
      "Epoch 2097/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9768\n",
      "Epoch 2097: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0332 - accuracy: 0.9768 - val_loss: 0.0488 - val_accuracy: 0.9550\n",
      "Epoch 2098/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9768\n",
      "Epoch 2098: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0331 - accuracy: 0.9768 - val_loss: 0.0488 - val_accuracy: 0.9550\n",
      "Epoch 2099/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9768\n",
      "Epoch 2099: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0331 - accuracy: 0.9768 - val_loss: 0.0488 - val_accuracy: 0.9550\n",
      "Epoch 2100/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9768\n",
      "Epoch 2100: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0331 - accuracy: 0.9768 - val_loss: 0.0488 - val_accuracy: 0.9550\n",
      "Epoch 2101/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9768\n",
      "Epoch 2101: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0331 - accuracy: 0.9768 - val_loss: 0.0488 - val_accuracy: 0.9550\n",
      "Epoch 2102/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9768\n",
      "Epoch 2102: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0331 - accuracy: 0.9768 - val_loss: 0.0487 - val_accuracy: 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2103/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9768\n",
      "Epoch 2103: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0331 - accuracy: 0.9768 - val_loss: 0.0487 - val_accuracy: 0.9550\n",
      "Epoch 2104/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9768\n",
      "Epoch 2104: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0330 - accuracy: 0.9768 - val_loss: 0.0487 - val_accuracy: 0.9550\n",
      "Epoch 2105/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9768\n",
      "Epoch 2105: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0330 - accuracy: 0.9768 - val_loss: 0.0487 - val_accuracy: 0.9550\n",
      "Epoch 2106/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9768\n",
      "Epoch 2106: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0330 - accuracy: 0.9768 - val_loss: 0.0487 - val_accuracy: 0.9550\n",
      "Epoch 2107/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9768\n",
      "Epoch 2107: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0330 - accuracy: 0.9768 - val_loss: 0.0486 - val_accuracy: 0.9550\n",
      "Epoch 2108/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9768\n",
      "Epoch 2108: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0330 - accuracy: 0.9768 - val_loss: 0.0486 - val_accuracy: 0.9550\n",
      "Epoch 2109/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9768\n",
      "Epoch 2109: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0330 - accuracy: 0.9768 - val_loss: 0.0486 - val_accuracy: 0.9550\n",
      "Epoch 2110/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9768\n",
      "Epoch 2110: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0330 - accuracy: 0.9768 - val_loss: 0.0486 - val_accuracy: 0.9550\n",
      "Epoch 2111/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9768\n",
      "Epoch 2111: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0329 - accuracy: 0.9768 - val_loss: 0.0486 - val_accuracy: 0.9550\n",
      "Epoch 2112/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9768\n",
      "Epoch 2112: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0329 - accuracy: 0.9768 - val_loss: 0.0486 - val_accuracy: 0.9550\n",
      "Epoch 2113/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9768\n",
      "Epoch 2113: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0329 - accuracy: 0.9768 - val_loss: 0.0485 - val_accuracy: 0.9550\n",
      "Epoch 2114/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9768\n",
      "Epoch 2114: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0329 - accuracy: 0.9768 - val_loss: 0.0485 - val_accuracy: 0.9550\n",
      "Epoch 2115/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9768\n",
      "Epoch 2115: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0329 - accuracy: 0.9768 - val_loss: 0.0485 - val_accuracy: 0.9550\n",
      "Epoch 2116/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9768\n",
      "Epoch 2116: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0329 - accuracy: 0.9768 - val_loss: 0.0485 - val_accuracy: 0.9550\n",
      "Epoch 2117/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9768\n",
      "Epoch 2117: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0328 - accuracy: 0.9768 - val_loss: 0.0485 - val_accuracy: 0.9550\n",
      "Epoch 2118/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9768\n",
      "Epoch 2118: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0328 - accuracy: 0.9768 - val_loss: 0.0484 - val_accuracy: 0.9550\n",
      "Epoch 2119/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9768\n",
      "Epoch 2119: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0328 - accuracy: 0.9768 - val_loss: 0.0484 - val_accuracy: 0.9550\n",
      "Epoch 2120/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9768\n",
      "Epoch 2120: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0328 - accuracy: 0.9768 - val_loss: 0.0484 - val_accuracy: 0.9550\n",
      "Epoch 2121/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9768\n",
      "Epoch 2121: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0328 - accuracy: 0.9768 - val_loss: 0.0484 - val_accuracy: 0.9550\n",
      "Epoch 2122/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9768\n",
      "Epoch 2122: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0328 - accuracy: 0.9768 - val_loss: 0.0484 - val_accuracy: 0.9550\n",
      "Epoch 2123/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9768\n",
      "Epoch 2123: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0327 - accuracy: 0.9768 - val_loss: 0.0483 - val_accuracy: 0.9550\n",
      "Epoch 2124/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9768\n",
      "Epoch 2124: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0327 - accuracy: 0.9768 - val_loss: 0.0483 - val_accuracy: 0.9550\n",
      "Epoch 2125/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9777\n",
      "Epoch 2125: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0327 - accuracy: 0.9777 - val_loss: 0.0483 - val_accuracy: 0.9550\n",
      "Epoch 2126/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9777\n",
      "Epoch 2126: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0327 - accuracy: 0.9777 - val_loss: 0.0483 - val_accuracy: 0.9550\n",
      "Epoch 2127/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9777\n",
      "Epoch 2127: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0327 - accuracy: 0.9777 - val_loss: 0.0483 - val_accuracy: 0.9550\n",
      "Epoch 2128/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9777\n",
      "Epoch 2128: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0327 - accuracy: 0.9777 - val_loss: 0.0483 - val_accuracy: 0.9550\n",
      "Epoch 2129/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9777\n",
      "Epoch 2129: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0326 - accuracy: 0.9777 - val_loss: 0.0482 - val_accuracy: 0.9550\n",
      "Epoch 2130/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9777\n",
      "Epoch 2130: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0326 - accuracy: 0.9777 - val_loss: 0.0482 - val_accuracy: 0.9550\n",
      "Epoch 2131/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9777\n",
      "Epoch 2131: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0326 - accuracy: 0.9777 - val_loss: 0.0482 - val_accuracy: 0.9550\n",
      "Epoch 2132/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9777\n",
      "Epoch 2132: val_accuracy did not improve from 0.95495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0326 - accuracy: 0.9777 - val_loss: 0.0482 - val_accuracy: 0.9550\n",
      "Epoch 2133/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9777\n",
      "Epoch 2133: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0326 - accuracy: 0.9777 - val_loss: 0.0482 - val_accuracy: 0.9550\n",
      "Epoch 2134/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9777\n",
      "Epoch 2134: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0326 - accuracy: 0.9777 - val_loss: 0.0481 - val_accuracy: 0.9550\n",
      "Epoch 2135/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9777\n",
      "Epoch 2135: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0325 - accuracy: 0.9777 - val_loss: 0.0481 - val_accuracy: 0.9550\n",
      "Epoch 2136/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9777\n",
      "Epoch 2136: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0325 - accuracy: 0.9777 - val_loss: 0.0481 - val_accuracy: 0.9550\n",
      "Epoch 2137/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9777\n",
      "Epoch 2137: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0325 - accuracy: 0.9777 - val_loss: 0.0481 - val_accuracy: 0.9550\n",
      "Epoch 2138/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9777\n",
      "Epoch 2138: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0325 - accuracy: 0.9777 - val_loss: 0.0481 - val_accuracy: 0.9550\n",
      "Epoch 2139/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9777\n",
      "Epoch 2139: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0325 - accuracy: 0.9777 - val_loss: 0.0481 - val_accuracy: 0.9550\n",
      "Epoch 2140/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9777\n",
      "Epoch 2140: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0325 - accuracy: 0.9777 - val_loss: 0.0480 - val_accuracy: 0.9550\n",
      "Epoch 2141/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9777\n",
      "Epoch 2141: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0325 - accuracy: 0.9777 - val_loss: 0.0480 - val_accuracy: 0.9550\n",
      "Epoch 2142/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9777\n",
      "Epoch 2142: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0324 - accuracy: 0.9777 - val_loss: 0.0480 - val_accuracy: 0.9550\n",
      "Epoch 2143/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9777\n",
      "Epoch 2143: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0324 - accuracy: 0.9777 - val_loss: 0.0480 - val_accuracy: 0.9550\n",
      "Epoch 2144/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9777\n",
      "Epoch 2144: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0324 - accuracy: 0.9777 - val_loss: 0.0480 - val_accuracy: 0.9550\n",
      "Epoch 2145/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9777\n",
      "Epoch 2145: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0324 - accuracy: 0.9777 - val_loss: 0.0479 - val_accuracy: 0.9550\n",
      "Epoch 2146/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9777\n",
      "Epoch 2146: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0324 - accuracy: 0.9777 - val_loss: 0.0479 - val_accuracy: 0.9550\n",
      "Epoch 2147/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9777\n",
      "Epoch 2147: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0324 - accuracy: 0.9777 - val_loss: 0.0479 - val_accuracy: 0.9550\n",
      "Epoch 2148/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9777\n",
      "Epoch 2148: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0323 - accuracy: 0.9777 - val_loss: 0.0479 - val_accuracy: 0.9550\n",
      "Epoch 2149/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9777\n",
      "Epoch 2149: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0323 - accuracy: 0.9777 - val_loss: 0.0479 - val_accuracy: 0.9550\n",
      "Epoch 2150/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9777\n",
      "Epoch 2150: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0323 - accuracy: 0.9777 - val_loss: 0.0479 - val_accuracy: 0.9550\n",
      "Epoch 2151/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9777\n",
      "Epoch 2151: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0323 - accuracy: 0.9777 - val_loss: 0.0478 - val_accuracy: 0.9550\n",
      "Epoch 2152/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9777\n",
      "Epoch 2152: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0323 - accuracy: 0.9777 - val_loss: 0.0478 - val_accuracy: 0.9550\n",
      "Epoch 2153/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9777\n",
      "Epoch 2153: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0323 - accuracy: 0.9777 - val_loss: 0.0478 - val_accuracy: 0.9550\n",
      "Epoch 2154/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9777\n",
      "Epoch 2154: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0322 - accuracy: 0.9777 - val_loss: 0.0478 - val_accuracy: 0.9550\n",
      "Epoch 2155/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9777\n",
      "Epoch 2155: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0322 - accuracy: 0.9777 - val_loss: 0.0478 - val_accuracy: 0.9550\n",
      "Epoch 2156/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9777\n",
      "Epoch 2156: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0322 - accuracy: 0.9777 - val_loss: 0.0477 - val_accuracy: 0.9550\n",
      "Epoch 2157/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9777\n",
      "Epoch 2157: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0322 - accuracy: 0.9777 - val_loss: 0.0477 - val_accuracy: 0.9550\n",
      "Epoch 2158/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9777\n",
      "Epoch 2158: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0322 - accuracy: 0.9777 - val_loss: 0.0477 - val_accuracy: 0.9550\n",
      "Epoch 2159/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9777\n",
      "Epoch 2159: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0322 - accuracy: 0.9777 - val_loss: 0.0477 - val_accuracy: 0.9550\n",
      "Epoch 2160/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9777\n",
      "Epoch 2160: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0322 - accuracy: 0.9777 - val_loss: 0.0477 - val_accuracy: 0.9550\n",
      "Epoch 2161/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9777\n",
      "Epoch 2161: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0321 - accuracy: 0.9777 - val_loss: 0.0477 - val_accuracy: 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2162/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9777\n",
      "Epoch 2162: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0321 - accuracy: 0.9777 - val_loss: 0.0476 - val_accuracy: 0.9550\n",
      "Epoch 2163/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9777\n",
      "Epoch 2163: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0321 - accuracy: 0.9777 - val_loss: 0.0476 - val_accuracy: 0.9550\n",
      "Epoch 2164/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9777\n",
      "Epoch 2164: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0321 - accuracy: 0.9777 - val_loss: 0.0476 - val_accuracy: 0.9550\n",
      "Epoch 2165/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9777\n",
      "Epoch 2165: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0321 - accuracy: 0.9777 - val_loss: 0.0476 - val_accuracy: 0.9550\n",
      "Epoch 2166/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9777\n",
      "Epoch 2166: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0321 - accuracy: 0.9777 - val_loss: 0.0476 - val_accuracy: 0.9550\n",
      "Epoch 2167/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9777\n",
      "Epoch 2167: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0320 - accuracy: 0.9777 - val_loss: 0.0475 - val_accuracy: 0.9550\n",
      "Epoch 2168/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9777\n",
      "Epoch 2168: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0320 - accuracy: 0.9777 - val_loss: 0.0475 - val_accuracy: 0.9550\n",
      "Epoch 2169/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9777\n",
      "Epoch 2169: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0320 - accuracy: 0.9777 - val_loss: 0.0475 - val_accuracy: 0.9550\n",
      "Epoch 2170/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9777\n",
      "Epoch 2170: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0320 - accuracy: 0.9777 - val_loss: 0.0475 - val_accuracy: 0.9550\n",
      "Epoch 2171/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9777\n",
      "Epoch 2171: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0320 - accuracy: 0.9777 - val_loss: 0.0475 - val_accuracy: 0.9550\n",
      "Epoch 2172/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9777\n",
      "Epoch 2172: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0320 - accuracy: 0.9777 - val_loss: 0.0475 - val_accuracy: 0.9550\n",
      "Epoch 2173/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9777\n",
      "Epoch 2173: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0320 - accuracy: 0.9777 - val_loss: 0.0474 - val_accuracy: 0.9550\n",
      "Epoch 2174/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9777\n",
      "Epoch 2174: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0319 - accuracy: 0.9777 - val_loss: 0.0474 - val_accuracy: 0.9550\n",
      "Epoch 2175/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9777\n",
      "Epoch 2175: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0319 - accuracy: 0.9777 - val_loss: 0.0474 - val_accuracy: 0.9550\n",
      "Epoch 2176/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9777\n",
      "Epoch 2176: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0319 - accuracy: 0.9777 - val_loss: 0.0474 - val_accuracy: 0.9550\n",
      "Epoch 2177/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9777\n",
      "Epoch 2177: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0319 - accuracy: 0.9777 - val_loss: 0.0474 - val_accuracy: 0.9550\n",
      "Epoch 2178/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9777\n",
      "Epoch 2178: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0319 - accuracy: 0.9777 - val_loss: 0.0473 - val_accuracy: 0.9550\n",
      "Epoch 2179/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9777\n",
      "Epoch 2179: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0319 - accuracy: 0.9777 - val_loss: 0.0473 - val_accuracy: 0.9550\n",
      "Epoch 2180/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9777\n",
      "Epoch 2180: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0318 - accuracy: 0.9777 - val_loss: 0.0473 - val_accuracy: 0.9550\n",
      "Epoch 2181/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9777\n",
      "Epoch 2181: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0318 - accuracy: 0.9777 - val_loss: 0.0473 - val_accuracy: 0.9550\n",
      "Epoch 2182/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9777\n",
      "Epoch 2182: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0318 - accuracy: 0.9777 - val_loss: 0.0473 - val_accuracy: 0.9550\n",
      "Epoch 2183/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9777\n",
      "Epoch 2183: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0318 - accuracy: 0.9777 - val_loss: 0.0473 - val_accuracy: 0.9550\n",
      "Epoch 2184/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9777\n",
      "Epoch 2184: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0318 - accuracy: 0.9777 - val_loss: 0.0472 - val_accuracy: 0.9550\n",
      "Epoch 2185/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9777\n",
      "Epoch 2185: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0318 - accuracy: 0.9777 - val_loss: 0.0472 - val_accuracy: 0.9550\n",
      "Epoch 2186/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9777\n",
      "Epoch 2186: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0318 - accuracy: 0.9777 - val_loss: 0.0472 - val_accuracy: 0.9550\n",
      "Epoch 2187/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9777\n",
      "Epoch 2187: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0317 - accuracy: 0.9777 - val_loss: 0.0472 - val_accuracy: 0.9550\n",
      "Epoch 2188/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9777\n",
      "Epoch 2188: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0317 - accuracy: 0.9777 - val_loss: 0.0472 - val_accuracy: 0.9550\n",
      "Epoch 2189/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9777\n",
      "Epoch 2189: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0317 - accuracy: 0.9777 - val_loss: 0.0472 - val_accuracy: 0.9550\n",
      "Epoch 2190/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9777\n",
      "Epoch 2190: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0317 - accuracy: 0.9777 - val_loss: 0.0471 - val_accuracy: 0.9550\n",
      "Epoch 2191/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9777\n",
      "Epoch 2191: val_accuracy did not improve from 0.95495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0317 - accuracy: 0.9777 - val_loss: 0.0471 - val_accuracy: 0.9550\n",
      "Epoch 2192/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9777\n",
      "Epoch 2192: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0317 - accuracy: 0.9777 - val_loss: 0.0471 - val_accuracy: 0.9550\n",
      "Epoch 2193/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9777\n",
      "Epoch 2193: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0316 - accuracy: 0.9777 - val_loss: 0.0471 - val_accuracy: 0.9550\n",
      "Epoch 2194/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9777\n",
      "Epoch 2194: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0316 - accuracy: 0.9777 - val_loss: 0.0471 - val_accuracy: 0.9550\n",
      "Epoch 2195/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9777\n",
      "Epoch 2195: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0316 - accuracy: 0.9777 - val_loss: 0.0471 - val_accuracy: 0.9550\n",
      "Epoch 2196/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9777\n",
      "Epoch 2196: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0316 - accuracy: 0.9777 - val_loss: 0.0470 - val_accuracy: 0.9550\n",
      "Epoch 2197/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9777\n",
      "Epoch 2197: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0316 - accuracy: 0.9777 - val_loss: 0.0470 - val_accuracy: 0.9550\n",
      "Epoch 2198/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9777\n",
      "Epoch 2198: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0316 - accuracy: 0.9777 - val_loss: 0.0470 - val_accuracy: 0.9550\n",
      "Epoch 2199/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9777\n",
      "Epoch 2199: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0316 - accuracy: 0.9777 - val_loss: 0.0470 - val_accuracy: 0.9550\n",
      "Epoch 2200/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9777\n",
      "Epoch 2200: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0315 - accuracy: 0.9777 - val_loss: 0.0470 - val_accuracy: 0.9550\n",
      "Epoch 2201/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9777\n",
      "Epoch 2201: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0315 - accuracy: 0.9777 - val_loss: 0.0470 - val_accuracy: 0.9550\n",
      "Epoch 2202/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9777\n",
      "Epoch 2202: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0315 - accuracy: 0.9777 - val_loss: 0.0469 - val_accuracy: 0.9550\n",
      "Epoch 2203/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9777\n",
      "Epoch 2203: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0315 - accuracy: 0.9777 - val_loss: 0.0469 - val_accuracy: 0.9550\n",
      "Epoch 2204/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9777\n",
      "Epoch 2204: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0315 - accuracy: 0.9777 - val_loss: 0.0469 - val_accuracy: 0.9550\n",
      "Epoch 2205/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9777\n",
      "Epoch 2205: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0315 - accuracy: 0.9777 - val_loss: 0.0469 - val_accuracy: 0.9550\n",
      "Epoch 2206/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9777\n",
      "Epoch 2206: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0315 - accuracy: 0.9777 - val_loss: 0.0468 - val_accuracy: 0.9550\n",
      "Epoch 2207/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9777\n",
      "Epoch 2207: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0314 - accuracy: 0.9777 - val_loss: 0.0469 - val_accuracy: 0.9550\n",
      "Epoch 2208/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9777\n",
      "Epoch 2208: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0314 - accuracy: 0.9777 - val_loss: 0.0468 - val_accuracy: 0.9550\n",
      "Epoch 2209/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9777\n",
      "Epoch 2209: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0314 - accuracy: 0.9777 - val_loss: 0.0468 - val_accuracy: 0.9550\n",
      "Epoch 2210/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9777\n",
      "Epoch 2210: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0314 - accuracy: 0.9777 - val_loss: 0.0468 - val_accuracy: 0.9550\n",
      "Epoch 2211/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9777\n",
      "Epoch 2211: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0314 - accuracy: 0.9777 - val_loss: 0.0468 - val_accuracy: 0.9550\n",
      "Epoch 2212/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9777\n",
      "Epoch 2212: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0314 - accuracy: 0.9777 - val_loss: 0.0468 - val_accuracy: 0.9550\n",
      "Epoch 2213/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9777\n",
      "Epoch 2213: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0313 - accuracy: 0.9777 - val_loss: 0.0467 - val_accuracy: 0.9550\n",
      "Epoch 2214/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9777\n",
      "Epoch 2214: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0313 - accuracy: 0.9777 - val_loss: 0.0467 - val_accuracy: 0.9550\n",
      "Epoch 2215/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9777\n",
      "Epoch 2215: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0313 - accuracy: 0.9777 - val_loss: 0.0467 - val_accuracy: 0.9550\n",
      "Epoch 2216/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9777\n",
      "Epoch 2216: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0313 - accuracy: 0.9777 - val_loss: 0.0467 - val_accuracy: 0.9550\n",
      "Epoch 2217/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9777\n",
      "Epoch 2217: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0313 - accuracy: 0.9777 - val_loss: 0.0467 - val_accuracy: 0.9550\n",
      "Epoch 2218/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9777\n",
      "Epoch 2218: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0313 - accuracy: 0.9777 - val_loss: 0.0467 - val_accuracy: 0.9550\n",
      "Epoch 2219/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9777\n",
      "Epoch 2219: val_accuracy did not improve from 0.95495\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0313 - accuracy: 0.9777 - val_loss: 0.0466 - val_accuracy: 0.9550\n",
      "Epoch 2220/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9777\n",
      "Epoch 2220: val_accuracy improved from 0.95495 to 0.95721, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0312 - accuracy: 0.9777 - val_loss: 0.0466 - val_accuracy: 0.9572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2221/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9777\n",
      "Epoch 2221: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0312 - accuracy: 0.9777 - val_loss: 0.0466 - val_accuracy: 0.9572\n",
      "Epoch 2222/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9777\n",
      "Epoch 2222: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0312 - accuracy: 0.9777 - val_loss: 0.0466 - val_accuracy: 0.9572\n",
      "Epoch 2223/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9777\n",
      "Epoch 2223: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0312 - accuracy: 0.9777 - val_loss: 0.0466 - val_accuracy: 0.9572\n",
      "Epoch 2224/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9777\n",
      "Epoch 2224: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0312 - accuracy: 0.9777 - val_loss: 0.0466 - val_accuracy: 0.9572\n",
      "Epoch 2225/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9777\n",
      "Epoch 2225: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0312 - accuracy: 0.9777 - val_loss: 0.0465 - val_accuracy: 0.9572\n",
      "Epoch 2226/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9777\n",
      "Epoch 2226: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0312 - accuracy: 0.9777 - val_loss: 0.0465 - val_accuracy: 0.9572\n",
      "Epoch 2227/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9777\n",
      "Epoch 2227: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0311 - accuracy: 0.9777 - val_loss: 0.0465 - val_accuracy: 0.9572\n",
      "Epoch 2228/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9777\n",
      "Epoch 2228: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0311 - accuracy: 0.9777 - val_loss: 0.0465 - val_accuracy: 0.9572\n",
      "Epoch 2229/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9777\n",
      "Epoch 2229: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0311 - accuracy: 0.9777 - val_loss: 0.0465 - val_accuracy: 0.9572\n",
      "Epoch 2230/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9777\n",
      "Epoch 2230: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0311 - accuracy: 0.9777 - val_loss: 0.0465 - val_accuracy: 0.9572\n",
      "Epoch 2231/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9777\n",
      "Epoch 2231: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0311 - accuracy: 0.9777 - val_loss: 0.0464 - val_accuracy: 0.9572\n",
      "Epoch 2232/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9777\n",
      "Epoch 2232: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0311 - accuracy: 0.9777 - val_loss: 0.0464 - val_accuracy: 0.9572\n",
      "Epoch 2233/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9777\n",
      "Epoch 2233: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0310 - accuracy: 0.9777 - val_loss: 0.0464 - val_accuracy: 0.9572\n",
      "Epoch 2234/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9777\n",
      "Epoch 2234: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0310 - accuracy: 0.9777 - val_loss: 0.0464 - val_accuracy: 0.9572\n",
      "Epoch 2235/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9777\n",
      "Epoch 2235: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0310 - accuracy: 0.9777 - val_loss: 0.0464 - val_accuracy: 0.9572\n",
      "Epoch 2236/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9777\n",
      "Epoch 2236: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0310 - accuracy: 0.9777 - val_loss: 0.0463 - val_accuracy: 0.9572\n",
      "Epoch 2237/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9777\n",
      "Epoch 2237: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0310 - accuracy: 0.9777 - val_loss: 0.0463 - val_accuracy: 0.9572\n",
      "Epoch 2238/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9777\n",
      "Epoch 2238: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0310 - accuracy: 0.9777 - val_loss: 0.0463 - val_accuracy: 0.9572\n",
      "Epoch 2239/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9777\n",
      "Epoch 2239: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0310 - accuracy: 0.9777 - val_loss: 0.0463 - val_accuracy: 0.9572\n",
      "Epoch 2240/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9777\n",
      "Epoch 2240: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0309 - accuracy: 0.9777 - val_loss: 0.0463 - val_accuracy: 0.9572\n",
      "Epoch 2241/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9777\n",
      "Epoch 2241: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0309 - accuracy: 0.9777 - val_loss: 0.0463 - val_accuracy: 0.9572\n",
      "Epoch 2242/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9777\n",
      "Epoch 2242: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0309 - accuracy: 0.9777 - val_loss: 0.0463 - val_accuracy: 0.9572\n",
      "Epoch 2243/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9777\n",
      "Epoch 2243: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0309 - accuracy: 0.9777 - val_loss: 0.0462 - val_accuracy: 0.9572\n",
      "Epoch 2244/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9777\n",
      "Epoch 2244: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0309 - accuracy: 0.9777 - val_loss: 0.0462 - val_accuracy: 0.9572\n",
      "Epoch 2245/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9777\n",
      "Epoch 2245: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0309 - accuracy: 0.9777 - val_loss: 0.0462 - val_accuracy: 0.9572\n",
      "Epoch 2246/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9777\n",
      "Epoch 2246: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0309 - accuracy: 0.9777 - val_loss: 0.0462 - val_accuracy: 0.9572\n",
      "Epoch 2247/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9777\n",
      "Epoch 2247: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0308 - accuracy: 0.9777 - val_loss: 0.0462 - val_accuracy: 0.9572\n",
      "Epoch 2248/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9777\n",
      "Epoch 2248: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0308 - accuracy: 0.9777 - val_loss: 0.0462 - val_accuracy: 0.9572\n",
      "Epoch 2249/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9777\n",
      "Epoch 2249: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0308 - accuracy: 0.9777 - val_loss: 0.0461 - val_accuracy: 0.9572\n",
      "Epoch 2250/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9777\n",
      "Epoch 2250: val_accuracy did not improve from 0.95721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0308 - accuracy: 0.9777 - val_loss: 0.0461 - val_accuracy: 0.9572\n",
      "Epoch 2251/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9777\n",
      "Epoch 2251: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0308 - accuracy: 0.9777 - val_loss: 0.0461 - val_accuracy: 0.9572\n",
      "Epoch 2252/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9777\n",
      "Epoch 2252: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0308 - accuracy: 0.9777 - val_loss: 0.0461 - val_accuracy: 0.9572\n",
      "Epoch 2253/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9777\n",
      "Epoch 2253: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0308 - accuracy: 0.9777 - val_loss: 0.0461 - val_accuracy: 0.9572\n",
      "Epoch 2254/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9777\n",
      "Epoch 2254: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0307 - accuracy: 0.9777 - val_loss: 0.0461 - val_accuracy: 0.9572\n",
      "Epoch 2255/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9777\n",
      "Epoch 2255: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0307 - accuracy: 0.9777 - val_loss: 0.0460 - val_accuracy: 0.9572\n",
      "Epoch 2256/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9777\n",
      "Epoch 2256: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0307 - accuracy: 0.9777 - val_loss: 0.0460 - val_accuracy: 0.9572\n",
      "Epoch 2257/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9777\n",
      "Epoch 2257: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0307 - accuracy: 0.9777 - val_loss: 0.0460 - val_accuracy: 0.9572\n",
      "Epoch 2258/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9777\n",
      "Epoch 2258: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0307 - accuracy: 0.9777 - val_loss: 0.0460 - val_accuracy: 0.9572\n",
      "Epoch 2259/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9777\n",
      "Epoch 2259: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0307 - accuracy: 0.9777 - val_loss: 0.0460 - val_accuracy: 0.9572\n",
      "Epoch 2260/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9777\n",
      "Epoch 2260: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0307 - accuracy: 0.9777 - val_loss: 0.0460 - val_accuracy: 0.9572\n",
      "Epoch 2261/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9777\n",
      "Epoch 2261: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0306 - accuracy: 0.9777 - val_loss: 0.0459 - val_accuracy: 0.9572\n",
      "Epoch 2262/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9777\n",
      "Epoch 2262: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0306 - accuracy: 0.9777 - val_loss: 0.0459 - val_accuracy: 0.9572\n",
      "Epoch 2263/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9777\n",
      "Epoch 2263: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0306 - accuracy: 0.9777 - val_loss: 0.0459 - val_accuracy: 0.9572\n",
      "Epoch 2264/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9777\n",
      "Epoch 2264: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0306 - accuracy: 0.9777 - val_loss: 0.0459 - val_accuracy: 0.9572\n",
      "Epoch 2265/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9777\n",
      "Epoch 2265: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0306 - accuracy: 0.9777 - val_loss: 0.0459 - val_accuracy: 0.9572\n",
      "Epoch 2266/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9777\n",
      "Epoch 2266: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0306 - accuracy: 0.9777 - val_loss: 0.0459 - val_accuracy: 0.9572\n",
      "Epoch 2267/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9777\n",
      "Epoch 2267: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0306 - accuracy: 0.9777 - val_loss: 0.0458 - val_accuracy: 0.9572\n",
      "Epoch 2268/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9777\n",
      "Epoch 2268: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0305 - accuracy: 0.9777 - val_loss: 0.0458 - val_accuracy: 0.9572\n",
      "Epoch 2269/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9777\n",
      "Epoch 2269: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0305 - accuracy: 0.9777 - val_loss: 0.0458 - val_accuracy: 0.9572\n",
      "Epoch 2270/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9777\n",
      "Epoch 2270: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0305 - accuracy: 0.9777 - val_loss: 0.0458 - val_accuracy: 0.9572\n",
      "Epoch 2271/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9777\n",
      "Epoch 2271: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0305 - accuracy: 0.9777 - val_loss: 0.0458 - val_accuracy: 0.9572\n",
      "Epoch 2272/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9777\n",
      "Epoch 2272: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0305 - accuracy: 0.9777 - val_loss: 0.0458 - val_accuracy: 0.9572\n",
      "Epoch 2273/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9777\n",
      "Epoch 2273: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0305 - accuracy: 0.9777 - val_loss: 0.0457 - val_accuracy: 0.9572\n",
      "Epoch 2274/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9777\n",
      "Epoch 2274: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0305 - accuracy: 0.9777 - val_loss: 0.0457 - val_accuracy: 0.9572\n",
      "Epoch 2275/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9777\n",
      "Epoch 2275: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0304 - accuracy: 0.9777 - val_loss: 0.0457 - val_accuracy: 0.9572\n",
      "Epoch 2276/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9777\n",
      "Epoch 2276: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0304 - accuracy: 0.9777 - val_loss: 0.0457 - val_accuracy: 0.9572\n",
      "Epoch 2277/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9787\n",
      "Epoch 2277: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0304 - accuracy: 0.9787 - val_loss: 0.0457 - val_accuracy: 0.9572\n",
      "Epoch 2278/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9787\n",
      "Epoch 2278: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0304 - accuracy: 0.9787 - val_loss: 0.0457 - val_accuracy: 0.9572\n",
      "Epoch 2279/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9787\n",
      "Epoch 2279: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0304 - accuracy: 0.9787 - val_loss: 0.0456 - val_accuracy: 0.9572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2280/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9787\n",
      "Epoch 2280: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0304 - accuracy: 0.9787 - val_loss: 0.0456 - val_accuracy: 0.9572\n",
      "Epoch 2281/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9787\n",
      "Epoch 2281: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0304 - accuracy: 0.9787 - val_loss: 0.0456 - val_accuracy: 0.9572\n",
      "Epoch 2282/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9787\n",
      "Epoch 2282: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0303 - accuracy: 0.9787 - val_loss: 0.0456 - val_accuracy: 0.9572\n",
      "Epoch 2283/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9787\n",
      "Epoch 2283: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0303 - accuracy: 0.9787 - val_loss: 0.0456 - val_accuracy: 0.9572\n",
      "Epoch 2284/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9787\n",
      "Epoch 2284: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0303 - accuracy: 0.9787 - val_loss: 0.0456 - val_accuracy: 0.9572\n",
      "Epoch 2285/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9787\n",
      "Epoch 2285: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0303 - accuracy: 0.9787 - val_loss: 0.0455 - val_accuracy: 0.9572\n",
      "Epoch 2286/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9787\n",
      "Epoch 2286: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0303 - accuracy: 0.9787 - val_loss: 0.0455 - val_accuracy: 0.9572\n",
      "Epoch 2287/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9787\n",
      "Epoch 2287: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0303 - accuracy: 0.9787 - val_loss: 0.0455 - val_accuracy: 0.9572\n",
      "Epoch 2288/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9787\n",
      "Epoch 2288: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0303 - accuracy: 0.9787 - val_loss: 0.0455 - val_accuracy: 0.9572\n",
      "Epoch 2289/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9787\n",
      "Epoch 2289: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0302 - accuracy: 0.9787 - val_loss: 0.0455 - val_accuracy: 0.9572\n",
      "Epoch 2290/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9787\n",
      "Epoch 2290: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0302 - accuracy: 0.9787 - val_loss: 0.0455 - val_accuracy: 0.9572\n",
      "Epoch 2291/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9787\n",
      "Epoch 2291: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0302 - accuracy: 0.9787 - val_loss: 0.0455 - val_accuracy: 0.9572\n",
      "Epoch 2292/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9787\n",
      "Epoch 2292: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0302 - accuracy: 0.9787 - val_loss: 0.0454 - val_accuracy: 0.9572\n",
      "Epoch 2293/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9787\n",
      "Epoch 2293: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0302 - accuracy: 0.9787 - val_loss: 0.0454 - val_accuracy: 0.9572\n",
      "Epoch 2294/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9787\n",
      "Epoch 2294: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0302 - accuracy: 0.9787 - val_loss: 0.0454 - val_accuracy: 0.9572\n",
      "Epoch 2295/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9787\n",
      "Epoch 2295: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0302 - accuracy: 0.9787 - val_loss: 0.0454 - val_accuracy: 0.9572\n",
      "Epoch 2296/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9787\n",
      "Epoch 2296: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0301 - accuracy: 0.9787 - val_loss: 0.0454 - val_accuracy: 0.9572\n",
      "Epoch 2297/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9787\n",
      "Epoch 2297: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0301 - accuracy: 0.9787 - val_loss: 0.0454 - val_accuracy: 0.9572\n",
      "Epoch 2298/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9787\n",
      "Epoch 2298: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0301 - accuracy: 0.9787 - val_loss: 0.0453 - val_accuracy: 0.9572\n",
      "Epoch 2299/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9787\n",
      "Epoch 2299: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0301 - accuracy: 0.9787 - val_loss: 0.0453 - val_accuracy: 0.9572\n",
      "Epoch 2300/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9787\n",
      "Epoch 2300: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0301 - accuracy: 0.9787 - val_loss: 0.0453 - val_accuracy: 0.9572\n",
      "Epoch 2301/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9787\n",
      "Epoch 2301: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0301 - accuracy: 0.9787 - val_loss: 0.0453 - val_accuracy: 0.9572\n",
      "Epoch 2302/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9787\n",
      "Epoch 2302: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0301 - accuracy: 0.9787 - val_loss: 0.0453 - val_accuracy: 0.9572\n",
      "Epoch 2303/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9787\n",
      "Epoch 2303: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0300 - accuracy: 0.9787 - val_loss: 0.0453 - val_accuracy: 0.9572\n",
      "Epoch 2304/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9787\n",
      "Epoch 2304: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0300 - accuracy: 0.9787 - val_loss: 0.0452 - val_accuracy: 0.9572\n",
      "Epoch 2305/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9787\n",
      "Epoch 2305: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0300 - accuracy: 0.9787 - val_loss: 0.0452 - val_accuracy: 0.9572\n",
      "Epoch 2306/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9787\n",
      "Epoch 2306: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0300 - accuracy: 0.9787 - val_loss: 0.0452 - val_accuracy: 0.9572\n",
      "Epoch 2307/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9787\n",
      "Epoch 2307: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0300 - accuracy: 0.9787 - val_loss: 0.0452 - val_accuracy: 0.9572\n",
      "Epoch 2308/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9787\n",
      "Epoch 2308: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0300 - accuracy: 0.9787 - val_loss: 0.0452 - val_accuracy: 0.9572\n",
      "Epoch 2309/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9787\n",
      "Epoch 2309: val_accuracy did not improve from 0.95721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0300 - accuracy: 0.9787 - val_loss: 0.0452 - val_accuracy: 0.9572\n",
      "Epoch 2310/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9787\n",
      "Epoch 2310: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0299 - accuracy: 0.9787 - val_loss: 0.0452 - val_accuracy: 0.9572\n",
      "Epoch 2311/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9787\n",
      "Epoch 2311: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0299 - accuracy: 0.9787 - val_loss: 0.0451 - val_accuracy: 0.9572\n",
      "Epoch 2312/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9787\n",
      "Epoch 2312: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0299 - accuracy: 0.9787 - val_loss: 0.0451 - val_accuracy: 0.9572\n",
      "Epoch 2313/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9787\n",
      "Epoch 2313: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0299 - accuracy: 0.9787 - val_loss: 0.0451 - val_accuracy: 0.9572\n",
      "Epoch 2314/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9787\n",
      "Epoch 2314: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0299 - accuracy: 0.9787 - val_loss: 0.0451 - val_accuracy: 0.9572\n",
      "Epoch 2315/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9787\n",
      "Epoch 2315: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0299 - accuracy: 0.9787 - val_loss: 0.0451 - val_accuracy: 0.9572\n",
      "Epoch 2316/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9787\n",
      "Epoch 2316: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0299 - accuracy: 0.9787 - val_loss: 0.0451 - val_accuracy: 0.9572\n",
      "Epoch 2317/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9787\n",
      "Epoch 2317: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0299 - accuracy: 0.9787 - val_loss: 0.0450 - val_accuracy: 0.9572\n",
      "Epoch 2318/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9787\n",
      "Epoch 2318: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0298 - accuracy: 0.9787 - val_loss: 0.0450 - val_accuracy: 0.9572\n",
      "Epoch 2319/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9787\n",
      "Epoch 2319: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0298 - accuracy: 0.9787 - val_loss: 0.0450 - val_accuracy: 0.9572\n",
      "Epoch 2320/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9787\n",
      "Epoch 2320: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0298 - accuracy: 0.9787 - val_loss: 0.0450 - val_accuracy: 0.9572\n",
      "Epoch 2321/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9787\n",
      "Epoch 2321: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0298 - accuracy: 0.9787 - val_loss: 0.0450 - val_accuracy: 0.9572\n",
      "Epoch 2322/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9787\n",
      "Epoch 2322: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0298 - accuracy: 0.9787 - val_loss: 0.0450 - val_accuracy: 0.9572\n",
      "Epoch 2323/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9787\n",
      "Epoch 2323: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0298 - accuracy: 0.9787 - val_loss: 0.0450 - val_accuracy: 0.9572\n",
      "Epoch 2324/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9787\n",
      "Epoch 2324: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0298 - accuracy: 0.9787 - val_loss: 0.0449 - val_accuracy: 0.9572\n",
      "Epoch 2325/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9787\n",
      "Epoch 2325: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0297 - accuracy: 0.9787 - val_loss: 0.0449 - val_accuracy: 0.9572\n",
      "Epoch 2326/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9787\n",
      "Epoch 2326: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0297 - accuracy: 0.9787 - val_loss: 0.0449 - val_accuracy: 0.9572\n",
      "Epoch 2327/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9787\n",
      "Epoch 2327: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0297 - accuracy: 0.9787 - val_loss: 0.0449 - val_accuracy: 0.9572\n",
      "Epoch 2328/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9787\n",
      "Epoch 2328: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0297 - accuracy: 0.9787 - val_loss: 0.0449 - val_accuracy: 0.9572\n",
      "Epoch 2329/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9787\n",
      "Epoch 2329: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0297 - accuracy: 0.9787 - val_loss: 0.0449 - val_accuracy: 0.9572\n",
      "Epoch 2330/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9787\n",
      "Epoch 2330: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0297 - accuracy: 0.9787 - val_loss: 0.0448 - val_accuracy: 0.9572\n",
      "Epoch 2331/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9787\n",
      "Epoch 2331: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0297 - accuracy: 0.9787 - val_loss: 0.0448 - val_accuracy: 0.9572\n",
      "Epoch 2332/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9787\n",
      "Epoch 2332: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0296 - accuracy: 0.9787 - val_loss: 0.0448 - val_accuracy: 0.9572\n",
      "Epoch 2333/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9787\n",
      "Epoch 2333: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0296 - accuracy: 0.9787 - val_loss: 0.0448 - val_accuracy: 0.9572\n",
      "Epoch 2334/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9787\n",
      "Epoch 2334: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0296 - accuracy: 0.9787 - val_loss: 0.0448 - val_accuracy: 0.9572\n",
      "Epoch 2335/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9787\n",
      "Epoch 2335: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0296 - accuracy: 0.9787 - val_loss: 0.0448 - val_accuracy: 0.9572\n",
      "Epoch 2336/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9787\n",
      "Epoch 2336: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0296 - accuracy: 0.9787 - val_loss: 0.0448 - val_accuracy: 0.9572\n",
      "Epoch 2337/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9787\n",
      "Epoch 2337: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0296 - accuracy: 0.9787 - val_loss: 0.0447 - val_accuracy: 0.9572\n",
      "Epoch 2338/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9787\n",
      "Epoch 2338: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0296 - accuracy: 0.9787 - val_loss: 0.0447 - val_accuracy: 0.9572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2339/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9787\n",
      "Epoch 2339: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0296 - accuracy: 0.9787 - val_loss: 0.0447 - val_accuracy: 0.9572\n",
      "Epoch 2340/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9787\n",
      "Epoch 2340: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0295 - accuracy: 0.9787 - val_loss: 0.0447 - val_accuracy: 0.9572\n",
      "Epoch 2341/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9787\n",
      "Epoch 2341: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0295 - accuracy: 0.9787 - val_loss: 0.0447 - val_accuracy: 0.9572\n",
      "Epoch 2342/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9787\n",
      "Epoch 2342: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0295 - accuracy: 0.9787 - val_loss: 0.0447 - val_accuracy: 0.9572\n",
      "Epoch 2343/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9787\n",
      "Epoch 2343: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0295 - accuracy: 0.9787 - val_loss: 0.0446 - val_accuracy: 0.9572\n",
      "Epoch 2344/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9787\n",
      "Epoch 2344: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0295 - accuracy: 0.9787 - val_loss: 0.0446 - val_accuracy: 0.9572\n",
      "Epoch 2345/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9787\n",
      "Epoch 2345: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0295 - accuracy: 0.9787 - val_loss: 0.0446 - val_accuracy: 0.9572\n",
      "Epoch 2346/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9787\n",
      "Epoch 2346: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0295 - accuracy: 0.9787 - val_loss: 0.0446 - val_accuracy: 0.9572\n",
      "Epoch 2347/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9787\n",
      "Epoch 2347: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0294 - accuracy: 0.9787 - val_loss: 0.0446 - val_accuracy: 0.9572\n",
      "Epoch 2348/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9787\n",
      "Epoch 2348: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0294 - accuracy: 0.9787 - val_loss: 0.0446 - val_accuracy: 0.9572\n",
      "Epoch 2349/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9787\n",
      "Epoch 2349: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0294 - accuracy: 0.9787 - val_loss: 0.0446 - val_accuracy: 0.9572\n",
      "Epoch 2350/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9787\n",
      "Epoch 2350: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0294 - accuracy: 0.9787 - val_loss: 0.0445 - val_accuracy: 0.9572\n",
      "Epoch 2351/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9787\n",
      "Epoch 2351: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0294 - accuracy: 0.9787 - val_loss: 0.0445 - val_accuracy: 0.9572\n",
      "Epoch 2352/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9787\n",
      "Epoch 2352: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0294 - accuracy: 0.9787 - val_loss: 0.0445 - val_accuracy: 0.9572\n",
      "Epoch 2353/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9787\n",
      "Epoch 2353: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0294 - accuracy: 0.9787 - val_loss: 0.0445 - val_accuracy: 0.9572\n",
      "Epoch 2354/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9787\n",
      "Epoch 2354: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0293 - accuracy: 0.9787 - val_loss: 0.0445 - val_accuracy: 0.9572\n",
      "Epoch 2355/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9787\n",
      "Epoch 2355: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0293 - accuracy: 0.9787 - val_loss: 0.0445 - val_accuracy: 0.9572\n",
      "Epoch 2356/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9787\n",
      "Epoch 2356: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0293 - accuracy: 0.9787 - val_loss: 0.0444 - val_accuracy: 0.9572\n",
      "Epoch 2357/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9787\n",
      "Epoch 2357: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0293 - accuracy: 0.9787 - val_loss: 0.0444 - val_accuracy: 0.9572\n",
      "Epoch 2358/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9787\n",
      "Epoch 2358: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0293 - accuracy: 0.9787 - val_loss: 0.0444 - val_accuracy: 0.9572\n",
      "Epoch 2359/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9787\n",
      "Epoch 2359: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0293 - accuracy: 0.9787 - val_loss: 0.0444 - val_accuracy: 0.9572\n",
      "Epoch 2360/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9787\n",
      "Epoch 2360: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0293 - accuracy: 0.9787 - val_loss: 0.0444 - val_accuracy: 0.9572\n",
      "Epoch 2361/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9787\n",
      "Epoch 2361: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0293 - accuracy: 0.9787 - val_loss: 0.0444 - val_accuracy: 0.9572\n",
      "Epoch 2362/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9787\n",
      "Epoch 2362: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0292 - accuracy: 0.9787 - val_loss: 0.0444 - val_accuracy: 0.9572\n",
      "Epoch 2363/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9787\n",
      "Epoch 2363: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0292 - accuracy: 0.9787 - val_loss: 0.0443 - val_accuracy: 0.9572\n",
      "Epoch 2364/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9787\n",
      "Epoch 2364: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0292 - accuracy: 0.9787 - val_loss: 0.0443 - val_accuracy: 0.9572\n",
      "Epoch 2365/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9787\n",
      "Epoch 2365: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0292 - accuracy: 0.9787 - val_loss: 0.0443 - val_accuracy: 0.9572\n",
      "Epoch 2366/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9787\n",
      "Epoch 2366: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0292 - accuracy: 0.9787 - val_loss: 0.0443 - val_accuracy: 0.9572\n",
      "Epoch 2367/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9787\n",
      "Epoch 2367: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0292 - accuracy: 0.9787 - val_loss: 0.0443 - val_accuracy: 0.9572\n",
      "Epoch 2368/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9787\n",
      "Epoch 2368: val_accuracy did not improve from 0.95721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0292 - accuracy: 0.9787 - val_loss: 0.0443 - val_accuracy: 0.9572\n",
      "Epoch 2369/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9787\n",
      "Epoch 2369: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0291 - accuracy: 0.9787 - val_loss: 0.0443 - val_accuracy: 0.9572\n",
      "Epoch 2370/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9787\n",
      "Epoch 2370: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0291 - accuracy: 0.9787 - val_loss: 0.0442 - val_accuracy: 0.9572\n",
      "Epoch 2371/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9787\n",
      "Epoch 2371: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0291 - accuracy: 0.9787 - val_loss: 0.0442 - val_accuracy: 0.9572\n",
      "Epoch 2372/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9787\n",
      "Epoch 2372: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0291 - accuracy: 0.9787 - val_loss: 0.0442 - val_accuracy: 0.9572\n",
      "Epoch 2373/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9787\n",
      "Epoch 2373: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0291 - accuracy: 0.9787 - val_loss: 0.0442 - val_accuracy: 0.9572\n",
      "Epoch 2374/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9787\n",
      "Epoch 2374: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0291 - accuracy: 0.9787 - val_loss: 0.0442 - val_accuracy: 0.9572\n",
      "Epoch 2375/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9787\n",
      "Epoch 2375: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - accuracy: 0.9787 - val_loss: 0.0442 - val_accuracy: 0.9572\n",
      "Epoch 2376/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9787\n",
      "Epoch 2376: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0291 - accuracy: 0.9787 - val_loss: 0.0441 - val_accuracy: 0.9572\n",
      "Epoch 2377/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9787\n",
      "Epoch 2377: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0290 - accuracy: 0.9787 - val_loss: 0.0441 - val_accuracy: 0.9572\n",
      "Epoch 2378/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9787\n",
      "Epoch 2378: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0290 - accuracy: 0.9787 - val_loss: 0.0441 - val_accuracy: 0.9572\n",
      "Epoch 2379/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9787\n",
      "Epoch 2379: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0290 - accuracy: 0.9787 - val_loss: 0.0441 - val_accuracy: 0.9572\n",
      "Epoch 2380/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9787\n",
      "Epoch 2380: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0290 - accuracy: 0.9787 - val_loss: 0.0441 - val_accuracy: 0.9572\n",
      "Epoch 2381/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9787\n",
      "Epoch 2381: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0290 - accuracy: 0.9787 - val_loss: 0.0441 - val_accuracy: 0.9572\n",
      "Epoch 2382/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9787\n",
      "Epoch 2382: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0290 - accuracy: 0.9787 - val_loss: 0.0441 - val_accuracy: 0.9572\n",
      "Epoch 2383/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9787\n",
      "Epoch 2383: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0290 - accuracy: 0.9787 - val_loss: 0.0440 - val_accuracy: 0.9572\n",
      "Epoch 2384/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9787\n",
      "Epoch 2384: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0289 - accuracy: 0.9787 - val_loss: 0.0440 - val_accuracy: 0.9572\n",
      "Epoch 2385/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9787\n",
      "Epoch 2385: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0289 - accuracy: 0.9787 - val_loss: 0.0440 - val_accuracy: 0.9572\n",
      "Epoch 2386/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9787\n",
      "Epoch 2386: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0289 - accuracy: 0.9787 - val_loss: 0.0440 - val_accuracy: 0.9572\n",
      "Epoch 2387/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9787\n",
      "Epoch 2387: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0289 - accuracy: 0.9787 - val_loss: 0.0440 - val_accuracy: 0.9572\n",
      "Epoch 2388/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9787\n",
      "Epoch 2388: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0289 - accuracy: 0.9787 - val_loss: 0.0440 - val_accuracy: 0.9572\n",
      "Epoch 2389/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9787\n",
      "Epoch 2389: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0289 - accuracy: 0.9787 - val_loss: 0.0440 - val_accuracy: 0.9572\n",
      "Epoch 2390/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9797\n",
      "Epoch 2390: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0289 - accuracy: 0.9797 - val_loss: 0.0439 - val_accuracy: 0.9572\n",
      "Epoch 2391/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9797\n",
      "Epoch 2391: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0289 - accuracy: 0.9797 - val_loss: 0.0439 - val_accuracy: 0.9572\n",
      "Epoch 2392/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9797\n",
      "Epoch 2392: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0288 - accuracy: 0.9797 - val_loss: 0.0439 - val_accuracy: 0.9572\n",
      "Epoch 2393/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9797\n",
      "Epoch 2393: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0288 - accuracy: 0.9797 - val_loss: 0.0439 - val_accuracy: 0.9572\n",
      "Epoch 2394/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9797\n",
      "Epoch 2394: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0288 - accuracy: 0.9797 - val_loss: 0.0439 - val_accuracy: 0.9572\n",
      "Epoch 2395/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9797\n",
      "Epoch 2395: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0288 - accuracy: 0.9797 - val_loss: 0.0439 - val_accuracy: 0.9572\n",
      "Epoch 2396/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9797\n",
      "Epoch 2396: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0288 - accuracy: 0.9797 - val_loss: 0.0439 - val_accuracy: 0.9572\n",
      "Epoch 2397/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9797\n",
      "Epoch 2397: val_accuracy did not improve from 0.95721\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0288 - accuracy: 0.9797 - val_loss: 0.0438 - val_accuracy: 0.9572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2398/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9797\n",
      "Epoch 2398: val_accuracy improved from 0.95721 to 0.95946, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0288 - accuracy: 0.9797 - val_loss: 0.0438 - val_accuracy: 0.9595\n",
      "Epoch 2399/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9797\n",
      "Epoch 2399: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0288 - accuracy: 0.9797 - val_loss: 0.0438 - val_accuracy: 0.9595\n",
      "Epoch 2400/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9797\n",
      "Epoch 2400: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0287 - accuracy: 0.9797 - val_loss: 0.0438 - val_accuracy: 0.9595\n",
      "Epoch 2401/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9797\n",
      "Epoch 2401: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0287 - accuracy: 0.9797 - val_loss: 0.0438 - val_accuracy: 0.9595\n",
      "Epoch 2402/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9797\n",
      "Epoch 2402: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0287 - accuracy: 0.9797 - val_loss: 0.0438 - val_accuracy: 0.9595\n",
      "Epoch 2403/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9797\n",
      "Epoch 2403: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0287 - accuracy: 0.9797 - val_loss: 0.0438 - val_accuracy: 0.9595\n",
      "Epoch 2404/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9797\n",
      "Epoch 2404: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0287 - accuracy: 0.9797 - val_loss: 0.0437 - val_accuracy: 0.9595\n",
      "Epoch 2405/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9797\n",
      "Epoch 2405: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0287 - accuracy: 0.9797 - val_loss: 0.0437 - val_accuracy: 0.9595\n",
      "Epoch 2406/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9797\n",
      "Epoch 2406: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0287 - accuracy: 0.9797 - val_loss: 0.0437 - val_accuracy: 0.9595\n",
      "Epoch 2407/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9797\n",
      "Epoch 2407: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0287 - accuracy: 0.9797 - val_loss: 0.0437 - val_accuracy: 0.9595\n",
      "Epoch 2408/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9797\n",
      "Epoch 2408: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0286 - accuracy: 0.9797 - val_loss: 0.0437 - val_accuracy: 0.9595\n",
      "Epoch 2409/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9797\n",
      "Epoch 2409: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0286 - accuracy: 0.9797 - val_loss: 0.0437 - val_accuracy: 0.9595\n",
      "Epoch 2410/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9797\n",
      "Epoch 2410: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0286 - accuracy: 0.9797 - val_loss: 0.0437 - val_accuracy: 0.9595\n",
      "Epoch 2411/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9797\n",
      "Epoch 2411: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0286 - accuracy: 0.9797 - val_loss: 0.0436 - val_accuracy: 0.9595\n",
      "Epoch 2412/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9797\n",
      "Epoch 2412: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0286 - accuracy: 0.9797 - val_loss: 0.0436 - val_accuracy: 0.9595\n",
      "Epoch 2413/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9797\n",
      "Epoch 2413: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0286 - accuracy: 0.9797 - val_loss: 0.0436 - val_accuracy: 0.9595\n",
      "Epoch 2414/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9797\n",
      "Epoch 2414: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0286 - accuracy: 0.9797 - val_loss: 0.0436 - val_accuracy: 0.9595\n",
      "Epoch 2415/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9797\n",
      "Epoch 2415: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0285 - accuracy: 0.9797 - val_loss: 0.0436 - val_accuracy: 0.9595\n",
      "Epoch 2416/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9797\n",
      "Epoch 2416: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0285 - accuracy: 0.9797 - val_loss: 0.0436 - val_accuracy: 0.9595\n",
      "Epoch 2417/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9797\n",
      "Epoch 2417: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0285 - accuracy: 0.9797 - val_loss: 0.0436 - val_accuracy: 0.9595\n",
      "Epoch 2418/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9797\n",
      "Epoch 2418: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0285 - accuracy: 0.9797 - val_loss: 0.0435 - val_accuracy: 0.9595\n",
      "Epoch 2419/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9797\n",
      "Epoch 2419: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0285 - accuracy: 0.9797 - val_loss: 0.0435 - val_accuracy: 0.9595\n",
      "Epoch 2420/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9797\n",
      "Epoch 2420: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0285 - accuracy: 0.9797 - val_loss: 0.0435 - val_accuracy: 0.9595\n",
      "Epoch 2421/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9797\n",
      "Epoch 2421: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0285 - accuracy: 0.9797 - val_loss: 0.0435 - val_accuracy: 0.9595\n",
      "Epoch 2422/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9797\n",
      "Epoch 2422: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0285 - accuracy: 0.9797 - val_loss: 0.0435 - val_accuracy: 0.9595\n",
      "Epoch 2423/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9797\n",
      "Epoch 2423: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0284 - accuracy: 0.9797 - val_loss: 0.0435 - val_accuracy: 0.9595\n",
      "Epoch 2424/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9797\n",
      "Epoch 2424: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0284 - accuracy: 0.9797 - val_loss: 0.0435 - val_accuracy: 0.9595\n",
      "Epoch 2425/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9797\n",
      "Epoch 2425: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0284 - accuracy: 0.9797 - val_loss: 0.0434 - val_accuracy: 0.9595\n",
      "Epoch 2426/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9797\n",
      "Epoch 2426: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0284 - accuracy: 0.9797 - val_loss: 0.0434 - val_accuracy: 0.9595\n",
      "Epoch 2427/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9797\n",
      "Epoch 2427: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0284 - accuracy: 0.9797 - val_loss: 0.0434 - val_accuracy: 0.9595\n",
      "Epoch 2428/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9797\n",
      "Epoch 2428: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0284 - accuracy: 0.9797 - val_loss: 0.0434 - val_accuracy: 0.9595\n",
      "Epoch 2429/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9797\n",
      "Epoch 2429: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0284 - accuracy: 0.9797 - val_loss: 0.0434 - val_accuracy: 0.9595\n",
      "Epoch 2430/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9797\n",
      "Epoch 2430: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0284 - accuracy: 0.9797 - val_loss: 0.0434 - val_accuracy: 0.9595\n",
      "Epoch 2431/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9797\n",
      "Epoch 2431: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0283 - accuracy: 0.9797 - val_loss: 0.0433 - val_accuracy: 0.9595\n",
      "Epoch 2432/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9797\n",
      "Epoch 2432: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0283 - accuracy: 0.9797 - val_loss: 0.0434 - val_accuracy: 0.9595\n",
      "Epoch 2433/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9797\n",
      "Epoch 2433: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0283 - accuracy: 0.9797 - val_loss: 0.0433 - val_accuracy: 0.9595\n",
      "Epoch 2434/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9797\n",
      "Epoch 2434: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0283 - accuracy: 0.9797 - val_loss: 0.0433 - val_accuracy: 0.9595\n",
      "Epoch 2435/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9797\n",
      "Epoch 2435: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0283 - accuracy: 0.9797 - val_loss: 0.0433 - val_accuracy: 0.9595\n",
      "Epoch 2436/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9797\n",
      "Epoch 2436: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0283 - accuracy: 0.9797 - val_loss: 0.0433 - val_accuracy: 0.9595\n",
      "Epoch 2437/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9797\n",
      "Epoch 2437: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0283 - accuracy: 0.9797 - val_loss: 0.0433 - val_accuracy: 0.9595\n",
      "Epoch 2438/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9806\n",
      "Epoch 2438: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0283 - accuracy: 0.9806 - val_loss: 0.0432 - val_accuracy: 0.9595\n",
      "Epoch 2439/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9806\n",
      "Epoch 2439: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0282 - accuracy: 0.9806 - val_loss: 0.0433 - val_accuracy: 0.9595\n",
      "Epoch 2440/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9806\n",
      "Epoch 2440: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0282 - accuracy: 0.9806 - val_loss: 0.0432 - val_accuracy: 0.9595\n",
      "Epoch 2441/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9806\n",
      "Epoch 2441: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0282 - accuracy: 0.9806 - val_loss: 0.0432 - val_accuracy: 0.9595\n",
      "Epoch 2442/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9806\n",
      "Epoch 2442: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0282 - accuracy: 0.9806 - val_loss: 0.0432 - val_accuracy: 0.9595\n",
      "Epoch 2443/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9806\n",
      "Epoch 2443: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0282 - accuracy: 0.9806 - val_loss: 0.0432 - val_accuracy: 0.9595\n",
      "Epoch 2444/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9806\n",
      "Epoch 2444: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0282 - accuracy: 0.9806 - val_loss: 0.0432 - val_accuracy: 0.9595\n",
      "Epoch 2445/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9806\n",
      "Epoch 2445: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0282 - accuracy: 0.9806 - val_loss: 0.0432 - val_accuracy: 0.9595\n",
      "Epoch 2446/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9806\n",
      "Epoch 2446: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0282 - accuracy: 0.9806 - val_loss: 0.0432 - val_accuracy: 0.9595\n",
      "Epoch 2447/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9806\n",
      "Epoch 2447: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0281 - accuracy: 0.9806 - val_loss: 0.0431 - val_accuracy: 0.9595\n",
      "Epoch 2448/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9806\n",
      "Epoch 2448: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0281 - accuracy: 0.9806 - val_loss: 0.0431 - val_accuracy: 0.9595\n",
      "Epoch 2449/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9806\n",
      "Epoch 2449: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0281 - accuracy: 0.9806 - val_loss: 0.0431 - val_accuracy: 0.9595\n",
      "Epoch 2450/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9806\n",
      "Epoch 2450: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0281 - accuracy: 0.9806 - val_loss: 0.0431 - val_accuracy: 0.9595\n",
      "Epoch 2451/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9806\n",
      "Epoch 2451: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0281 - accuracy: 0.9806 - val_loss: 0.0431 - val_accuracy: 0.9595\n",
      "Epoch 2452/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9806\n",
      "Epoch 2452: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0281 - accuracy: 0.9806 - val_loss: 0.0431 - val_accuracy: 0.9595\n",
      "Epoch 2453/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9806\n",
      "Epoch 2453: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0281 - accuracy: 0.9806 - val_loss: 0.0431 - val_accuracy: 0.9595\n",
      "Epoch 2454/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9806\n",
      "Epoch 2454: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0281 - accuracy: 0.9806 - val_loss: 0.0430 - val_accuracy: 0.9595\n",
      "Epoch 2455/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9806\n",
      "Epoch 2455: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0280 - accuracy: 0.9806 - val_loss: 0.0430 - val_accuracy: 0.9595\n",
      "Epoch 2456/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9806\n",
      "Epoch 2456: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0280 - accuracy: 0.9806 - val_loss: 0.0430 - val_accuracy: 0.9595\n",
      "Epoch 2457/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9806\n",
      "Epoch 2457: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0280 - accuracy: 0.9806 - val_loss: 0.0430 - val_accuracy: 0.9595\n",
      "Epoch 2458/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9806\n",
      "Epoch 2458: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0280 - accuracy: 0.9806 - val_loss: 0.0430 - val_accuracy: 0.9595\n",
      "Epoch 2459/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9806\n",
      "Epoch 2459: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0280 - accuracy: 0.9806 - val_loss: 0.0430 - val_accuracy: 0.9595\n",
      "Epoch 2460/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9806\n",
      "Epoch 2460: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0280 - accuracy: 0.9806 - val_loss: 0.0430 - val_accuracy: 0.9595\n",
      "Epoch 2461/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9806\n",
      "Epoch 2461: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0280 - accuracy: 0.9806 - val_loss: 0.0429 - val_accuracy: 0.9595\n",
      "Epoch 2462/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9806\n",
      "Epoch 2462: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0280 - accuracy: 0.9806 - val_loss: 0.0429 - val_accuracy: 0.9595\n",
      "Epoch 2463/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9806\n",
      "Epoch 2463: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0279 - accuracy: 0.9806 - val_loss: 0.0429 - val_accuracy: 0.9595\n",
      "Epoch 2464/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9806\n",
      "Epoch 2464: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0279 - accuracy: 0.9806 - val_loss: 0.0429 - val_accuracy: 0.9595\n",
      "Epoch 2465/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9806\n",
      "Epoch 2465: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0279 - accuracy: 0.9806 - val_loss: 0.0429 - val_accuracy: 0.9595\n",
      "Epoch 2466/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9806\n",
      "Epoch 2466: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0279 - accuracy: 0.9806 - val_loss: 0.0429 - val_accuracy: 0.9595\n",
      "Epoch 2467/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9806\n",
      "Epoch 2467: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0279 - accuracy: 0.9806 - val_loss: 0.0429 - val_accuracy: 0.9595\n",
      "Epoch 2468/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9806\n",
      "Epoch 2468: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0279 - accuracy: 0.9806 - val_loss: 0.0428 - val_accuracy: 0.9595\n",
      "Epoch 2469/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9806\n",
      "Epoch 2469: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0279 - accuracy: 0.9806 - val_loss: 0.0428 - val_accuracy: 0.9595\n",
      "Epoch 2470/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9806\n",
      "Epoch 2470: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0279 - accuracy: 0.9806 - val_loss: 0.0428 - val_accuracy: 0.9595\n",
      "Epoch 2471/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9806\n",
      "Epoch 2471: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0278 - accuracy: 0.9806 - val_loss: 0.0428 - val_accuracy: 0.9595\n",
      "Epoch 2472/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9806\n",
      "Epoch 2472: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0278 - accuracy: 0.9806 - val_loss: 0.0428 - val_accuracy: 0.9595\n",
      "Epoch 2473/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9806\n",
      "Epoch 2473: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0278 - accuracy: 0.9806 - val_loss: 0.0428 - val_accuracy: 0.9595\n",
      "Epoch 2474/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9806\n",
      "Epoch 2474: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0278 - accuracy: 0.9806 - val_loss: 0.0428 - val_accuracy: 0.9595\n",
      "Epoch 2475/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9806\n",
      "Epoch 2475: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0278 - accuracy: 0.9806 - val_loss: 0.0427 - val_accuracy: 0.9595\n",
      "Epoch 2476/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9806\n",
      "Epoch 2476: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0278 - accuracy: 0.9806 - val_loss: 0.0427 - val_accuracy: 0.9595\n",
      "Epoch 2477/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9806\n",
      "Epoch 2477: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0278 - accuracy: 0.9806 - val_loss: 0.0427 - val_accuracy: 0.9595\n",
      "Epoch 2478/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9806\n",
      "Epoch 2478: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0278 - accuracy: 0.9806 - val_loss: 0.0427 - val_accuracy: 0.9595\n",
      "Epoch 2479/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9806\n",
      "Epoch 2479: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0277 - accuracy: 0.9806 - val_loss: 0.0427 - val_accuracy: 0.9595\n",
      "Epoch 2480/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9806\n",
      "Epoch 2480: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0277 - accuracy: 0.9806 - val_loss: 0.0427 - val_accuracy: 0.9595\n",
      "Epoch 2481/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9806\n",
      "Epoch 2481: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0277 - accuracy: 0.9806 - val_loss: 0.0427 - val_accuracy: 0.9595\n",
      "Epoch 2482/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9806\n",
      "Epoch 2482: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0277 - accuracy: 0.9806 - val_loss: 0.0427 - val_accuracy: 0.9595\n",
      "Epoch 2483/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9806\n",
      "Epoch 2483: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0277 - accuracy: 0.9806 - val_loss: 0.0426 - val_accuracy: 0.9595\n",
      "Epoch 2484/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9806\n",
      "Epoch 2484: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0277 - accuracy: 0.9806 - val_loss: 0.0426 - val_accuracy: 0.9595\n",
      "Epoch 2485/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9806\n",
      "Epoch 2485: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0277 - accuracy: 0.9806 - val_loss: 0.0426 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2486/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9806\n",
      "Epoch 2486: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0277 - accuracy: 0.9806 - val_loss: 0.0426 - val_accuracy: 0.9595\n",
      "Epoch 2487/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9806\n",
      "Epoch 2487: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0276 - accuracy: 0.9806 - val_loss: 0.0426 - val_accuracy: 0.9595\n",
      "Epoch 2488/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9806\n",
      "Epoch 2488: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0276 - accuracy: 0.9806 - val_loss: 0.0426 - val_accuracy: 0.9595\n",
      "Epoch 2489/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9806\n",
      "Epoch 2489: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0276 - accuracy: 0.9806 - val_loss: 0.0426 - val_accuracy: 0.9595\n",
      "Epoch 2490/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9806\n",
      "Epoch 2490: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0276 - accuracy: 0.9806 - val_loss: 0.0425 - val_accuracy: 0.9595\n",
      "Epoch 2491/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9806\n",
      "Epoch 2491: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0276 - accuracy: 0.9806 - val_loss: 0.0425 - val_accuracy: 0.9595\n",
      "Epoch 2492/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9806\n",
      "Epoch 2492: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0276 - accuracy: 0.9806 - val_loss: 0.0425 - val_accuracy: 0.9595\n",
      "Epoch 2493/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9806\n",
      "Epoch 2493: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0276 - accuracy: 0.9806 - val_loss: 0.0425 - val_accuracy: 0.9595\n",
      "Epoch 2494/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9806\n",
      "Epoch 2494: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0276 - accuracy: 0.9806 - val_loss: 0.0425 - val_accuracy: 0.9595\n",
      "Epoch 2495/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9806\n",
      "Epoch 2495: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0276 - accuracy: 0.9806 - val_loss: 0.0425 - val_accuracy: 0.9595\n",
      "Epoch 2496/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9806\n",
      "Epoch 2496: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0275 - accuracy: 0.9806 - val_loss: 0.0425 - val_accuracy: 0.9595\n",
      "Epoch 2497/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9806\n",
      "Epoch 2497: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0275 - accuracy: 0.9806 - val_loss: 0.0425 - val_accuracy: 0.9595\n",
      "Epoch 2498/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9806\n",
      "Epoch 2498: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0275 - accuracy: 0.9806 - val_loss: 0.0424 - val_accuracy: 0.9595\n",
      "Epoch 2499/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9806\n",
      "Epoch 2499: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0275 - accuracy: 0.9806 - val_loss: 0.0424 - val_accuracy: 0.9595\n",
      "Epoch 2500/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9806\n",
      "Epoch 2500: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0275 - accuracy: 0.9806 - val_loss: 0.0424 - val_accuracy: 0.9595\n",
      "Epoch 2501/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9806\n",
      "Epoch 2501: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0275 - accuracy: 0.9806 - val_loss: 0.0424 - val_accuracy: 0.9595\n",
      "Epoch 2502/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9806\n",
      "Epoch 2502: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0275 - accuracy: 0.9806 - val_loss: 0.0424 - val_accuracy: 0.9595\n",
      "Epoch 2503/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9806\n",
      "Epoch 2503: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0275 - accuracy: 0.9806 - val_loss: 0.0424 - val_accuracy: 0.9595\n",
      "Epoch 2504/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9806\n",
      "Epoch 2504: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0274 - accuracy: 0.9806 - val_loss: 0.0424 - val_accuracy: 0.9595\n",
      "Epoch 2505/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9806\n",
      "Epoch 2505: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0274 - accuracy: 0.9806 - val_loss: 0.0423 - val_accuracy: 0.9595\n",
      "Epoch 2506/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9806\n",
      "Epoch 2506: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0274 - accuracy: 0.9806 - val_loss: 0.0423 - val_accuracy: 0.9595\n",
      "Epoch 2507/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9806\n",
      "Epoch 2507: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0274 - accuracy: 0.9806 - val_loss: 0.0423 - val_accuracy: 0.9595\n",
      "Epoch 2508/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9806\n",
      "Epoch 2508: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0274 - accuracy: 0.9806 - val_loss: 0.0423 - val_accuracy: 0.9595\n",
      "Epoch 2509/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9806\n",
      "Epoch 2509: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0274 - accuracy: 0.9806 - val_loss: 0.0423 - val_accuracy: 0.9595\n",
      "Epoch 2510/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9806\n",
      "Epoch 2510: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0274 - accuracy: 0.9806 - val_loss: 0.0423 - val_accuracy: 0.9595\n",
      "Epoch 2511/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9806\n",
      "Epoch 2511: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0274 - accuracy: 0.9806 - val_loss: 0.0423 - val_accuracy: 0.9595\n",
      "Epoch 2512/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9806\n",
      "Epoch 2512: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0273 - accuracy: 0.9806 - val_loss: 0.0423 - val_accuracy: 0.9595\n",
      "Epoch 2513/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9806\n",
      "Epoch 2513: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0273 - accuracy: 0.9806 - val_loss: 0.0422 - val_accuracy: 0.9595\n",
      "Epoch 2514/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9806\n",
      "Epoch 2514: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0273 - accuracy: 0.9806 - val_loss: 0.0422 - val_accuracy: 0.9595\n",
      "Epoch 2515/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9806\n",
      "Epoch 2515: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0273 - accuracy: 0.9806 - val_loss: 0.0422 - val_accuracy: 0.9595\n",
      "Epoch 2516/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9806\n",
      "Epoch 2516: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0273 - accuracy: 0.9806 - val_loss: 0.0422 - val_accuracy: 0.9595\n",
      "Epoch 2517/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9806\n",
      "Epoch 2517: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0273 - accuracy: 0.9806 - val_loss: 0.0422 - val_accuracy: 0.9595\n",
      "Epoch 2518/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9806\n",
      "Epoch 2518: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0273 - accuracy: 0.9806 - val_loss: 0.0422 - val_accuracy: 0.9595\n",
      "Epoch 2519/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9806\n",
      "Epoch 2519: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0273 - accuracy: 0.9806 - val_loss: 0.0422 - val_accuracy: 0.9595\n",
      "Epoch 2520/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9806\n",
      "Epoch 2520: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0273 - accuracy: 0.9806 - val_loss: 0.0421 - val_accuracy: 0.9595\n",
      "Epoch 2521/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9806\n",
      "Epoch 2521: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0272 - accuracy: 0.9806 - val_loss: 0.0421 - val_accuracy: 0.9595\n",
      "Epoch 2522/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9806\n",
      "Epoch 2522: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0272 - accuracy: 0.9806 - val_loss: 0.0421 - val_accuracy: 0.9595\n",
      "Epoch 2523/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9806\n",
      "Epoch 2523: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0272 - accuracy: 0.9806 - val_loss: 0.0421 - val_accuracy: 0.9595\n",
      "Epoch 2524/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9806\n",
      "Epoch 2524: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0272 - accuracy: 0.9806 - val_loss: 0.0421 - val_accuracy: 0.9595\n",
      "Epoch 2525/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9806\n",
      "Epoch 2525: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0272 - accuracy: 0.9806 - val_loss: 0.0421 - val_accuracy: 0.9595\n",
      "Epoch 2526/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9806\n",
      "Epoch 2526: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0272 - accuracy: 0.9806 - val_loss: 0.0421 - val_accuracy: 0.9595\n",
      "Epoch 2527/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9806\n",
      "Epoch 2527: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0272 - accuracy: 0.9806 - val_loss: 0.0421 - val_accuracy: 0.9595\n",
      "Epoch 2528/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9806\n",
      "Epoch 2528: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0272 - accuracy: 0.9806 - val_loss: 0.0420 - val_accuracy: 0.9595\n",
      "Epoch 2529/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9806\n",
      "Epoch 2529: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0271 - accuracy: 0.9806 - val_loss: 0.0420 - val_accuracy: 0.9595\n",
      "Epoch 2530/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9806\n",
      "Epoch 2530: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0271 - accuracy: 0.9806 - val_loss: 0.0420 - val_accuracy: 0.9595\n",
      "Epoch 2531/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9806\n",
      "Epoch 2531: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0271 - accuracy: 0.9806 - val_loss: 0.0420 - val_accuracy: 0.9595\n",
      "Epoch 2532/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9806\n",
      "Epoch 2532: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0271 - accuracy: 0.9806 - val_loss: 0.0420 - val_accuracy: 0.9595\n",
      "Epoch 2533/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9806\n",
      "Epoch 2533: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0271 - accuracy: 0.9806 - val_loss: 0.0420 - val_accuracy: 0.9595\n",
      "Epoch 2534/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9806\n",
      "Epoch 2534: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0271 - accuracy: 0.9806 - val_loss: 0.0420 - val_accuracy: 0.9595\n",
      "Epoch 2535/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9806\n",
      "Epoch 2535: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0271 - accuracy: 0.9806 - val_loss: 0.0420 - val_accuracy: 0.9595\n",
      "Epoch 2536/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9806\n",
      "Epoch 2536: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0271 - accuracy: 0.9806 - val_loss: 0.0419 - val_accuracy: 0.9595\n",
      "Epoch 2537/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9806\n",
      "Epoch 2537: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0270 - accuracy: 0.9806 - val_loss: 0.0419 - val_accuracy: 0.9595\n",
      "Epoch 2538/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9806\n",
      "Epoch 2538: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0270 - accuracy: 0.9806 - val_loss: 0.0419 - val_accuracy: 0.9595\n",
      "Epoch 2539/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9806\n",
      "Epoch 2539: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0270 - accuracy: 0.9806 - val_loss: 0.0419 - val_accuracy: 0.9595\n",
      "Epoch 2540/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9806\n",
      "Epoch 2540: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0270 - accuracy: 0.9806 - val_loss: 0.0419 - val_accuracy: 0.9595\n",
      "Epoch 2541/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9806\n",
      "Epoch 2541: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0270 - accuracy: 0.9806 - val_loss: 0.0419 - val_accuracy: 0.9595\n",
      "Epoch 2542/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9806\n",
      "Epoch 2542: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0270 - accuracy: 0.9806 - val_loss: 0.0419 - val_accuracy: 0.9595\n",
      "Epoch 2543/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9806\n",
      "Epoch 2543: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0270 - accuracy: 0.9806 - val_loss: 0.0419 - val_accuracy: 0.9595\n",
      "Epoch 2544/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9806\n",
      "Epoch 2544: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0270 - accuracy: 0.9806 - val_loss: 0.0418 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2545/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9806\n",
      "Epoch 2545: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0270 - accuracy: 0.9806 - val_loss: 0.0418 - val_accuracy: 0.9595\n",
      "Epoch 2546/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9806\n",
      "Epoch 2546: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0269 - accuracy: 0.9806 - val_loss: 0.0418 - val_accuracy: 0.9595\n",
      "Epoch 2547/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9806\n",
      "Epoch 2547: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0269 - accuracy: 0.9806 - val_loss: 0.0418 - val_accuracy: 0.9595\n",
      "Epoch 2548/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9806\n",
      "Epoch 2548: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0269 - accuracy: 0.9806 - val_loss: 0.0418 - val_accuracy: 0.9595\n",
      "Epoch 2549/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9806\n",
      "Epoch 2549: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0269 - accuracy: 0.9806 - val_loss: 0.0418 - val_accuracy: 0.9595\n",
      "Epoch 2550/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9806\n",
      "Epoch 2550: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0269 - accuracy: 0.9806 - val_loss: 0.0418 - val_accuracy: 0.9595\n",
      "Epoch 2551/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9806\n",
      "Epoch 2551: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0269 - accuracy: 0.9806 - val_loss: 0.0418 - val_accuracy: 0.9595\n",
      "Epoch 2552/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9806\n",
      "Epoch 2552: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0269 - accuracy: 0.9806 - val_loss: 0.0417 - val_accuracy: 0.9595\n",
      "Epoch 2553/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9806\n",
      "Epoch 2553: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0269 - accuracy: 0.9806 - val_loss: 0.0417 - val_accuracy: 0.9595\n",
      "Epoch 2554/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9806\n",
      "Epoch 2554: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0269 - accuracy: 0.9806 - val_loss: 0.0417 - val_accuracy: 0.9595\n",
      "Epoch 2555/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9806\n",
      "Epoch 2555: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0268 - accuracy: 0.9806 - val_loss: 0.0417 - val_accuracy: 0.9595\n",
      "Epoch 2556/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9806\n",
      "Epoch 2556: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0268 - accuracy: 0.9806 - val_loss: 0.0417 - val_accuracy: 0.9595\n",
      "Epoch 2557/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9806\n",
      "Epoch 2557: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0268 - accuracy: 0.9806 - val_loss: 0.0417 - val_accuracy: 0.9595\n",
      "Epoch 2558/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9806\n",
      "Epoch 2558: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0268 - accuracy: 0.9806 - val_loss: 0.0417 - val_accuracy: 0.9595\n",
      "Epoch 2559/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9806\n",
      "Epoch 2559: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0268 - accuracy: 0.9806 - val_loss: 0.0416 - val_accuracy: 0.9595\n",
      "Epoch 2560/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9806\n",
      "Epoch 2560: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0268 - accuracy: 0.9806 - val_loss: 0.0416 - val_accuracy: 0.9595\n",
      "Epoch 2561/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9806\n",
      "Epoch 2561: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0268 - accuracy: 0.9806 - val_loss: 0.0416 - val_accuracy: 0.9595\n",
      "Epoch 2562/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9806\n",
      "Epoch 2562: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0268 - accuracy: 0.9806 - val_loss: 0.0416 - val_accuracy: 0.9595\n",
      "Epoch 2563/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9806\n",
      "Epoch 2563: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0267 - accuracy: 0.9806 - val_loss: 0.0416 - val_accuracy: 0.9595\n",
      "Epoch 2564/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9806\n",
      "Epoch 2564: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0267 - accuracy: 0.9806 - val_loss: 0.0416 - val_accuracy: 0.9595\n",
      "Epoch 2565/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9806\n",
      "Epoch 2565: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0267 - accuracy: 0.9806 - val_loss: 0.0416 - val_accuracy: 0.9595\n",
      "Epoch 2566/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9806\n",
      "Epoch 2566: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0267 - accuracy: 0.9806 - val_loss: 0.0416 - val_accuracy: 0.9595\n",
      "Epoch 2567/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9806\n",
      "Epoch 2567: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0267 - accuracy: 0.9806 - val_loss: 0.0415 - val_accuracy: 0.9595\n",
      "Epoch 2568/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9806\n",
      "Epoch 2568: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0267 - accuracy: 0.9806 - val_loss: 0.0415 - val_accuracy: 0.9595\n",
      "Epoch 2569/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9806\n",
      "Epoch 2569: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0267 - accuracy: 0.9806 - val_loss: 0.0415 - val_accuracy: 0.9595\n",
      "Epoch 2570/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9806\n",
      "Epoch 2570: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0267 - accuracy: 0.9806 - val_loss: 0.0415 - val_accuracy: 0.9595\n",
      "Epoch 2571/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9806\n",
      "Epoch 2571: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0267 - accuracy: 0.9806 - val_loss: 0.0415 - val_accuracy: 0.9595\n",
      "Epoch 2572/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9806\n",
      "Epoch 2572: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0266 - accuracy: 0.9806 - val_loss: 0.0415 - val_accuracy: 0.9595\n",
      "Epoch 2573/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9806\n",
      "Epoch 2573: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0266 - accuracy: 0.9806 - val_loss: 0.0415 - val_accuracy: 0.9595\n",
      "Epoch 2574/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9806\n",
      "Epoch 2574: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0266 - accuracy: 0.9806 - val_loss: 0.0415 - val_accuracy: 0.9595\n",
      "Epoch 2575/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9806\n",
      "Epoch 2575: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0266 - accuracy: 0.9806 - val_loss: 0.0414 - val_accuracy: 0.9595\n",
      "Epoch 2576/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9806\n",
      "Epoch 2576: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0266 - accuracy: 0.9806 - val_loss: 0.0414 - val_accuracy: 0.9595\n",
      "Epoch 2577/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9806\n",
      "Epoch 2577: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0266 - accuracy: 0.9806 - val_loss: 0.0414 - val_accuracy: 0.9595\n",
      "Epoch 2578/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9806\n",
      "Epoch 2578: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0266 - accuracy: 0.9806 - val_loss: 0.0414 - val_accuracy: 0.9595\n",
      "Epoch 2579/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9806\n",
      "Epoch 2579: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0266 - accuracy: 0.9806 - val_loss: 0.0414 - val_accuracy: 0.9595\n",
      "Epoch 2580/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9806\n",
      "Epoch 2580: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0266 - accuracy: 0.9806 - val_loss: 0.0414 - val_accuracy: 0.9595\n",
      "Epoch 2581/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9806\n",
      "Epoch 2581: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0265 - accuracy: 0.9806 - val_loss: 0.0414 - val_accuracy: 0.9595\n",
      "Epoch 2582/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9806\n",
      "Epoch 2582: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0265 - accuracy: 0.9806 - val_loss: 0.0414 - val_accuracy: 0.9595\n",
      "Epoch 2583/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9806\n",
      "Epoch 2583: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0265 - accuracy: 0.9806 - val_loss: 0.0414 - val_accuracy: 0.9595\n",
      "Epoch 2584/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9806\n",
      "Epoch 2584: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0265 - accuracy: 0.9806 - val_loss: 0.0413 - val_accuracy: 0.9595\n",
      "Epoch 2585/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9806\n",
      "Epoch 2585: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0265 - accuracy: 0.9806 - val_loss: 0.0413 - val_accuracy: 0.9595\n",
      "Epoch 2586/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9806\n",
      "Epoch 2586: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0265 - accuracy: 0.9806 - val_loss: 0.0413 - val_accuracy: 0.9595\n",
      "Epoch 2587/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9806\n",
      "Epoch 2587: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0265 - accuracy: 0.9806 - val_loss: 0.0413 - val_accuracy: 0.9595\n",
      "Epoch 2588/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9806\n",
      "Epoch 2588: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0265 - accuracy: 0.9806 - val_loss: 0.0413 - val_accuracy: 0.9595\n",
      "Epoch 2589/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9816\n",
      "Epoch 2589: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0264 - accuracy: 0.9816 - val_loss: 0.0413 - val_accuracy: 0.9595\n",
      "Epoch 2590/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9816\n",
      "Epoch 2590: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0264 - accuracy: 0.9816 - val_loss: 0.0413 - val_accuracy: 0.9595\n",
      "Epoch 2591/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9816\n",
      "Epoch 2591: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0264 - accuracy: 0.9816 - val_loss: 0.0413 - val_accuracy: 0.9595\n",
      "Epoch 2592/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9816\n",
      "Epoch 2592: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0264 - accuracy: 0.9816 - val_loss: 0.0412 - val_accuracy: 0.9595\n",
      "Epoch 2593/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9816\n",
      "Epoch 2593: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0264 - accuracy: 0.9816 - val_loss: 0.0412 - val_accuracy: 0.9595\n",
      "Epoch 2594/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9816\n",
      "Epoch 2594: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0264 - accuracy: 0.9816 - val_loss: 0.0412 - val_accuracy: 0.9595\n",
      "Epoch 2595/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9816\n",
      "Epoch 2595: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0264 - accuracy: 0.9816 - val_loss: 0.0412 - val_accuracy: 0.9595\n",
      "Epoch 2596/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9816\n",
      "Epoch 2596: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0264 - accuracy: 0.9816 - val_loss: 0.0412 - val_accuracy: 0.9595\n",
      "Epoch 2597/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9816\n",
      "Epoch 2597: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0264 - accuracy: 0.9816 - val_loss: 0.0412 - val_accuracy: 0.9595\n",
      "Epoch 2598/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9816\n",
      "Epoch 2598: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0263 - accuracy: 0.9816 - val_loss: 0.0412 - val_accuracy: 0.9595\n",
      "Epoch 2599/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9816\n",
      "Epoch 2599: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0263 - accuracy: 0.9816 - val_loss: 0.0412 - val_accuracy: 0.9595\n",
      "Epoch 2600/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9816\n",
      "Epoch 2600: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0263 - accuracy: 0.9816 - val_loss: 0.0411 - val_accuracy: 0.9595\n",
      "Epoch 2601/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9816\n",
      "Epoch 2601: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0263 - accuracy: 0.9816 - val_loss: 0.0411 - val_accuracy: 0.9595\n",
      "Epoch 2602/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9816\n",
      "Epoch 2602: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0263 - accuracy: 0.9816 - val_loss: 0.0411 - val_accuracy: 0.9595\n",
      "Epoch 2603/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9816\n",
      "Epoch 2603: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0263 - accuracy: 0.9816 - val_loss: 0.0411 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2604/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9816\n",
      "Epoch 2604: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0263 - accuracy: 0.9816 - val_loss: 0.0411 - val_accuracy: 0.9595\n",
      "Epoch 2605/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9816\n",
      "Epoch 2605: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0263 - accuracy: 0.9816 - val_loss: 0.0411 - val_accuracy: 0.9595\n",
      "Epoch 2606/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9816\n",
      "Epoch 2606: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0263 - accuracy: 0.9816 - val_loss: 0.0411 - val_accuracy: 0.9595\n",
      "Epoch 2607/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9816\n",
      "Epoch 2607: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0262 - accuracy: 0.9816 - val_loss: 0.0411 - val_accuracy: 0.9595\n",
      "Epoch 2608/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9816\n",
      "Epoch 2608: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0262 - accuracy: 0.9816 - val_loss: 0.0410 - val_accuracy: 0.9595\n",
      "Epoch 2609/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9816\n",
      "Epoch 2609: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0262 - accuracy: 0.9816 - val_loss: 0.0410 - val_accuracy: 0.9595\n",
      "Epoch 2610/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9816\n",
      "Epoch 2610: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0262 - accuracy: 0.9816 - val_loss: 0.0410 - val_accuracy: 0.9595\n",
      "Epoch 2611/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9816\n",
      "Epoch 2611: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0262 - accuracy: 0.9816 - val_loss: 0.0410 - val_accuracy: 0.9595\n",
      "Epoch 2612/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9816\n",
      "Epoch 2612: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0262 - accuracy: 0.9816 - val_loss: 0.0410 - val_accuracy: 0.9595\n",
      "Epoch 2613/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9816\n",
      "Epoch 2613: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0262 - accuracy: 0.9816 - val_loss: 0.0410 - val_accuracy: 0.9595\n",
      "Epoch 2614/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9816\n",
      "Epoch 2614: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0262 - accuracy: 0.9816 - val_loss: 0.0410 - val_accuracy: 0.9595\n",
      "Epoch 2615/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9816\n",
      "Epoch 2615: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0262 - accuracy: 0.9816 - val_loss: 0.0410 - val_accuracy: 0.9595\n",
      "Epoch 2616/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9816\n",
      "Epoch 2616: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0261 - accuracy: 0.9816 - val_loss: 0.0409 - val_accuracy: 0.9595\n",
      "Epoch 2617/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9816\n",
      "Epoch 2617: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0261 - accuracy: 0.9816 - val_loss: 0.0409 - val_accuracy: 0.9595\n",
      "Epoch 2618/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9816\n",
      "Epoch 2618: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0261 - accuracy: 0.9816 - val_loss: 0.0409 - val_accuracy: 0.9595\n",
      "Epoch 2619/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9816\n",
      "Epoch 2619: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0261 - accuracy: 0.9816 - val_loss: 0.0409 - val_accuracy: 0.9595\n",
      "Epoch 2620/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9816\n",
      "Epoch 2620: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0261 - accuracy: 0.9816 - val_loss: 0.0409 - val_accuracy: 0.9595\n",
      "Epoch 2621/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9816\n",
      "Epoch 2621: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0261 - accuracy: 0.9816 - val_loss: 0.0409 - val_accuracy: 0.9595\n",
      "Epoch 2622/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9816\n",
      "Epoch 2622: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0261 - accuracy: 0.9816 - val_loss: 0.0409 - val_accuracy: 0.9595\n",
      "Epoch 2623/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9816\n",
      "Epoch 2623: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0261 - accuracy: 0.9816 - val_loss: 0.0409 - val_accuracy: 0.9595\n",
      "Epoch 2624/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9816\n",
      "Epoch 2624: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0261 - accuracy: 0.9816 - val_loss: 0.0409 - val_accuracy: 0.9595\n",
      "Epoch 2625/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9816\n",
      "Epoch 2625: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0260 - accuracy: 0.9816 - val_loss: 0.0408 - val_accuracy: 0.9595\n",
      "Epoch 2626/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9816\n",
      "Epoch 2626: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0260 - accuracy: 0.9816 - val_loss: 0.0408 - val_accuracy: 0.9595\n",
      "Epoch 2627/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9816\n",
      "Epoch 2627: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0260 - accuracy: 0.9816 - val_loss: 0.0408 - val_accuracy: 0.9595\n",
      "Epoch 2628/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9816\n",
      "Epoch 2628: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0260 - accuracy: 0.9816 - val_loss: 0.0408 - val_accuracy: 0.9595\n",
      "Epoch 2629/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9816\n",
      "Epoch 2629: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0260 - accuracy: 0.9816 - val_loss: 0.0408 - val_accuracy: 0.9595\n",
      "Epoch 2630/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9816\n",
      "Epoch 2630: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0260 - accuracy: 0.9816 - val_loss: 0.0408 - val_accuracy: 0.9595\n",
      "Epoch 2631/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9816\n",
      "Epoch 2631: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0260 - accuracy: 0.9816 - val_loss: 0.0408 - val_accuracy: 0.9595\n",
      "Epoch 2632/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9816\n",
      "Epoch 2632: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0260 - accuracy: 0.9816 - val_loss: 0.0408 - val_accuracy: 0.9595\n",
      "Epoch 2633/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9816\n",
      "Epoch 2633: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0260 - accuracy: 0.9816 - val_loss: 0.0407 - val_accuracy: 0.9595\n",
      "Epoch 2634/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9816\n",
      "Epoch 2634: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0259 - accuracy: 0.9816 - val_loss: 0.0407 - val_accuracy: 0.9595\n",
      "Epoch 2635/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9816\n",
      "Epoch 2635: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0259 - accuracy: 0.9816 - val_loss: 0.0407 - val_accuracy: 0.9595\n",
      "Epoch 2636/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9816\n",
      "Epoch 2636: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0259 - accuracy: 0.9816 - val_loss: 0.0407 - val_accuracy: 0.9595\n",
      "Epoch 2637/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9816\n",
      "Epoch 2637: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0259 - accuracy: 0.9816 - val_loss: 0.0407 - val_accuracy: 0.9595\n",
      "Epoch 2638/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9826\n",
      "Epoch 2638: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0259 - accuracy: 0.9826 - val_loss: 0.0407 - val_accuracy: 0.9595\n",
      "Epoch 2639/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9826\n",
      "Epoch 2639: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0259 - accuracy: 0.9826 - val_loss: 0.0407 - val_accuracy: 0.9595\n",
      "Epoch 2640/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9826\n",
      "Epoch 2640: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0259 - accuracy: 0.9826 - val_loss: 0.0407 - val_accuracy: 0.9595\n",
      "Epoch 2641/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9826\n",
      "Epoch 2641: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0259 - accuracy: 0.9826 - val_loss: 0.0407 - val_accuracy: 0.9595\n",
      "Epoch 2642/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9826\n",
      "Epoch 2642: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0259 - accuracy: 0.9826 - val_loss: 0.0406 - val_accuracy: 0.9595\n",
      "Epoch 2643/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2643: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0406 - val_accuracy: 0.9595\n",
      "Epoch 2644/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2644: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0406 - val_accuracy: 0.9595\n",
      "Epoch 2645/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2645: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0406 - val_accuracy: 0.9595\n",
      "Epoch 2646/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2646: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0406 - val_accuracy: 0.9595\n",
      "Epoch 2647/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2647: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0406 - val_accuracy: 0.9595\n",
      "Epoch 2648/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2648: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0406 - val_accuracy: 0.9595\n",
      "Epoch 2649/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2649: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0406 - val_accuracy: 0.9595\n",
      "Epoch 2650/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2650: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0405 - val_accuracy: 0.9595\n",
      "Epoch 2651/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2651: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0405 - val_accuracy: 0.9595\n",
      "Epoch 2652/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9826\n",
      "Epoch 2652: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0258 - accuracy: 0.9826 - val_loss: 0.0405 - val_accuracy: 0.9595\n",
      "Epoch 2653/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9826\n",
      "Epoch 2653: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0257 - accuracy: 0.9826 - val_loss: 0.0405 - val_accuracy: 0.9595\n",
      "Epoch 2654/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9826\n",
      "Epoch 2654: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0257 - accuracy: 0.9826 - val_loss: 0.0405 - val_accuracy: 0.9595\n",
      "Epoch 2655/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9826\n",
      "Epoch 2655: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0257 - accuracy: 0.9826 - val_loss: 0.0405 - val_accuracy: 0.9595\n",
      "Epoch 2656/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9826\n",
      "Epoch 2656: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0257 - accuracy: 0.9826 - val_loss: 0.0405 - val_accuracy: 0.9595\n",
      "Epoch 2657/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9826\n",
      "Epoch 2657: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0257 - accuracy: 0.9826 - val_loss: 0.0405 - val_accuracy: 0.9595\n",
      "Epoch 2658/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9826\n",
      "Epoch 2658: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0257 - accuracy: 0.9826 - val_loss: 0.0405 - val_accuracy: 0.9595\n",
      "Epoch 2659/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9826\n",
      "Epoch 2659: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0257 - accuracy: 0.9826 - val_loss: 0.0404 - val_accuracy: 0.9595\n",
      "Epoch 2660/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9826\n",
      "Epoch 2660: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0257 - accuracy: 0.9826 - val_loss: 0.0404 - val_accuracy: 0.9595\n",
      "Epoch 2661/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9826\n",
      "Epoch 2661: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0257 - accuracy: 0.9826 - val_loss: 0.0404 - val_accuracy: 0.9595\n",
      "Epoch 2662/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9826\n",
      "Epoch 2662: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0256 - accuracy: 0.9826 - val_loss: 0.0404 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2663/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9826\n",
      "Epoch 2663: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0256 - accuracy: 0.9826 - val_loss: 0.0404 - val_accuracy: 0.9595\n",
      "Epoch 2664/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9826\n",
      "Epoch 2664: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0256 - accuracy: 0.9826 - val_loss: 0.0404 - val_accuracy: 0.9595\n",
      "Epoch 2665/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9826\n",
      "Epoch 2665: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0256 - accuracy: 0.9826 - val_loss: 0.0404 - val_accuracy: 0.9595\n",
      "Epoch 2666/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9826\n",
      "Epoch 2666: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0256 - accuracy: 0.9826 - val_loss: 0.0404 - val_accuracy: 0.9595\n",
      "Epoch 2667/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9826\n",
      "Epoch 2667: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0256 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9595\n",
      "Epoch 2668/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9826\n",
      "Epoch 2668: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0256 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9595\n",
      "Epoch 2669/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9826\n",
      "Epoch 2669: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0256 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9595\n",
      "Epoch 2670/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9826\n",
      "Epoch 2670: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0256 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9595\n",
      "Epoch 2671/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2671: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9595\n",
      "Epoch 2672/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2672: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9595\n",
      "Epoch 2673/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2673: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9595\n",
      "Epoch 2674/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2674: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9595\n",
      "Epoch 2675/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2675: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9595\n",
      "Epoch 2676/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2676: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9595\n",
      "Epoch 2677/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2677: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9595\n",
      "Epoch 2678/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2678: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9595\n",
      "Epoch 2679/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2679: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9595\n",
      "Epoch 2680/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9826\n",
      "Epoch 2680: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0255 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9595\n",
      "Epoch 2681/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9826\n",
      "Epoch 2681: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0254 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9595\n",
      "Epoch 2682/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9826\n",
      "Epoch 2682: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0254 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9595\n",
      "Epoch 2683/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9826\n",
      "Epoch 2683: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0254 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9595\n",
      "Epoch 2684/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9826\n",
      "Epoch 2684: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0254 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9595\n",
      "Epoch 2685/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9826\n",
      "Epoch 2685: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0254 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9595\n",
      "Epoch 2686/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9826\n",
      "Epoch 2686: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0254 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9595\n",
      "Epoch 2687/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9826\n",
      "Epoch 2687: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0254 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9595\n",
      "Epoch 2688/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9826\n",
      "Epoch 2688: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0254 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9595\n",
      "Epoch 2689/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9826\n",
      "Epoch 2689: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0254 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9595\n",
      "Epoch 2690/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9826\n",
      "Epoch 2690: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0253 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9595\n",
      "Epoch 2691/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9826\n",
      "Epoch 2691: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0253 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9595\n",
      "Epoch 2692/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9826\n",
      "Epoch 2692: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0253 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9595\n",
      "Epoch 2693/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9826\n",
      "Epoch 2693: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0253 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9595\n",
      "Epoch 2694/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9826\n",
      "Epoch 2694: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0253 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9595\n",
      "Epoch 2695/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9826\n",
      "Epoch 2695: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0253 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9595\n",
      "Epoch 2696/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9826\n",
      "Epoch 2696: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0253 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9595\n",
      "Epoch 2697/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9826\n",
      "Epoch 2697: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0253 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9595\n",
      "Epoch 2698/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9826\n",
      "Epoch 2698: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0253 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9595\n",
      "Epoch 2699/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2699: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9595\n",
      "Epoch 2700/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2700: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9595\n",
      "Epoch 2701/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2701: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9595\n",
      "Epoch 2702/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2702: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0399 - val_accuracy: 0.9595\n",
      "Epoch 2703/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2703: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0400 - val_accuracy: 0.9595\n",
      "Epoch 2704/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2704: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0399 - val_accuracy: 0.9595\n",
      "Epoch 2705/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2705: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0399 - val_accuracy: 0.9595\n",
      "Epoch 2706/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2706: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0399 - val_accuracy: 0.9595\n",
      "Epoch 2707/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2707: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0399 - val_accuracy: 0.9595\n",
      "Epoch 2708/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9826\n",
      "Epoch 2708: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0252 - accuracy: 0.9826 - val_loss: 0.0399 - val_accuracy: 0.9595\n",
      "Epoch 2709/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2709: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0399 - val_accuracy: 0.9595\n",
      "Epoch 2710/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2710: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0399 - val_accuracy: 0.9595\n",
      "Epoch 2711/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2711: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0398 - val_accuracy: 0.9595\n",
      "Epoch 2712/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2712: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0398 - val_accuracy: 0.9595\n",
      "Epoch 2713/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2713: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0398 - val_accuracy: 0.9595\n",
      "Epoch 2714/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2714: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0398 - val_accuracy: 0.9595\n",
      "Epoch 2715/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2715: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0398 - val_accuracy: 0.9595\n",
      "Epoch 2716/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2716: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0398 - val_accuracy: 0.9595\n",
      "Epoch 2717/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2717: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0398 - val_accuracy: 0.9595\n",
      "Epoch 2718/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9826\n",
      "Epoch 2718: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0251 - accuracy: 0.9826 - val_loss: 0.0398 - val_accuracy: 0.9595\n",
      "Epoch 2719/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9826\n",
      "Epoch 2719: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0250 - accuracy: 0.9826 - val_loss: 0.0398 - val_accuracy: 0.9595\n",
      "Epoch 2720/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9826\n",
      "Epoch 2720: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0250 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n",
      "Epoch 2721/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9826\n",
      "Epoch 2721: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0250 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2722/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9826\n",
      "Epoch 2722: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0250 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n",
      "Epoch 2723/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9826\n",
      "Epoch 2723: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0250 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n",
      "Epoch 2724/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9826\n",
      "Epoch 2724: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0250 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n",
      "Epoch 2725/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9826\n",
      "Epoch 2725: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0250 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n",
      "Epoch 2726/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9826\n",
      "Epoch 2726: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0250 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n",
      "Epoch 2727/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9826\n",
      "Epoch 2727: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0250 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n",
      "Epoch 2728/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2728: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n",
      "Epoch 2729/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2729: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0397 - val_accuracy: 0.9595\n",
      "Epoch 2730/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2730: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0396 - val_accuracy: 0.9595\n",
      "Epoch 2731/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2731: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0396 - val_accuracy: 0.9595\n",
      "Epoch 2732/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2732: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0396 - val_accuracy: 0.9595\n",
      "Epoch 2733/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2733: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0396 - val_accuracy: 0.9595\n",
      "Epoch 2734/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2734: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0396 - val_accuracy: 0.9595\n",
      "Epoch 2735/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2735: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0396 - val_accuracy: 0.9595\n",
      "Epoch 2736/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2736: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0396 - val_accuracy: 0.9595\n",
      "Epoch 2737/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9826\n",
      "Epoch 2737: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0249 - accuracy: 0.9826 - val_loss: 0.0396 - val_accuracy: 0.9595\n",
      "Epoch 2738/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2738: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0396 - val_accuracy: 0.9595\n",
      "Epoch 2739/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2739: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0395 - val_accuracy: 0.9595\n",
      "Epoch 2740/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2740: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0395 - val_accuracy: 0.9595\n",
      "Epoch 2741/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2741: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0395 - val_accuracy: 0.9595\n",
      "Epoch 2742/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2742: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0395 - val_accuracy: 0.9595\n",
      "Epoch 2743/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2743: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0395 - val_accuracy: 0.9595\n",
      "Epoch 2744/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2744: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0395 - val_accuracy: 0.9595\n",
      "Epoch 2745/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2745: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0395 - val_accuracy: 0.9595\n",
      "Epoch 2746/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2746: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0395 - val_accuracy: 0.9595\n",
      "Epoch 2747/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9826\n",
      "Epoch 2747: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0248 - accuracy: 0.9826 - val_loss: 0.0395 - val_accuracy: 0.9595\n",
      "Epoch 2748/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2748: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2749/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2749: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2750/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2750: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2751/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2751: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2752/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2752: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2753/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2753: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2754/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2754: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2755/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2755: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2756/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2756: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2757/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9826\n",
      "Epoch 2757: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0247 - accuracy: 0.9826 - val_loss: 0.0394 - val_accuracy: 0.9595\n",
      "Epoch 2758/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2758: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0393 - val_accuracy: 0.9595\n",
      "Epoch 2759/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2759: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0393 - val_accuracy: 0.9595\n",
      "Epoch 2760/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2760: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0393 - val_accuracy: 0.9595\n",
      "Epoch 2761/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2761: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0393 - val_accuracy: 0.9595\n",
      "Epoch 2762/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2762: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0393 - val_accuracy: 0.9595\n",
      "Epoch 2763/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2763: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0393 - val_accuracy: 0.9595\n",
      "Epoch 2764/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2764: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0393 - val_accuracy: 0.9595\n",
      "Epoch 2765/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2765: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0393 - val_accuracy: 0.9595\n",
      "Epoch 2766/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2766: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0393 - val_accuracy: 0.9595\n",
      "Epoch 2767/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9826\n",
      "Epoch 2767: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0246 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2768/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2768: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2769/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2769: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2770/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2770: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2771/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2771: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2772/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2772: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2773/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2773: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2774/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2774: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2775/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2775: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2776/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2776: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0392 - val_accuracy: 0.9595\n",
      "Epoch 2777/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9826\n",
      "Epoch 2777: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0245 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9595\n",
      "Epoch 2778/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2778: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9595\n",
      "Epoch 2779/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2779: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9595\n",
      "Epoch 2780/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2780: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2781/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2781: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9595\n",
      "Epoch 2782/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2782: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9595\n",
      "Epoch 2783/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2783: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9595\n",
      "Epoch 2784/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2784: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9595\n",
      "Epoch 2785/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2785: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0391 - val_accuracy: 0.9595\n",
      "Epoch 2786/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2786: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2787/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9826\n",
      "Epoch 2787: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0244 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2788/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2788: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2789/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2789: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2790/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2790: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2791/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2791: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2792/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2792: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2793/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2793: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2794/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2794: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2795/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2795: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0390 - val_accuracy: 0.9595\n",
      "Epoch 2796/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2796: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9595\n",
      "Epoch 2797/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9826\n",
      "Epoch 2797: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0243 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9595\n",
      "Epoch 2798/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2798: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9595\n",
      "Epoch 2799/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2799: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9595\n",
      "Epoch 2800/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2800: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9595\n",
      "Epoch 2801/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2801: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9595\n",
      "Epoch 2802/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2802: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9595\n",
      "Epoch 2803/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2803: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9595\n",
      "Epoch 2804/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2804: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0389 - val_accuracy: 0.9595\n",
      "Epoch 2805/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2805: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2806/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2806: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2807/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9826\n",
      "Epoch 2807: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0242 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2808/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2808: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2809/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2809: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2810/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2810: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2811/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2811: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2812/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2812: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2813/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2813: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2814/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2814: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0388 - val_accuracy: 0.9595\n",
      "Epoch 2815/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2815: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2816/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2816: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2817/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9826\n",
      "Epoch 2817: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0241 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2818/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2818: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2819/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2819: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2820/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2820: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2821/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2821: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2822/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2822: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2823/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2823: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2824/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2824: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0387 - val_accuracy: 0.9595\n",
      "Epoch 2825/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2825: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2826/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2826: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2827/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2827: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2828/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9826\n",
      "Epoch 2828: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0240 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2829/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2829: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2830/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2830: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2831/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2831: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2832/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2832: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2833/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2833: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2834/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2834: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0386 - val_accuracy: 0.9595\n",
      "Epoch 2835/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2835: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n",
      "Epoch 2836/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2836: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n",
      "Epoch 2837/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2837: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n",
      "Epoch 2838/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9826\n",
      "Epoch 2838: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0239 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n",
      "Epoch 2839/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2839: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2840/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2840: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n",
      "Epoch 2841/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2841: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n",
      "Epoch 2842/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2842: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n",
      "Epoch 2843/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2843: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n",
      "Epoch 2844/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2844: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0385 - val_accuracy: 0.9595\n",
      "Epoch 2845/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2845: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2846/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2846: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2847/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2847: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2848/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9826\n",
      "Epoch 2848: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0238 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2849/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2849: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2850/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2850: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2851/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2851: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2852/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2852: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2853/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2853: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2854/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2854: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0384 - val_accuracy: 0.9595\n",
      "Epoch 2855/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2855: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2856/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2856: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2857/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2857: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2858/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2858: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2859/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9826\n",
      "Epoch 2859: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0237 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2860/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9826\n",
      "Epoch 2860: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0236 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2861/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9826\n",
      "Epoch 2861: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0236 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2862/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9826\n",
      "Epoch 2862: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0236 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2863/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9826\n",
      "Epoch 2863: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0236 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2864/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9826\n",
      "Epoch 2864: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0236 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2865/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9826\n",
      "Epoch 2865: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0236 - accuracy: 0.9826 - val_loss: 0.0383 - val_accuracy: 0.9595\n",
      "Epoch 2866/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9826\n",
      "Epoch 2866: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0236 - accuracy: 0.9826 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2867/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9835\n",
      "Epoch 2867: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0236 - accuracy: 0.9835 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2868/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9835\n",
      "Epoch 2868: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0236 - accuracy: 0.9835 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2869/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9835\n",
      "Epoch 2869: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0236 - accuracy: 0.9835 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2870/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2870: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2871/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2871: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2872/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2872: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2873/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2873: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2874/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2874: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2875/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2875: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0382 - val_accuracy: 0.9595\n",
      "Epoch 2876/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2876: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2877/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2877: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2878/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2878: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2879/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2879: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2880/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9835\n",
      "Epoch 2880: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0235 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2881/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2881: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2882/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2882: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2883/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2883: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2884/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2884: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2885/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2885: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0381 - val_accuracy: 0.9595\n",
      "Epoch 2886/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2886: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2887/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2887: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2888/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2888: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2889/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2889: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2890/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2890: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2891/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9835\n",
      "Epoch 2891: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0234 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2892/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2892: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2893/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2893: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2894/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2894: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2895/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2895: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2896/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2896: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0380 - val_accuracy: 0.9595\n",
      "Epoch 2897/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2897: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n",
      "Epoch 2898/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2898: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2899/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2899: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n",
      "Epoch 2900/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2900: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n",
      "Epoch 2901/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2901: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n",
      "Epoch 2902/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9835\n",
      "Epoch 2902: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0233 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n",
      "Epoch 2903/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2903: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n",
      "Epoch 2904/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2904: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n",
      "Epoch 2905/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2905: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n",
      "Epoch 2906/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2906: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9595\n",
      "Epoch 2907/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2907: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2908/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2908: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2909/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2909: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2910/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2910: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2911/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2911: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2912/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2912: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2913/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9835\n",
      "Epoch 2913: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0232 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2914/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2914: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2915/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2915: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2916/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2916: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2917/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2917: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0378 - val_accuracy: 0.9595\n",
      "Epoch 2918/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2918: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2919/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2919: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2920/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2920: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2921/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2921: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2922/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2922: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2923/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2923: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2924/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9835\n",
      "Epoch 2924: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0231 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2925/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2925: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2926/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2926: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2927/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2927: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2928/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2928: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0377 - val_accuracy: 0.9595\n",
      "Epoch 2929/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2929: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2930/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2930: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2931/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2931: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2932/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2932: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2933/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2933: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2934/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2934: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2935/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9835\n",
      "Epoch 2935: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0230 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2936/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2936: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2937/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2937: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2938/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2938: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2939/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2939: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0376 - val_accuracy: 0.9595\n",
      "Epoch 2940/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2940: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2941/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2941: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2942/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2942: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2943/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2943: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2944/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2944: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2945/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2945: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2946/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9835\n",
      "Epoch 2946: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0229 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2947/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2947: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2948/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2948: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2949/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2949: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0375 - val_accuracy: 0.9595\n",
      "Epoch 2950/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2950: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2951/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2951: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2952/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2952: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2953/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2953: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2954/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2954: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2955/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2955: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2956/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2956: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2957/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9835\n",
      "Epoch 2957: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0228 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2958/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9835\n",
      "Epoch 2958: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0227 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2959/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9835\n",
      "Epoch 2959: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0227 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2960/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9835\n",
      "Epoch 2960: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0227 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2961/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9835\n",
      "Epoch 2961: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0227 - accuracy: 0.9835 - val_loss: 0.0374 - val_accuracy: 0.9595\n",
      "Epoch 2962/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9845\n",
      "Epoch 2962: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0227 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2963/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9845\n",
      "Epoch 2963: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0227 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2964/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9845\n",
      "Epoch 2964: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0227 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2965/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9845\n",
      "Epoch 2965: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0227 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2966/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9845\n",
      "Epoch 2966: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0227 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2967/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9845\n",
      "Epoch 2967: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0227 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2968/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9845\n",
      "Epoch 2968: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0227 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2969/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2969: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2970/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2970: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2971/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2971: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2972/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2972: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0373 - val_accuracy: 0.9595\n",
      "Epoch 2973/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2973: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2974/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2974: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2975/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2975: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2976/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2976: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2977/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2977: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2978/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2978: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2979/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2979: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2980/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9845\n",
      "Epoch 2980: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0226 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2981/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2981: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2982/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2982: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2983/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2983: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9595\n",
      "Epoch 2984/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2984: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2985/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2985: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2986/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2986: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2987/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2987: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2988/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2988: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2989/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2989: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2990/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2990: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2991/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9845\n",
      "Epoch 2991: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0225 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2992/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 2992: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2993/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 2993: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2994/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 2994: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0371 - val_accuracy: 0.9595\n",
      "Epoch 2995/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 2995: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 2996/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 2996: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 2997/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 2997: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 2998/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 2998: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 2999/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 2999: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 3000/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 3000: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 3001/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 3001: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 3002/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 3002: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 3003/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9845\n",
      "Epoch 3003: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0224 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 3004/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3004: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 3005/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3005: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 3006/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3006: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0370 - val_accuracy: 0.9595\n",
      "Epoch 3007/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3007: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3008/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3008: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3009/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3009: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3010/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3010: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3011/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3011: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3012/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3012: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3013/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3013: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3014/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9845\n",
      "Epoch 3014: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0223 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3015/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3015: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3016/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3016: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3017/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3017: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3018/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3018: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9595\n",
      "Epoch 3019/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3019: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3020/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3020: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3021/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3021: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3022/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3022: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3023/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3023: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3024/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3024: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3025/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3025: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3026/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9845\n",
      "Epoch 3026: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0222 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3027/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3027: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3028/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3028: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3029/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3029: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0368 - val_accuracy: 0.9595\n",
      "Epoch 3030/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3030: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3031/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3031: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3032/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3032: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3033/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3033: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3034/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3034: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3035/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3035: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3036/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3036: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3037/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3037: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3038/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9845\n",
      "Epoch 3038: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0221 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3039/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3039: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3040/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3040: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3041/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3041: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0367 - val_accuracy: 0.9595\n",
      "Epoch 3042/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3042: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3043/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3043: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3044/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3044: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3045/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3045: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3046/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3046: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3047/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3047: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3048/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3048: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3049/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9845\n",
      "Epoch 3049: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0220 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3050/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3050: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3051/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3051: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3052/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3052: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3053/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3053: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9595\n",
      "Epoch 3054/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3054: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3055/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3055: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3056/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3056: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3057/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3057: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3058/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3058: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3059/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3059: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3060/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3060: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3061/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9845\n",
      "Epoch 3061: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0219 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3062/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3062: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3063/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3063: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3064/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3064: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3065/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3065: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0365 - val_accuracy: 0.9595\n",
      "Epoch 3066/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3066: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3067/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3067: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3068/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3068: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3069/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3069: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3070/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3070: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3071/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3071: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3072/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3072: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3073/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9845\n",
      "Epoch 3073: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0218 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3074/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3074: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3075/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3075: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3076/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3076: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3077/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3077: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9595\n",
      "Epoch 3078/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3078: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3079/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3079: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3080/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3080: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3081/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3081: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3082/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3082: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3083/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3083: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3084/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3084: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3085/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3085: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3086/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9845\n",
      "Epoch 3086: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0217 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3087/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3087: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3088/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3088: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3089/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3089: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3090/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3090: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0363 - val_accuracy: 0.9595\n",
      "Epoch 3091/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3091: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3092/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3092: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3093/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3093: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3094/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3094: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3095/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3095: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3096/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3096: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3097/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3097: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3098/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9845\n",
      "Epoch 3098: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0216 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3099/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3099: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3100/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3100: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3101/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3101: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3102/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3102: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0362 - val_accuracy: 0.9595\n",
      "Epoch 3103/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3103: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3104/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3104: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3105/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3105: val_accuracy did not improve from 0.95946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3106/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3106: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3107/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3107: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3108/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3108: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3109/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3109: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3110/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9845\n",
      "Epoch 3110: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0215 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3111/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3111: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3112/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3112: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3113/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3113: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3114/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3114: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3115/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3115: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0361 - val_accuracy: 0.9595\n",
      "Epoch 3116/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3116: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3117/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3117: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3118/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3118: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3119/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3119: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3120/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3120: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3121/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3121: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3122/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9845\n",
      "Epoch 3122: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0214 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3123/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3123: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3124/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3124: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3125/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3125: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3126/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3126: val_accuracy did not improve from 0.95946\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9595\n",
      "Epoch 3127/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3127: val_accuracy improved from 0.95946 to 0.96171, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0360 - val_accuracy: 0.9617\n",
      "Epoch 3128/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3128: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3129/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3129: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3130/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3130: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3131/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3131: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3132/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3132: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3133/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3133: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3134/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3134: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3135/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9845\n",
      "Epoch 3135: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0213 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3136/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3136: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3137/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3137: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3138/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3138: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3139/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3139: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3140/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3140: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0359 - val_accuracy: 0.9617\n",
      "Epoch 3141/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3141: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3142/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3142: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3143/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3143: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3144/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3144: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3145/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3145: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3146/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3146: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3147/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9845\n",
      "Epoch 3147: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0212 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3148/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3148: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3149/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3149: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3150/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3150: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3151/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3151: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3152/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3152: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3153/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3153: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0358 - val_accuracy: 0.9617\n",
      "Epoch 3154/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3154: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9617\n",
      "Epoch 3155/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3155: val_accuracy did not improve from 0.96171\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9617\n",
      "Epoch 3156/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3156: val_accuracy improved from 0.96171 to 0.96396, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3157/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3157: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9617\n",
      "Epoch 3158/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3158: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3159/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3159: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3160/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9845\n",
      "Epoch 3160: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0211 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3161/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3161: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3162/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3162: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3163/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3163: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3164/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3164: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3165/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3165: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3166/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3166: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0357 - val_accuracy: 0.9640\n",
      "Epoch 3167/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3167: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3168/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3168: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3169/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3169: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3170/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3170: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3171/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3171: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3172/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3172: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3173/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9845\n",
      "Epoch 3173: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0210 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3174/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3174: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3175/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3175: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3176/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3176: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3177/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3177: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3178/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3178: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3179/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3179: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3180/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3180: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0356 - val_accuracy: 0.9640\n",
      "Epoch 3181/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3181: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3182/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3182: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3183/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3183: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3184/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3184: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3185/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3185: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3186/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9845\n",
      "Epoch 3186: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0209 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3187/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3187: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3188/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3188: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3189/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3189: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3190/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3190: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3191/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3191: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3192/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3192: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3193/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3193: val_accuracy did not improve from 0.96396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0355 - val_accuracy: 0.9640\n",
      "Epoch 3194/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3194: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3195/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3195: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3196/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3196: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3197/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3197: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3198/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9845\n",
      "Epoch 3198: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0208 - accuracy: 0.9845 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3199/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9855\n",
      "Epoch 3199: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0208 - accuracy: 0.9855 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3200/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3200: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3201/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3201: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3202/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3202: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3203/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3203: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3204/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3204: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3205/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3205: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3206/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3206: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3207/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3207: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0354 - val_accuracy: 0.9640\n",
      "Epoch 3208/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3208: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3209/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3209: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3210/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3210: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3211/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3211: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3212/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9855\n",
      "Epoch 3212: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0207 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3213/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3213: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3214/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3214: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3215/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3215: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3216/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3216: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3217/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3217: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3218/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3218: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3219/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3219: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3220/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3220: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0353 - val_accuracy: 0.9640\n",
      "Epoch 3221/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3221: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3222/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3222: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3223/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3223: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3224/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3224: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3225/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9855\n",
      "Epoch 3225: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0206 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3226/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3226: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3227/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3227: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3228/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3228: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3229/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3229: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3230/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3230: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3231/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3231: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3232/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3232: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3233/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3233: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3234/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3234: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9640\n",
      "Epoch 3235/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3235: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3236/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3236: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3237/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3237: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3238/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9855\n",
      "Epoch 3238: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0205 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3239/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3239: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3240/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3240: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3241/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3241: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3242/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3242: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3243/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3243: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3244/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3244: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3245/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3245: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3246/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3246: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3247/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3247: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3248/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3248: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0351 - val_accuracy: 0.9640\n",
      "Epoch 3249/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3249: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3250/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3250: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3251/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9855\n",
      "Epoch 3251: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0204 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3252/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3252: val_accuracy did not improve from 0.96396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3253/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3253: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3254/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3254: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3255/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3255: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3256/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3256: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3257/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3257: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3258/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3258: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3259/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3259: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3260/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3260: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3261/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3261: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3262/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3262: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3263/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3263: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9640\n",
      "Epoch 3264/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3264: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9640\n",
      "Epoch 3265/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9855\n",
      "Epoch 3265: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0203 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9640\n",
      "Epoch 3266/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3266: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9640\n",
      "Epoch 3267/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3267: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9640\n",
      "Epoch 3268/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3268: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9640\n",
      "Epoch 3269/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3269: val_accuracy did not improve from 0.96396\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9640\n",
      "Epoch 3270/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3270: val_accuracy improved from 0.96396 to 0.96622, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9662\n",
      "Epoch 3271/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3271: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9662\n",
      "Epoch 3272/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3272: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9662\n",
      "Epoch 3273/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3273: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9662\n",
      "Epoch 3274/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3274: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9662\n",
      "Epoch 3275/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3275: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9662\n",
      "Epoch 3276/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3276: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9662\n",
      "Epoch 3277/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3277: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0349 - val_accuracy: 0.9662\n",
      "Epoch 3278/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9855\n",
      "Epoch 3278: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0202 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3279/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3279: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3280/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3280: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3281/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3281: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3282/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3282: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3283/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3283: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3284/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3284: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3285/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3285: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3286/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3286: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3287/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3287: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3288/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3288: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3289/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3289: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3290/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3290: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3291/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3291: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3292/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9855\n",
      "Epoch 3292: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0201 - accuracy: 0.9855 - val_loss: 0.0348 - val_accuracy: 0.9662\n",
      "Epoch 3293/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3293: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3294/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3294: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3295/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3295: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3296/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3296: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3297/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3297: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3298/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3298: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3299/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3299: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3300/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3300: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3301/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3301: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3302/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3302: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3303/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3303: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3304/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3304: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3305/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3305: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3306/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9855\n",
      "Epoch 3306: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0200 - accuracy: 0.9855 - val_loss: 0.0347 - val_accuracy: 0.9662\n",
      "Epoch 3307/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3307: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3308/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3308: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3309/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3309: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3310/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3310: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3311/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3311: val_accuracy did not improve from 0.96622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3312/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3312: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3313/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3313: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3314/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3314: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3315/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3315: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3316/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3316: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3317/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3317: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3318/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3318: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3319/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3319: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3320/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9855\n",
      "Epoch 3320: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0199 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3321/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3321: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0346 - val_accuracy: 0.9662\n",
      "Epoch 3322/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3322: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3323/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3323: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3324/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3324: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3325/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3325: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3326/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3326: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3327/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3327: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3328/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3328: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3329/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3329: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3330/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9855\n",
      "Epoch 3330: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0198 - accuracy: 0.9855 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3331/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9864\n",
      "Epoch 3331: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0198 - accuracy: 0.9864 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3332/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9864\n",
      "Epoch 3332: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0198 - accuracy: 0.9864 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3333/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9874\n",
      "Epoch 3333: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0198 - accuracy: 0.9874 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3334/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9874\n",
      "Epoch 3334: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0198 - accuracy: 0.9874 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3335/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3335: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3336/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3336: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0345 - val_accuracy: 0.9662\n",
      "Epoch 3337/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3337: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3338/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3338: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3339/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3339: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3340/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3340: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3341/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3341: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3342/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3342: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3343/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3343: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3344/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3344: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3345/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3345: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3346/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3346: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3347/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3347: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3348/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9874\n",
      "Epoch 3348: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0197 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3349/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3349: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3350/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3350: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3351/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3351: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3352/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3352: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0344 - val_accuracy: 0.9662\n",
      "Epoch 3353/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3353: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3354/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3354: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3355/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3355: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3356/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3356: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3357/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3357: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3358/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3358: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3359/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3359: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3360/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3360: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3361/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3361: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3362/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9874\n",
      "Epoch 3362: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0196 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3363/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3363: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3364/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3364: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3365/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3365: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3366/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3366: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3367/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3367: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0343 - val_accuracy: 0.9662\n",
      "Epoch 3368/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3368: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3369/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3369: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3370/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3370: val_accuracy did not improve from 0.96622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3371/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3371: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3372/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3372: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3373/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3373: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3374/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3374: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3375/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3375: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3376/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9874\n",
      "Epoch 3376: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0195 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3377/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3377: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3378/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3378: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3379/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3379: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3380/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3380: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3381/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3381: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3382/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3382: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3383/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3383: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3384/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3384: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9662\n",
      "Epoch 3385/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3385: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3386/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3386: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3387/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3387: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3388/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3388: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3389/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3389: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3390/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3390: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3391/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9874\n",
      "Epoch 3391: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0194 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3392/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3392: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3393/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3393: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3394/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3394: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3395/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3395: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3396/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3396: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3397/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3397: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3398/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3398: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n",
      "Epoch 3399/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3399: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3400/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3400: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3401/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3401: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3402/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3402: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3403/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3403: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3404/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3404: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3405/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9874\n",
      "Epoch 3405: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0193 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3406/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3406: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3407/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3407: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3408/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3408: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3409/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3409: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3410/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3410: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3411/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3411: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3412/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3412: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3413/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3413: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3414/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3414: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3415/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3415: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0340 - val_accuracy: 0.9662\n",
      "Epoch 3416/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3416: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9662\n",
      "Epoch 3417/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3417: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9662\n",
      "Epoch 3418/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3418: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9662\n",
      "Epoch 3419/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3419: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9662\n",
      "Epoch 3420/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9874\n",
      "Epoch 3420: val_accuracy did not improve from 0.96622\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0192 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9662\n",
      "Epoch 3421/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3421: val_accuracy improved from 0.96622 to 0.96847, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3422/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3422: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3423/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3423: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3424/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3424: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3425/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3425: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3426/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3426: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3427/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3427: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3428/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3428: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3429/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3429: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3430/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3430: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3431/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3431: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3432/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3432: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0339 - val_accuracy: 0.9685\n",
      "Epoch 3433/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3433: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3434/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3434: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3435/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9874\n",
      "Epoch 3435: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0191 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3436/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3436: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3437/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3437: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3438/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3438: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3439/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3439: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3440/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3440: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3441/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3441: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3442/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3442: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3443/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3443: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3444/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3444: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3445/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3445: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3446/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3446: val_accuracy did not improve from 0.96847\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9685\n",
      "Epoch 3447/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3447: val_accuracy improved from 0.96847 to 0.97072, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9707\n",
      "Epoch 3448/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3448: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0338 - val_accuracy: 0.9707\n",
      "Epoch 3449/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3449: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3450/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9874\n",
      "Epoch 3450: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0190 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3451/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3451: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3452/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3452: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3453/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3453: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3454/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3454: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3455/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3455: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3456/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3456: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3457/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3457: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3458/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3458: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3459/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3459: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3460/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3460: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3461/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3461: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3462/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3462: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3463/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3463: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3464/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3464: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3465/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9874\n",
      "Epoch 3465: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0189 - accuracy: 0.9874 - val_loss: 0.0337 - val_accuracy: 0.9707\n",
      "Epoch 3466/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3466: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9707\n",
      "Epoch 3467/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3467: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9707\n",
      "Epoch 3468/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3468: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9707\n",
      "Epoch 3469/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3469: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9707\n",
      "Epoch 3470/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3470: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9707\n",
      "Epoch 3471/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3471: val_accuracy did not improve from 0.97072\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9707\n",
      "Epoch 3472/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3472: val_accuracy improved from 0.97072 to 0.97297, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3473/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3473: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3474/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3474: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3475/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3475: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3476/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3476: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3477/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3477: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3478/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3478: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3479/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3479: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3480/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9874\n",
      "Epoch 3480: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0188 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3481/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3481: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3482/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3482: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3483/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3483: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0336 - val_accuracy: 0.9730\n",
      "Epoch 3484/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3484: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3485/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3485: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3486/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3486: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3487/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3487: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3488/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3488: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3489/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3489: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3490/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3490: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3491/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3491: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3492/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3492: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3493/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3493: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3494/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3494: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3495/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9874\n",
      "Epoch 3495: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0187 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3496/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3496: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3497/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3497: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3498/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3498: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3499/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3499: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3500/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3500: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0335 - val_accuracy: 0.9730\n",
      "Epoch 3501/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3501: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3502/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3502: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3503/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3503: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3504/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3504: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3505/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3505: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3506/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3506: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3507/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3507: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3508/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3508: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3509/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3509: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3510/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9874\n",
      "Epoch 3510: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0186 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3511/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3511: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3512/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3512: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3513/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3513: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3514/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3514: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3515/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3515: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3516/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3516: val_accuracy did not improve from 0.97297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3517/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3517: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3518/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3518: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0334 - val_accuracy: 0.9730\n",
      "Epoch 3519/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3519: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9730\n",
      "Epoch 3520/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3520: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9730\n",
      "Epoch 3521/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3521: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9730\n",
      "Epoch 3522/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3522: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9730\n",
      "Epoch 3523/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3523: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9730\n",
      "Epoch 3524/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3524: val_accuracy did not improve from 0.97297\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9730\n",
      "Epoch 3525/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3525: val_accuracy improved from 0.97297 to 0.97523, saving model to Best_Logistic_before_over_Sampling_Batch.h5\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3526/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9874\n",
      "Epoch 3526: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0185 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3527/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3527: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3528/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3528: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3529/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3529: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3530/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3530: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3531/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3531: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3532/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3532: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3533/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3533: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3534/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3534: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3535/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3535: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3536/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3536: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0333 - val_accuracy: 0.9752\n",
      "Epoch 3537/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3537: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3538/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3538: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3539/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3539: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3540/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3540: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3541/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3541: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3542/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9874\n",
      "Epoch 3542: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0184 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3543/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3543: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3544/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3544: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3545/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3545: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3546/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3546: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3547/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3547: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3548/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3548: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3549/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3549: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3550/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3550: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3551/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3551: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3552/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3552: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3553/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3553: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3554/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3554: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0332 - val_accuracy: 0.9752\n",
      "Epoch 3555/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3555: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3556/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3556: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3557/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9874\n",
      "Epoch 3557: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0183 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3558/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3558: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3559/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3559: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3560/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3560: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3561/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3561: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3562/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3562: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3563/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3563: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3564/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3564: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3565/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3565: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3566/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3566: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3567/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3567: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3568/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3568: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3569/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3569: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3570/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3570: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3571/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3571: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3572/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3572: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3573/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9874\n",
      "Epoch 3573: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0182 - accuracy: 0.9874 - val_loss: 0.0331 - val_accuracy: 0.9752\n",
      "Epoch 3574/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9874\n",
      "Epoch 3574: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0181 - accuracy: 0.9874 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3575/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9874\n",
      "Epoch 3575: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0181 - accuracy: 0.9874 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3576/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3576: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3577/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3577: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3578/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3578: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3579/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3579: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3580/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3580: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3581/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3581: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3582/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3582: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3583/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3583: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3584/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3584: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3585/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3585: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3586/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3586: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3587/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3587: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3588/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3588: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3589/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9864\n",
      "Epoch 3589: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0181 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3590/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3590: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3591/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3591: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3592/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3592: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0330 - val_accuracy: 0.9752\n",
      "Epoch 3593/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3593: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3594/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3594: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3595/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3595: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3596/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3596: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3597/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3597: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3598/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3598: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3599/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3599: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3600/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3600: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3601/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3601: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3602/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3602: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3603/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3603: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3604/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3604: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3605/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3605: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3606/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9864\n",
      "Epoch 3606: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0180 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3607/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3607: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3608/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3608: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3609/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3609: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3610/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3610: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3611/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3611: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0329 - val_accuracy: 0.9752\n",
      "Epoch 3612/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3612: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3613/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3613: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3614/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3614: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3615/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3615: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3616/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3616: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3617/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3617: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3618/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3618: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3619/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3619: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3620/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3620: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3621/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3621: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3622/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9864\n",
      "Epoch 3622: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0179 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3623/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3623: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3624/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3624: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3625/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3625: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3626/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3626: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3627/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3627: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3628/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3628: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3629/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3629: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3630/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3630: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3631/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3631: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0328 - val_accuracy: 0.9752\n",
      "Epoch 3632/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3632: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3633/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3633: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3634/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3634: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3635/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3635: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3636/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3636: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3637/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3637: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3638/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9864\n",
      "Epoch 3638: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0178 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3639/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3639: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3640/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3640: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3641/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3641: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3642/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3642: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3643/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3643: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3644/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3644: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3645/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3645: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3646/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3646: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3647/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3647: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3648/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3648: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3649/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3649: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3650/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3650: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3651/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3651: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0327 - val_accuracy: 0.9752\n",
      "Epoch 3652/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3652: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3653/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3653: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3654/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3654: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3655/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9864\n",
      "Epoch 3655: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0177 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3656/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3656: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3657/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3657: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3658/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3658: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3659/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3659: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3660/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3660: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3661/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3661: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3662/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3662: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3663/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3663: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3664/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3664: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3665/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3665: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3666/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3666: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3667/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3667: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3668/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3668: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3669/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3669: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3670/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3670: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3671/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3671: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9752\n",
      "Epoch 3672/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9864\n",
      "Epoch 3672: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0176 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3673/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3673: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3674/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3674: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3675/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3675: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3676/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3676: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3677/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3677: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3678/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3678: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3679/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3679: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3680/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3680: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3681/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3681: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3682/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3682: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3683/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3683: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3684/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3684: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3685/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3685: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3686/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3686: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3687/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3687: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3688/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3688: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3689/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9864\n",
      "Epoch 3689: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0175 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3690/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3690: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3691/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3691: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3692/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3692: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0325 - val_accuracy: 0.9752\n",
      "Epoch 3693/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3693: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3694/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3694: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3695/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3695: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3696/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3696: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3697/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3697: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3698/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3698: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3699/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3699: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3700/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3700: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3701/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3701: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3702/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3702: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3703/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3703: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3704/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3704: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3705/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3705: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3706/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9864\n",
      "Epoch 3706: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0174 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3707/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3707: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3708/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3708: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3709/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3709: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3710/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3710: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3711/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3711: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3712/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3712: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3713/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3713: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0324 - val_accuracy: 0.9752\n",
      "Epoch 3714/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3714: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3715/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3715: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3716/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3716: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3717/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3717: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3718/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3718: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3719/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3719: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3720/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3720: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3721/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3721: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3722/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3722: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3723/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9864\n",
      "Epoch 3723: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0173 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3724/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3724: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3725/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3725: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3726/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3726: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3727/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3727: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3728/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3728: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3729/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3729: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3730/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3730: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3731/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3731: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3732/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3732: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3733/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3733: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3734/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3734: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0323 - val_accuracy: 0.9752\n",
      "Epoch 3735/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3735: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3736/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3736: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3737/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3737: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3738/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3738: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3739/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3739: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3740/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9864\n",
      "Epoch 3740: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0172 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3741/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3741: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3742/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3742: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3743/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3743: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3744/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3744: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3745/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3745: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3746/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3746: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3747/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3747: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3748/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3748: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3749/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3749: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3750/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3750: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3751/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3751: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3752/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3752: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3753/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3753: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3754/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3754: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3755/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3755: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3756/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3756: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0322 - val_accuracy: 0.9752\n",
      "Epoch 3757/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3757: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3758/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9864\n",
      "Epoch 3758: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3759/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3759: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3760/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3760: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3761/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3761: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3762/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3762: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3763/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3763: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3764/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3764: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3765/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3765: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3766/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3766: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3767/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3767: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3768/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3768: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3769/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3769: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3770/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3770: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3771/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9864\n",
      "Epoch 3771: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0170 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3772/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9874\n",
      "Epoch 3772: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0170 - accuracy: 0.9874 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3773/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9874\n",
      "Epoch 3773: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0170 - accuracy: 0.9874 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3774/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9874\n",
      "Epoch 3774: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0170 - accuracy: 0.9874 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3775/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9874\n",
      "Epoch 3775: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0170 - accuracy: 0.9874 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3776/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3776: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3777/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3777: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3778/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3778: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3779/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3779: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0321 - val_accuracy: 0.9752\n",
      "Epoch 3780/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3780: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3781/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3781: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3782/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3782: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3783/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3783: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3784/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3784: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3785/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3785: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3786/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3786: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3787/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3787: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3788/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3788: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3789/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3789: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3790/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3790: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3791/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3791: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3792/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3792: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3793/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9874\n",
      "Epoch 3793: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0169 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3794/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3794: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3795/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3795: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3796/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3796: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3797/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3797: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3798/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3798: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3799/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3799: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3800/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3800: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3801/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3801: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3802/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3802: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9752\n",
      "Epoch 3803/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3803: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3804/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3804: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3805/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3805: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3806/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3806: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3807/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3807: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3808/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3808: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3809/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3809: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3810/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3810: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3811/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9874\n",
      "Epoch 3811: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0168 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3812/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3812: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3813/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3813: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3814/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3814: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3815/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3815: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3816/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3816: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3817/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3817: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3818/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3818: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3819/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3819: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3820/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3820: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3821/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3821: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3822/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3822: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3823/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3823: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3824/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3824: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3825/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3825: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0319 - val_accuracy: 0.9752\n",
      "Epoch 3826/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3826: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3827/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3827: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3828/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3828: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3829/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9874\n",
      "Epoch 3829: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0167 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3830/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3830: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3831/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3831: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3832/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3832: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3833/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3833: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3834/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3834: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3835/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3835: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3836/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3836: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3837/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3837: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3838/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3838: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3839/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3839: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3840/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3840: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3841/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3841: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3842/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3842: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3843/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3843: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3844/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9874\n",
      "Epoch 3844: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0166 - accuracy: 0.9874 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3845/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9884\n",
      "Epoch 3845: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0166 - accuracy: 0.9884 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3846/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9884\n",
      "Epoch 3846: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0166 - accuracy: 0.9884 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3847/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9884\n",
      "Epoch 3847: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0166 - accuracy: 0.9884 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3848/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9884\n",
      "Epoch 3848: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0166 - accuracy: 0.9884 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3849/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3849: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0318 - val_accuracy: 0.9752\n",
      "Epoch 3850/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3850: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3851/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3851: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3852/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3852: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3853/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3853: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3854/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3854: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3855/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3855: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3856/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3856: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3857/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3857: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3858/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3858: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3859/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3859: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3860/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3860: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3861/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3861: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3862/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3862: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3863/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3863: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3864/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3864: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3865/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3865: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3866/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9884\n",
      "Epoch 3866: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0165 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3867/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3867: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3868/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3868: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3869/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3869: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3870/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3870: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3871/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3871: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3872/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3872: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3873/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3873: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3874/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3874: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0317 - val_accuracy: 0.9752\n",
      "Epoch 3875/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3875: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3876/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3876: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3877/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3877: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3878/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3878: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3879/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3879: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3880/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3880: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3881/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3881: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3882/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3882: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3883/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3883: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3884/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3884: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3885/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9884\n",
      "Epoch 3885: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0164 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3886/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3886: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3887/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3887: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3888/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3888: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3889/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3889: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3890/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3890: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3891/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3891: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3892/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3892: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3893/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3893: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3894/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3894: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3895/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3895: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3896/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3896: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3897/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3897: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3898/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3898: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n",
      "Epoch 3899/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3899: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0316 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3900/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3900: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3901/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3901: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3902/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3902: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3903/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3903: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3904/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9884\n",
      "Epoch 3904: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0163 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3905/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3905: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3906/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3906: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3907/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3907: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3908/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3908: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3909/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3909: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3910/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3910: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3911/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3911: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3912/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3912: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3913/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3913: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3914/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3914: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3915/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3915: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3916/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3916: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3917/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3917: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3918/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3918: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3919/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3919: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3920/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3920: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3921/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3921: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3922/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3922: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3923/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9884\n",
      "Epoch 3923: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3924/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3924: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3925/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3925: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0315 - val_accuracy: 0.9752\n",
      "Epoch 3926/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3926: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3927/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3927: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3928/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3928: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3929/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3929: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3930/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3930: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3931/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3931: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3932/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3932: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3933/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3933: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3934/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3934: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3935/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3935: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3936/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3936: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3937/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3937: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3938/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3938: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3939/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3939: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3940/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3940: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3941/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3941: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3942/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9884\n",
      "Epoch 3942: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0161 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3943/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3943: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3944/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3944: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3945/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3945: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3946/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3946: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3947/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3947: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3948/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3948: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3949/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3949: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3950/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3950: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3951/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3951: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0314 - val_accuracy: 0.9752\n",
      "Epoch 3952/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3952: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3953/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3953: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3954/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3954: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3955/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3955: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3956/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3956: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3957/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3957: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3958/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3958: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3959/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3959: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3960/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3960: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3961/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9884\n",
      "Epoch 3961: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3962/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3962: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3963/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3963: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3964/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3964: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3965/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3965: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3966/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3966: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3967/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3967: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3968/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3968: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3969/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3969: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3970/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3970: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3971/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3971: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3972/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3972: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3973/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3973: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3974/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3974: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3975/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3975: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3976/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3976: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3977/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3977: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3978/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3978: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3979/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3979: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0313 - val_accuracy: 0.9752\n",
      "Epoch 3980/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3980: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3981/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9884\n",
      "Epoch 3981: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0159 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3982/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3982: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3983/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3983: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3984/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3984: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3985/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3985: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3986/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3986: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3987/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3987: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3988/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3988: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3989/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3989: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3990/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3990: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3991/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3991: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3992/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3992: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3993/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3993: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3994/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3994: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3995/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3995: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3996/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3996: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3997/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3997: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3998/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3998: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 3999/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 3999: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 4000/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9884\n",
      "Epoch 4000: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0158 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 4001/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4001: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 4002/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4002: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 4003/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4003: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 4004/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4004: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 4005/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4005: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 4006/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4006: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 4007/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4007: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0312 - val_accuracy: 0.9752\n",
      "Epoch 4008/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4008: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4009/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4009: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4010/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4010: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4011/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4011: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4012/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4012: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4013/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4013: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4014/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4014: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4015/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4015: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4016/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4016: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4017/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4017: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4018/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4018: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4019/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4019: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4020/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9884\n",
      "Epoch 4020: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0157 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4021/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4021: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4022/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4022: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4023/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4023: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4024/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4024: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4025/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4025: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4026/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4026: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4027/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4027: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4028/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4028: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4029/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4029: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4030/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4030: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4031/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4031: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4032/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4032: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4033/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4033: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4034/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4034: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4035/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4035: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4036/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4036: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0311 - val_accuracy: 0.9752\n",
      "Epoch 4037/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4037: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4038/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9884\n",
      "Epoch 4038: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4039/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9894\n",
      "Epoch 4039: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0156 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4040/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9894\n",
      "Epoch 4040: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0156 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4041/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4041: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4042/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4042: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4043/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4043: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4044/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4044: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4045/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4045: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4046/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4046: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4047/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4047: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4048/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4048: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4049/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4049: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4050/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4050: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4051/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4051: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4052/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4052: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4053/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4053: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4054/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4054: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4055/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4055: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4056/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4056: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4057/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4057: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4058/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4058: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4059/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4059: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4060/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4060: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4061/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9894\n",
      "Epoch 4061: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0155 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4062/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4062: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4063/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4063: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4064/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4064: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4065/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4065: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4066/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4066: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0310 - val_accuracy: 0.9752\n",
      "Epoch 4067/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4067: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4068/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4068: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4069/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4069: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4070/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4070: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4071/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4071: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4072/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4072: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4073/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4073: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4074/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4074: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4075/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4075: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4076/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4076: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4077/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4077: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4078/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4078: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4079/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4079: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4080/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4080: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4081/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9894\n",
      "Epoch 4081: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0154 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4082/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4082: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4083/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4083: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4084/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4084: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4085/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4085: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4086/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4086: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4087/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4087: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4088/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4088: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4089/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4089: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4090/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4090: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4091/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4091: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4092/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4092: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4093/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4093: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4094/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4094: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4095/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4095: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4096/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4096: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4097/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4097: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0309 - val_accuracy: 0.9752\n",
      "Epoch 4098/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4098: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4099/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4099: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4100/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4100: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4101/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4101: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4102/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9894\n",
      "Epoch 4102: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0153 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4103/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4103: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4104/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4104: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4105/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4105: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4106/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4106: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4107/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4107: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4108/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4108: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4109/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4109: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4110/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4110: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4111/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4111: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4112/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4112: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4113/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4113: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4114/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4114: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4115/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4115: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4116/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4116: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4117/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4117: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4118/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4118: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4119/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4119: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4120/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4120: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4121/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4121: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4122/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4122: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4123/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9894\n",
      "Epoch 4123: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0152 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4124/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4124: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4125/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4125: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4126/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4126: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4127/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4127: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4128/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4128: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4129/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4129: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9752\n",
      "Epoch 4130/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4130: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4131/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4131: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4132/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4132: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4133/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4133: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4134/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4134: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4135/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4135: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4136/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4136: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4137/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4137: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4138/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4138: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4139/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4139: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4140/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4140: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4141/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4141: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4142/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4142: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4143/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4143: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4144/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9894\n",
      "Epoch 4144: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4145/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4145: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4146/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4146: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4147/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4147: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4148/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4148: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4149/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4149: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4150/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4150: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4151/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4151: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4152/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4152: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4153/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4153: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4154/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4154: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4155/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9894\n",
      "Epoch 4155: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4156/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4156: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4157/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4157: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4158/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4158: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4159/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4159: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4160/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4160: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4161/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4161: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4162/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4162: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0307 - val_accuracy: 0.9752\n",
      "Epoch 4163/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4163: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4164/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4164: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4165/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9903\n",
      "Epoch 4165: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0150 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4166/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4166: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4167/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4167: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4168/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4168: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4169/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4169: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4170/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4170: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4171/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4171: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4172/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4172: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4173/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4173: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4174/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4174: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4175/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4175: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4176/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4176: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4177/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4177: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4178/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4178: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4179/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4179: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4180/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4180: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4181/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4181: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4182/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4182: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4183/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4183: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4184/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4184: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4185/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4185: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4186/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4186: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4187/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9903\n",
      "Epoch 4187: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0149 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4188/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4188: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4189/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4189: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4190/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4190: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4191/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4191: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4192/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4192: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4193/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4193: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4194/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4194: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4195/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4195: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4196/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4196: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4197/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4197: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0306 - val_accuracy: 0.9752\n",
      "Epoch 4198/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4198: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4199/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4199: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4200/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4200: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4201/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4201: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4202/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4202: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4203/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4203: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4204/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4204: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4205/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4205: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4206/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4206: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4207/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4207: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4208/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9903\n",
      "Epoch 4208: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0148 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4209/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4209: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4210/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4210: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4211/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4211: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4212/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4212: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4213/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4213: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4214/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4214: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4215/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4215: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4216/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4216: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4217/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4217: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4218/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4218: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4219/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4219: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4220/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4220: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4221/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4221: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4222/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4222: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4223/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4223: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4224/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4224: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4225/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4225: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4226/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4226: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4227/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4227: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4228/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4228: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4229/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4229: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4230/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9903\n",
      "Epoch 4230: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0147 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4231/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4231: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4232/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4232: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0305 - val_accuracy: 0.9752\n",
      "Epoch 4233/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4233: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4234/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4234: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4235/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4235: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4236/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4236: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4237/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4237: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4238/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4238: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4239/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4239: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4240/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4240: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4241/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4241: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4242/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4242: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4243/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4243: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4244/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4244: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4245/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4245: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4246/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4246: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4247/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4247: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4248/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4248: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4249/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4249: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4250/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4250: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4251/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4251: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4252/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9903\n",
      "Epoch 4252: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0146 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4253/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4253: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4254/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4254: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4255/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4255: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4256/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4256: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4257/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4257: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4258/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4258: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4259/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4259: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4260/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4260: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4261/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4261: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4262/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4262: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4263/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4263: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4264/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4264: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4265/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4265: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4266/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4266: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4267/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4267: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4268/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4268: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4269/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4269: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0304 - val_accuracy: 0.9752\n",
      "Epoch 4270/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4270: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4271/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4271: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4272/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4272: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4273/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4273: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4274/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4274: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4275/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9903\n",
      "Epoch 4275: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4276/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4276: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4277/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4277: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4278/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4278: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4279/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4279: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4280/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4280: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4281/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4281: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4282/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4282: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4283/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4283: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4284/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4284: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4285/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4285: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4286/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4286: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4287/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4287: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4288/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4288: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4289/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4289: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4290/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4290: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4291/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4291: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4292/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4292: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4293/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4293: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4294/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4294: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4295/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4295: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4296/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4296: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4297/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9903\n",
      "Epoch 4297: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0144 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4298/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4298: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4299/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4299: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4300/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4300: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4301/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4301: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4302/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4302: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4303/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4303: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4304/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4304: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4305/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4305: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4306/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4306: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4307/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4307: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4308/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4308: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0303 - val_accuracy: 0.9752\n",
      "Epoch 4309/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4309: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4310/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4310: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4311/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4311: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4312/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4312: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4313/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4313: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4314/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4314: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4315/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4315: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4316/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4316: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4317/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4317: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4318/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4318: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4319/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4319: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4320/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9903\n",
      "Epoch 4320: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0143 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4321/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4321: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4322/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4322: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4323/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4323: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4324/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4324: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4325/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4325: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4326/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4326: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4327/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4327: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4328/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4328: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4329/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4329: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4330/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4330: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4331/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4331: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4332/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4332: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4333/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4333: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4334/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4334: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4335/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4335: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4336/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4336: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4337/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4337: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4338/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4338: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4339/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4339: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4340/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4340: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4341/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4341: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4342/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4342: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4343/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9903\n",
      "Epoch 4343: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0142 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4344/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4344: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4345/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4345: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4346/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4346: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4347/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4347: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4348/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4348: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4349/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4349: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4350/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4350: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0302 - val_accuracy: 0.9752\n",
      "Epoch 4351/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4351: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4352/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4352: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4353/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4353: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4354/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4354: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4355/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4355: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4356/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4356: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4357/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4357: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4358/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4358: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4359/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4359: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4360/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4360: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4361/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4361: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4362/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4362: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4363/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4363: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4364/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4364: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4365/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4365: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4366/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4366: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4367/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9903\n",
      "Epoch 4367: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0141 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4368/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4368: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4369/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4369: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4370/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4370: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4371/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4371: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4372/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4372: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4373/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4373: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4374/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4374: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4375/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4375: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4376/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4376: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4377/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4377: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4378/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4378: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4379/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4379: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4380/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4380: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4381/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4381: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4382/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4382: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4383/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4383: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4384/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4384: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4385/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4385: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4386/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4386: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4387/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4387: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4388/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4388: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4389/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4389: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4390/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9903\n",
      "Epoch 4390: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0140 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4391/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4391: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4392/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4392: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4393/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4393: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9752\n",
      "Epoch 4394/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4394: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4395/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4395: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4396/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4396: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4397/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4397: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4398/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4398: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4399/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4399: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4400/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4400: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4401/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4401: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4402/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4402: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4403/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4403: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4404/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4404: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4405/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4405: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4406/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4406: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4407/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4407: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4408/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4408: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4409/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4409: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4410/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4410: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4411/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4411: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4412/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4412: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4413/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4413: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4414/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9903\n",
      "Epoch 4414: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0139 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4415/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4415: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4416/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4416: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4417/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4417: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4418/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4418: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4419/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4419: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4420/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4420: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4421/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4421: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4422/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4422: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4423/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4423: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4424/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4424: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4425/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4425: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4426/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4426: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4427/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4427: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4428/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4428: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4429/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4429: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4430/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4430: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4431/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4431: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4432/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4432: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4433/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4433: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4434/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4434: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4435/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4435: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4436/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4436: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4437/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4437: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4438/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9903\n",
      "Epoch 4438: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0138 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4439/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4439: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0300 - val_accuracy: 0.9752\n",
      "Epoch 4440/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4440: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4441/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4441: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4442/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4442: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4443/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4443: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4444/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4444: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4445/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4445: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4446/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4446: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4447/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4447: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4448/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4448: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4449/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4449: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4450/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4450: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4451/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4451: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4452/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4452: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4453/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4453: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4454/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4454: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4455/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4455: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4456/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4456: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4457/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4457: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4458/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4458: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4459/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4459: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4460/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4460: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4461/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4461: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4462/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9903\n",
      "Epoch 4462: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0137 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4463/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4463: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4464/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4464: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4465/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4465: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4466/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4466: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4467/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4467: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4468/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4468: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4469/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4469: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4470/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4470: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4471/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4471: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4472/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4472: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4473/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4473: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4474/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4474: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4475/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4475: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4476/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4476: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4477/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4477: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4478/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4478: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4479/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4479: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4480/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4480: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4481/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4481: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4482/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4482: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4483/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4483: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4484/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4484: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4485/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4485: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4486/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4486: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4487/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9903\n",
      "Epoch 4487: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0136 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4488/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4488: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0299 - val_accuracy: 0.9752\n",
      "Epoch 4489/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4489: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4490/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4490: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4491/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4491: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4492/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4492: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4493/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4493: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4494/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4494: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4495/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4495: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4496/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4496: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4497/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4497: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4498/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4498: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4499/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4499: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4500/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4500: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4501/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4501: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4502/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4502: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4503/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4503: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4504/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4504: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4505/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4505: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4506/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4506: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4507/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4507: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4508/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4508: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4509/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4509: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4510/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4510: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4511/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4511: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4512/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9903\n",
      "Epoch 4512: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0135 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4513/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4513: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4514/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4514: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4515/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4515: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4516/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4516: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4517/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4517: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4518/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4518: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4519/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4519: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4520/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4520: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4521/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4521: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4522/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4522: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4523/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4523: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4524/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4524: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4525/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4525: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4526/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4526: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4527/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4527: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4528/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4528: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4529/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4529: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4530/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4530: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4531/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4531: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4532/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4532: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4533/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4533: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4534/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4534: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4535/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4535: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4536/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4536: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4537/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9903\n",
      "Epoch 4537: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4538/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4538: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4539/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4539: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4540/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4540: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4541/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4541: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0298 - val_accuracy: 0.9752\n",
      "Epoch 4542/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4542: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4543/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4543: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4544/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4544: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4545/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4545: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4546/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4546: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4547/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4547: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4548/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4548: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4549/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4549: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4550/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4550: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4551/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4551: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4552/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4552: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4553/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4553: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4554/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4554: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4555/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4555: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4556/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4556: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4557/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4557: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4558/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4558: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4559/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4559: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4560/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4560: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4561/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4561: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4562/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9903\n",
      "Epoch 4562: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0133 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4563/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4563: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4564/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4564: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4565/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4565: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4566/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4566: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4567/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4567: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4568/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4568: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4569/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4569: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4570/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4570: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4571/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4571: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4572/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4572: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4573/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4573: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4574/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4574: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4575/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4575: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4576/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4576: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4577/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4577: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4578/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4578: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4579/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4579: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4580/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4580: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4581/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4581: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4582/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4582: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4583/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4583: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4584/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4584: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4585/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4585: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4586/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4586: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4587/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4587: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4588/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9903\n",
      "Epoch 4588: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0132 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4589/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4589: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4590/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4590: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4591/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4591: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4592/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4592: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4593/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4593: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4594/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4594: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4595/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4595: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4596/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4596: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4597/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4597: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4598/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4598: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0297 - val_accuracy: 0.9752\n",
      "Epoch 4599/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4599: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4600/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4600: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4601/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4601: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4602/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4602: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4603/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4603: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4604/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4604: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4605/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4605: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4606/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4606: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4607/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4607: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4608/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4608: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4609/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4609: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4610/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4610: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4611/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4611: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4612/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4612: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4613/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4613: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4614/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9903\n",
      "Epoch 4614: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0131 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4615/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4615: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4616/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4616: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4617/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4617: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4618/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4618: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4619/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4619: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4620/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4620: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4621/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4621: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4622/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4622: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4623/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4623: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4624/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4624: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4625/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4625: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4626/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4626: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4627/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4627: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4628/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4628: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4629/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4629: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4630/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4630: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4631/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4631: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4632/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4632: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4633/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4633: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4634/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4634: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4635/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4635: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4636/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4636: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4637/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4637: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4638/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4638: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4639/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4639: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4640/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9903\n",
      "Epoch 4640: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0130 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4641/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4641: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4642/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4642: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4643/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4643: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4644/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4644: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4645/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4645: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4646/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4646: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4647/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4647: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4648/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4648: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4649/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4649: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4650/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4650: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4651/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4651: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4652/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4652: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4653/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4653: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4654/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4654: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4655/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4655: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4656/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4656: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4657/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4657: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4658/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4658: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4659/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4659: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4660/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4660: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4661/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4661: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0296 - val_accuracy: 0.9752\n",
      "Epoch 4662/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4662: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4663/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4663: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4664/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4664: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4665/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4665: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4666/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9903\n",
      "Epoch 4666: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0129 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4667/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4667: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4668/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4668: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4669/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4669: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4670/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4670: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4671/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4671: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4672/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4672: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4673/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4673: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4674/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4674: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4675/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4675: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4676/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4676: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4677/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4677: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4678/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4678: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4679/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4679: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4680/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4680: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4681/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4681: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4682/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4682: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4683/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4683: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4684/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4684: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4685/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4685: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4686/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4686: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4687/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4687: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4688/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4688: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4689/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4689: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4690/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4690: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4691/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4691: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4692/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4692: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4693/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9903\n",
      "Epoch 4693: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0128 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4694/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9903\n",
      "Epoch 4694: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0127 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4695/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9903\n",
      "Epoch 4695: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0127 - accuracy: 0.9903 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4696/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4696: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4697/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4697: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4698/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4698: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4699/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4699: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4700/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4700: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4701/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4701: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4702/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4702: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4703/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4703: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4704/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4704: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4705/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4705: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4706/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4706: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4707/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4707: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4708/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4708: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4709/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4709: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4710/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4710: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4711/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4711: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4712/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4712: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4713/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4713: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4714/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4714: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4715/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4715: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4716/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4716: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4717/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4717: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4718/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4718: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4719/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4719: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4720/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9913\n",
      "Epoch 4720: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4721/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4721: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4722/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4722: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4723/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4723: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4724/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4724: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4725/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4725: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4726/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4726: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4727/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4727: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4728/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4728: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4729/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4729: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4730/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4730: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4731/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4731: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9752\n",
      "Epoch 4732/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4732: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4733/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4733: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4734/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4734: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4735/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4735: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4736/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4736: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4737/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4737: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4738/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4738: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4739/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4739: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4740/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4740: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4741/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4741: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4742/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4742: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4743/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4743: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4744/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4744: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4745/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4745: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4746/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4746: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4747/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9913\n",
      "Epoch 4747: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0126 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4748/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4748: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4749/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4749: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4750/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4750: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4751/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4751: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4752/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4752: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4753/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4753: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4754/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4754: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4755/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4755: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4756/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4756: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4757/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4757: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4758/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4758: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4759/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4759: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4760/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4760: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4761/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4761: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4762/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4762: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4763/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4763: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4764/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4764: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4765/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4765: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4766/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4766: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4767/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4767: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4768/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4768: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4769/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4769: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4770/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4770: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4771/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4771: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4772/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4772: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4773/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4773: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4774/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4774: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4775/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9913\n",
      "Epoch 4775: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0125 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4776/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4776: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4777/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4777: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4778/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4778: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4779/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4779: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4780/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4780: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4781/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4781: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4782/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4782: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4783/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4783: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4784/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4784: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4785/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4785: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4786/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4786: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4787/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4787: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4788/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4788: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4789/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4789: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4790/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4790: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4791/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4791: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4792/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4792: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4793/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4793: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4794/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4794: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4795/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4795: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4796/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4796: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4797/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4797: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4798/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4798: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4799/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4799: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4800/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4800: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4801/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4801: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4802/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4802: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4803/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9913\n",
      "Epoch 4803: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4804/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9913\n",
      "Epoch 4804: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0123 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4805/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9913\n",
      "Epoch 4805: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0123 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4806/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9913\n",
      "Epoch 4806: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0123 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4807/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9913\n",
      "Epoch 4807: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0123 - accuracy: 0.9913 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4808/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4808: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4809/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4809: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4810/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4810: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4811/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4811: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0294 - val_accuracy: 0.9752\n",
      "Epoch 4812/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4812: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4813/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4813: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4814/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4814: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4815/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4815: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4816/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4816: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4817/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4817: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4818/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4818: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4819/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4819: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4820/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4820: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4821/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4821: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4822/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4822: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4823/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4823: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4824/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4824: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4825/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4825: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4826/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4826: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4827/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4827: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4828/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4828: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4829/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4829: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4830/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4830: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4831/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9923\n",
      "Epoch 4831: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0123 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4832/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4832: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4833/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4833: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4834/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4834: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4835/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4835: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4836/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4836: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4837/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4837: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4838/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4838: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4839/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4839: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4840/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4840: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4841/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4841: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4842/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4842: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4843/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4843: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4844/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4844: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4845/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4845: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4846/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4846: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4847/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4847: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4848/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4848: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4849/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4849: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4850/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4850: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4851/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4851: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4852/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4852: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4853/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4853: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4854/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4854: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4855/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4855: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4856/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4856: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4857/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4857: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4858/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4858: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4859/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4859: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4860/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9923\n",
      "Epoch 4860: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0122 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4861/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4861: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4862/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4862: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4863/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4863: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4864/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4864: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4865/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4865: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4866/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4866: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4867/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4867: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4868/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4868: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4869/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4869: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4870/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4870: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4871/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4871: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4872/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4872: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4873/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4873: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4874/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4874: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4875/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4875: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4876/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4876: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4877/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4877: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4878/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4878: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4879/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4879: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4880/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4880: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4881/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4881: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4882/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4882: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4883/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4883: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4884/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4884: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4885/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4885: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4886/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4886: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4887/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4887: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4888/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4888: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4889/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9923\n",
      "Epoch 4889: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0121 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4890/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4890: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4891/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4891: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4892/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4892: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4893/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4893: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4894/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4894: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4895/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4895: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4896/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4896: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4897/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4897: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4898/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4898: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4899/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4899: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4900/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4900: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4901/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4901: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4902/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4902: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4903/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4903: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4904/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4904: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4905/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4905: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4906/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4906: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4907/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4907: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4908/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4908: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4909/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4909: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0293 - val_accuracy: 0.9752\n",
      "Epoch 4910/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4910: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4911/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4911: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4912/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4912: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4913/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4913: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4914/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4914: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4915/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4915: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4916/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4916: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4917/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4917: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4918/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9923\n",
      "Epoch 4918: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0120 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4919/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4919: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4920/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4920: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4921/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4921: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4922/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4922: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4923/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4923: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4924/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4924: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4925/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4925: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4926/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4926: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4927/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4927: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4928/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4928: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4929/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4929: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4930/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4930: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4931/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4931: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4932/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4932: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4933/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4933: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4934/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4934: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4935/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4935: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4936/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4936: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4937/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4937: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4938/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4938: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4939/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4939: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4940/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4940: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4941/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4941: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4942/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4942: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4943/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4943: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4944/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4944: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4945/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4945: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4946/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4946: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4947/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4947: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4948/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9923\n",
      "Epoch 4948: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0119 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4949/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4949: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4950/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4950: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4951/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4951: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4952/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4952: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4953/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4953: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4954/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4954: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4955/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4955: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4956/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4956: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4957/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4957: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4958/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4958: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4959/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4959: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4960/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4960: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4961/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4961: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4962/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4962: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4963/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4963: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4964/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4964: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4965/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4965: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4966/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4966: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4967/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4967: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4968/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4968: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4969/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4969: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4970/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4970: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4971/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4971: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4972/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4972: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4973/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4973: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4974/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4974: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4975/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4975: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4976/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4976: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4977/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4977: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4978/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9923\n",
      "Epoch 4978: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4979/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4979: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4980/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4980: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4981/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4981: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4982/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4982: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4983/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4983: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4984/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4984: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4985/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4985: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4986/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4986: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4987/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4987: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4988/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4988: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4989/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4989: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4990/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4990: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4991/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4991: val_accuracy did not improve from 0.97523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4992/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4992: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4993/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4993: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4994/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4994: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4995/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4995: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4996/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4996: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4997/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4997: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4998/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4998: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 4999/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 4999: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n",
      "Epoch 5000/5000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9923\n",
      "Epoch 5000: val_accuracy did not improve from 0.97523\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0117 - accuracy: 0.9923 - val_loss: 0.0292 - val_accuracy: 0.9752\n"
     ]
    }
   ],
   "source": [
    "mc = ModelCheckpoint('Best_Logistic_before_over_Sampling_Batch.h5', \n",
    "                     monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "model_NN=keras.Sequential([keras.Input(shape=(23)),\n",
    "                        layers.Dense(4,activation='softmax')])\n",
    "\n",
    "\n",
    "model_NN.compile(loss=SparseCategoricalFocalLoss(from_logits=False,gamma=2),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_NN.fit(X_train,Y_train,validation_data=[X_test,Y_test] \n",
    "          ,  batch_size=X_train.shape[0] , epochs=5000 , verbose =1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8f9d18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy=history.history[\"val_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0dab5a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs=np.arange(1,5001)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(epochs , accuracy , color='#ff0000' , label='training_acc')\n",
    "plt.plot(epochs , val_accuracy , color='#0000FF',label='val_acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('trainig accuracy versus val_acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuarcy ')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "397b05dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes=[0,1,2,3]\n",
    "reg_f1_score_training=[95,82,95,100]\n",
    "reg_f1_score_val     =[89,78,91,99]\n",
    "\n",
    "RF_f1_score_training =[100,100,100,100]\n",
    "RF_f1_score_val      =[93,84,92,98]\n",
    "\n",
    "\n",
    "SVM_f1_score_training =[95,81,94,99]\n",
    "SVM_f1_score_val      =[88,69,83,97]\n",
    "\n",
    "\n",
    "NN_f1_score_training =[98,94,98,100]\n",
    "NN_f1_score_val      =[92,88,95,99]\n",
    "\n",
    "x_indexes = np.arange(len(classes))\n",
    "width=0.125 # you can change it \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x_indexes-width,reg_f1_score_val,\n",
    "        color=\"black\",label=\"Logistic_F1_Score\",width=width)\n",
    "\n",
    "plt.bar(x_indexes,RF_f1_score_val\n",
    "        ,color=\"blue\",label=\"Random_Forest_F1_Score\",width=width)\n",
    "\n",
    "plt.bar(x_indexes+width,NN_f1_score_val,\n",
    "        color=\"red\",label=\"NN_Focal_Loss_F1_Score\",width=width)\n",
    "plt.bar(x_indexes+2*width,SVM_f1_score_val,\n",
    "        color=\"orange\",label=\"SVM_Focal_Loss_F1_Score\",width=width)\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(ticks=x_indexes,labels=classes)\n",
    "\n",
    "plt.title(\"Different Models vs different F1_score\")\n",
    "\n",
    "\n",
    "plt.use_sticky_edges = False\n",
    "\n",
    "   \n",
    "\n",
    "leg = plt.legend(bbox_to_anchor=(1,1))\n",
    "leg.get_frame().set_edgecolor('r')\n",
    "\n",
    "leg.get_frame().set_linewidth(1.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"F1_Score_For_Different_Models_before_applying_anything.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b685c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bars=[\"SVM\",\"REG\",\"RF\",\"NN\"]\n",
    "accuracies= [87,92,93,97.5]\n",
    "\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "plt.bar(y_pos,accuracies,width=.5)\n",
    "\n",
    "plt.xticks(y_pos,bars)\n",
    "plt.title(\"Accuracies of different models on the bare data\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e40c2",
   "metadata": {},
   "source": [
    "## applying the different oversampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "46e6e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df[df[\"Body_Level\"]==3]\n",
    "for i in range(4):\n",
    "    df_minority = df[df[\"Body_Level\"]==i]\n",
    "    df_minority_upsample = resample(df_minority , replace =True , n_samples=680,random_state=42)\n",
    "    df_ = pd.concat([df_,df_minority_upsample], axis=0,)\n",
    "\n",
    "oversampled=df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13572262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3400, 23)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=oversampled.drop([\"Body_Level\"],axis=1).to_numpy()\n",
    "Y_train=oversampled[\"Body_Level\"].to_numpy()\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e3ee84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "286df890",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, Y_train, Y_test = train_test_split(X_train_scaled\n",
    "                                                     , Y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2437627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LogisticRegression(random_state=42,max_iter=1000).fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5c6821a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred=reg_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "60bfcf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       483\n",
      "           1       0.96      0.91      0.93       482\n",
      "           2       0.96      0.96      0.96       483\n",
      "           3       1.00      1.00      1.00       932\n",
      "\n",
      "    accuracy                           0.97      2380\n",
      "   macro avg       0.97      0.97      0.97      2380\n",
      "weighted avg       0.97      0.97      0.97      2380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0293dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred=reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c65fd40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       197\n",
      "           1       0.92      0.87      0.89       198\n",
      "           2       0.94      0.92      0.93       197\n",
      "           3       1.00      1.00      1.00       428\n",
      "\n",
      "    accuracy                           0.96      1020\n",
      "   macro avg       0.95      0.95      0.94      1020\n",
      "weighted avg       0.96      0.96      0.96      1020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db060dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest_model = RandomForestClassifier(max_depth=40,random_state=42).fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "07a93174",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred=rand_forest_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "bf774c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       483\n",
      "           1       1.00      1.00      1.00       482\n",
      "           2       1.00      1.00      1.00       483\n",
      "           3       1.00      1.00      1.00       932\n",
      "\n",
      "    accuracy                           1.00      2380\n",
      "   macro avg       1.00      1.00      1.00      2380\n",
      "weighted avg       1.00      1.00      1.00      2380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "1d0effaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred=rand_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "a148b719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       197\n",
      "           1       0.95      0.99      0.97       198\n",
      "           2       0.98      0.97      0.97       197\n",
      "           3       1.00      0.99      0.99       428\n",
      "\n",
      "    accuracy                           0.98      1020\n",
      "   macro avg       0.98      0.98      0.98      1020\n",
      "weighted avg       0.98      0.98      0.98      1020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "30174272",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(gamma='auto').fit(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "63a64393",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred=svm_model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "46b76d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       483\n",
      "           1       0.96      0.94      0.95       482\n",
      "           2       0.95      0.97      0.96       483\n",
      "           3       1.00      1.00      1.00       932\n",
      "\n",
      "    accuracy                           0.98      2380\n",
      "   macro avg       0.97      0.97      0.97      2380\n",
      "weighted avg       0.98      0.98      0.98      2380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, Y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "b88038cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred=svm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "22413897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       197\n",
      "           1       0.90      0.92      0.91       198\n",
      "           2       0.94      0.96      0.95       197\n",
      "           3       0.99      0.98      0.98       428\n",
      "\n",
      "    accuracy                           0.96      1020\n",
      "   macro avg       0.95      0.95      0.95      1020\n",
      "weighted avg       0.96      0.96      0.96      1020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892c2a0",
   "metadata": {},
   "source": [
    "## Using SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "0d53fac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    680\n",
       "2    406\n",
       "1    201\n",
       "0    190\n",
       "Name: Body_Level, dtype: int64"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original[\"Body_Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "27cecca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "oversampled_SMOTE= df_original.copy()\n",
    "for i in range (4):\n",
    "    sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "    oversampled_X, oversampled_Y = sm.fit_resample(oversampled.drop('Body_Level', axis=1)\n",
    "                                                   , oversampled['Body_Level'])\n",
    "    oversampled_SMOTE = pd.concat([pd.DataFrame(oversampled_Y), pd.DataFrame(oversampled_X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "d560a456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body_Level</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>H_Cal_Consump</th>\n",
       "      <th>Veg_Consump</th>\n",
       "      <th>Water_Consump</th>\n",
       "      <th>Alcohol_Consump</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>...</th>\n",
       "      <th>Time_E_Dev</th>\n",
       "      <th>FBM__Always</th>\n",
       "      <th>FBM__Frequently</th>\n",
       "      <th>FBM__Sometimes</th>\n",
       "      <th>FBM__no</th>\n",
       "      <th>Automobile</th>\n",
       "      <th>Bike</th>\n",
       "      <th>Motorbike</th>\n",
       "      <th>Public_Transportation</th>\n",
       "      <th>Walking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.000307</td>\n",
       "      <td>1.607939</td>\n",
       "      <td>100.618239</td>\n",
       "      <td>1</td>\n",
       "      <td>2.877743</td>\n",
       "      <td>1.000463</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.431200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.649439</td>\n",
       "      <td>84.897738</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.039313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>23.246702</td>\n",
       "      <td>1.652971</td>\n",
       "      <td>93.310284</td>\n",
       "      <td>1</td>\n",
       "      <td>1.368978</td>\n",
       "      <td>1.953152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>30.551762</td>\n",
       "      <td>1.784377</td>\n",
       "      <td>102.872505</td>\n",
       "      <td>1</td>\n",
       "      <td>2.271306</td>\n",
       "      <td>1.771198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413752</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.443011</td>\n",
       "      <td>1.873368</td>\n",
       "      <td>121.829620</td>\n",
       "      <td>1</td>\n",
       "      <td>2.722161</td>\n",
       "      <td>2.375707</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4075</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.148230</td>\n",
       "      <td>1.660446</td>\n",
       "      <td>49.558304</td>\n",
       "      <td>1</td>\n",
       "      <td>2.004146</td>\n",
       "      <td>2.020764</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.267290</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.491055</td>\n",
       "      <td>1.586952</td>\n",
       "      <td>43.087508</td>\n",
       "      <td>0</td>\n",
       "      <td>2.008760</td>\n",
       "      <td>1.792022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4077</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.084625</td>\n",
       "      <td>1.787264</td>\n",
       "      <td>58.585146</td>\n",
       "      <td>1</td>\n",
       "      <td>2.530233</td>\n",
       "      <td>2.387945</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.672508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4078</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.015485</td>\n",
       "      <td>1.899247</td>\n",
       "      <td>59.975792</td>\n",
       "      <td>0</td>\n",
       "      <td>2.963578</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4080 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Body_Level  Gender        Age    Height      Weight  H_Cal_Consump  \\\n",
       "0              3       1  24.000307  1.607939  100.618239              1   \n",
       "1              3       0  18.000000  1.649439   84.897738              1   \n",
       "2              3       1  23.246702  1.652971   93.310284              1   \n",
       "3              3       1  30.551762  1.784377  102.872505              1   \n",
       "4              3       1  24.443011  1.873368  121.829620              1   \n",
       "...          ...     ...        ...       ...         ...            ...   \n",
       "4075           0       0  27.148230  1.660446   49.558304              1   \n",
       "4076           0       0  21.491055  1.586952   43.087508              0   \n",
       "4077           0       1  21.084625  1.787264   58.585146              1   \n",
       "4078           0       0  21.000000  1.520000   42.000000              0   \n",
       "4079           0       1  17.015485  1.899247   59.975792              0   \n",
       "\n",
       "      Veg_Consump  Water_Consump  Alcohol_Consump  Smoking  ...  Time_E_Dev  \\\n",
       "0        2.877743       1.000463                0        0  ...    1.431200   \n",
       "1        2.000000       1.039313                0        0  ...    0.000000   \n",
       "2        1.368978       1.953152                0        0  ...    0.221830   \n",
       "3        2.271306       1.771198                1        0  ...    0.413752   \n",
       "4        2.722161       2.375707                1        0  ...    0.000000   \n",
       "...           ...            ...              ...      ...  ...         ...   \n",
       "4075     2.004146       2.020764                0        0  ...    1.267290   \n",
       "4076     2.008760       1.792022                0        0  ...    0.000000   \n",
       "4077     2.530233       2.387945                0        0  ...    1.672508   \n",
       "4078     3.000000       1.000000                1        0  ...    0.000000   \n",
       "4079     2.963578       2.000000                0        0  ...    0.898586   \n",
       "\n",
       "      FBM__Always  FBM__Frequently  FBM__Sometimes  FBM__no  Automobile  Bike  \\\n",
       "0               0                0               1        0           0     0   \n",
       "1               0                0               1        0           0     0   \n",
       "2               0                0               1        0           1     0   \n",
       "3               0                0               1        0           1     0   \n",
       "4               0                0               1        0           0     0   \n",
       "...           ...              ...             ...      ...         ...   ...   \n",
       "4075            0                1               0        0           0     0   \n",
       "4076            0                1               0        0           0     0   \n",
       "4077            0                0               1        0           1     0   \n",
       "4078            0                1               0        0           0     0   \n",
       "4079            0                0               1        0           0     0   \n",
       "\n",
       "      Motorbike  Public_Transportation  Walking  \n",
       "0             0                      1        0  \n",
       "1             0                      1        0  \n",
       "2             0                      0        0  \n",
       "3             0                      0        0  \n",
       "4             0                      1        0  \n",
       "...         ...                    ...      ...  \n",
       "4075          0                      1        0  \n",
       "4076          0                      1        0  \n",
       "4077          0                      0        0  \n",
       "4078          0                      1        0  \n",
       "4079          0                      0        0  \n",
       "\n",
       "[4080 rows x 24 columns]"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "60cb971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=oversampled.drop([\"Body_Level\"],axis=1).to_numpy()\n",
    "Y_train=oversampled[\"Body_Level\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "c2359a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "6396916a",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, Y_train, Y_test = train_test_split(X_train_scaled\n",
    "                                                     , Y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "fce2f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model  = LogisticRegression(random_state=42,max_iter=1000).fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "f7649403",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred=reg_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "45b52590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       483\n",
      "           1       0.96      0.91      0.93       482\n",
      "           2       0.96      0.96      0.96       483\n",
      "           3       1.00      1.00      1.00       932\n",
      "\n",
      "    accuracy                           0.97      2380\n",
      "   macro avg       0.97      0.97      0.97      2380\n",
      "weighted avg       0.97      0.97      0.97      2380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, Y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "d6cbaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred=reg_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "37ea99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       197\n",
      "           1       0.92      0.87      0.89       198\n",
      "           2       0.94      0.92      0.93       197\n",
      "           3       1.00      1.00      1.00       428\n",
      "\n",
      "    accuracy                           0.96      1020\n",
      "   macro avg       0.95      0.95      0.94      1020\n",
      "weighted avg       0.96      0.96      0.96      1020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "195d4d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=40, random_state=42)"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_model = RandomForestClassifier(max_depth=40,random_state=42)\n",
    "rand_model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "ce6dfdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred=rand_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "f237a5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       483\n",
      "           1       1.00      1.00      1.00       482\n",
      "           2       1.00      1.00      1.00       483\n",
      "           3       1.00      1.00      1.00       932\n",
      "\n",
      "    accuracy                           1.00      2380\n",
      "   macro avg       1.00      1.00      1.00      2380\n",
      "weighted avg       1.00      1.00      1.00      2380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train, Y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "98c0a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred=rand_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "2bd62de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       197\n",
      "           1       0.95      0.99      0.97       198\n",
      "           2       0.98      0.97      0.97       197\n",
      "           3       1.00      0.99      0.99       428\n",
      "\n",
      "    accuracy                           0.98      1020\n",
      "   macro avg       0.98      0.98      0.98      1020\n",
      "weighted avg       0.98      0.98      0.98      1020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff32e31",
   "metadata": {},
   "source": [
    "## Trying deep learning models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801664a",
   "metadata": {},
   "source": [
    "## some techniques to consider \n",
    "- learning rate decay \n",
    "- drawing data with T_SNE\n",
    "- spliting the data well (Think in taking equal number of observation from each class )\n",
    "- how to tune hyperparameters and what hyperparmeters will be tuned\n",
    "- what is calibration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "e279a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6293/3506208505.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  oversampled[df[\"Body_Level\"]==0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>H_Cal_Consump</th>\n",
       "      <th>Veg_Consump</th>\n",
       "      <th>Water_Consump</th>\n",
       "      <th>Alcohol_Consump</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Meal_Count</th>\n",
       "      <th>...</th>\n",
       "      <th>Body_Level</th>\n",
       "      <th>FBM__Always</th>\n",
       "      <th>FBM__Frequently</th>\n",
       "      <th>FBM__Sometimes</th>\n",
       "      <th>FBM__no</th>\n",
       "      <th>Automobile</th>\n",
       "      <th>Bike</th>\n",
       "      <th>Motorbike</th>\n",
       "      <th>Public_Transportation</th>\n",
       "      <th>Walking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>21.997683</td>\n",
       "      <td>1.689441</td>\n",
       "      <td>51.107925</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.774576</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.715118</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>18.656912</td>\n",
       "      <td>1.574017</td>\n",
       "      <td>41.220175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.649974</td>\n",
       "      <td>1.549974</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.513835</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>19.329542</td>\n",
       "      <td>1.767335</td>\n",
       "      <td>55.700497</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.157395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>34.799519</td>\n",
       "      <td>1.689141</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.652779</td>\n",
       "      <td>1.601140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.804944</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.717601</td>\n",
       "      <td>51.073918</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.426874</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.547298</td>\n",
       "      <td>1.722461</td>\n",
       "      <td>51.881263</td>\n",
       "      <td>1</td>\n",
       "      <td>2.663421</td>\n",
       "      <td>1.041110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>21.813083</td>\n",
       "      <td>1.712515</td>\n",
       "      <td>51.710723</td>\n",
       "      <td>1</td>\n",
       "      <td>2.881300</td>\n",
       "      <td>1.248180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.087544</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.538398</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.718970</td>\n",
       "      <td>1.164644</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.473088</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>680 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender        Age    Height     Weight  H_Cal_Consump  Veg_Consump  \\\n",
       "102       0  21.997683  1.689441  51.107925              1     3.000000   \n",
       "179       0  22.000000  1.680000  52.000000              1     3.000000   \n",
       "92        0  18.656912  1.574017  41.220175              1     1.649974   \n",
       "14        1  17.000000  1.800000  58.000000              1     2.000000   \n",
       "106       1  19.329542  1.767335  55.700497              1     2.000000   \n",
       "..      ...        ...       ...        ...            ...          ...   \n",
       "104       0  34.799519  1.689141  50.000000              1     2.652779   \n",
       "166       0  23.000000  1.717601  51.073918              1     2.000000   \n",
       "0         0  22.547298  1.722461  51.881263              1     2.663421   \n",
       "130       0  21.813083  1.712515  51.710723              1     2.881300   \n",
       "91        0  19.000000  1.538398  42.000000              1     2.718970   \n",
       "\n",
       "     Water_Consump  Alcohol_Consump  Smoking  Meal_Count  ...  Body_Level  \\\n",
       "102       1.774576                0        0    3.715118  ...           0   \n",
       "179       2.000000                0        0    3.000000  ...           0   \n",
       "92        1.549974                1        0    1.513835  ...           0   \n",
       "14        2.000000                0        0    3.000000  ...           0   \n",
       "106       2.157395                0        0    4.000000  ...           0   \n",
       "..             ...              ...      ...         ...  ...         ...   \n",
       "104       1.601140                0        0    3.804944  ...           0   \n",
       "166       1.426874                0        0    3.000000  ...           0   \n",
       "0         1.041110                0        0    3.000000  ...           0   \n",
       "130       1.248180                0        0    3.087544  ...           0   \n",
       "91        1.164644                1        0    1.473088  ...           0   \n",
       "\n",
       "     FBM__Always  FBM__Frequently  FBM__Sometimes  FBM__no  Automobile  Bike  \\\n",
       "102            0                1               0        0           0     0   \n",
       "179            0                1               0        0           0     0   \n",
       "92             0                0               1        0           0     0   \n",
       "14             0                1               0        0           0     0   \n",
       "106            0                0               1        0           1     0   \n",
       "..           ...              ...             ...      ...         ...   ...   \n",
       "104            0                1               0        0           0     0   \n",
       "166            0                1               0        0           0     0   \n",
       "0              0                1               0        0           0     0   \n",
       "130            0                1               0        0           0     0   \n",
       "91             0                1               0        0           0     0   \n",
       "\n",
       "     Motorbike  Public_Transportation  Walking  \n",
       "102          0                      1        0  \n",
       "179          0                      1        0  \n",
       "92           0                      1        0  \n",
       "14           0                      0        1  \n",
       "106          0                      0        0  \n",
       "..         ...                    ...      ...  \n",
       "104          0                      1        0  \n",
       "166          0                      1        0  \n",
       "0            0                      1        0  \n",
       "130          0                      1        0  \n",
       "91           0                      1        0  \n",
       "\n",
       "[680 rows x 24 columns]"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184862f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee97181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa5b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66491023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "importances = mutual_info_classif(df.drop([\"Body_Level\"],axis=1).to_numpy()\n",
    "                                  ,df[\"Body_Level\"].to_numpy())\n",
    "feat_importances=pd.Series(importances,df.drop([\"Body_Level\"],axis=1).columns)\n",
    "feat_importances.plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a063278",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = df.drop([\"FBM__Frequently\",\"FBM__Sometimes\",\"FBM__no\",\"FBM__Always\",\n",
    "              \"Motorbike\",\"Public_Transportation\",\"Walking\",\n",
    "              \"Bike\",\"Body_Level\",\"Automobile\",\"Fam_Hist\"],axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "6c58876a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>H_Cal_Consump</th>\n",
       "      <th>Veg_Consump</th>\n",
       "      <th>Water_Consump</th>\n",
       "      <th>Alcohol_Consump</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Meal_Count</th>\n",
       "      <th>...</th>\n",
       "      <th>Body_Level</th>\n",
       "      <th>FBM__Always</th>\n",
       "      <th>FBM__Frequently</th>\n",
       "      <th>FBM__Sometimes</th>\n",
       "      <th>FBM__no</th>\n",
       "      <th>Automobile</th>\n",
       "      <th>Bike</th>\n",
       "      <th>Motorbike</th>\n",
       "      <th>Public_Transportation</th>\n",
       "      <th>Walking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.547298</td>\n",
       "      <td>1.722461</td>\n",
       "      <td>51.881263</td>\n",
       "      <td>1</td>\n",
       "      <td>2.663421</td>\n",
       "      <td>1.041110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19.799054</td>\n",
       "      <td>1.743702</td>\n",
       "      <td>54.927529</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.847264</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.289260</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>17.823438</td>\n",
       "      <td>1.708406</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.642241</td>\n",
       "      <td>1.099231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.452590</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>19.007177</td>\n",
       "      <td>1.690727</td>\n",
       "      <td>49.895716</td>\n",
       "      <td>1</td>\n",
       "      <td>1.212908</td>\n",
       "      <td>1.029703</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.207071</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>19.729250</td>\n",
       "      <td>1.793315</td>\n",
       "      <td>58.195150</td>\n",
       "      <td>1</td>\n",
       "      <td>2.508835</td>\n",
       "      <td>2.076933</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.435905</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>1</td>\n",
       "      <td>26.774115</td>\n",
       "      <td>1.755938</td>\n",
       "      <td>112.287678</td>\n",
       "      <td>1</td>\n",
       "      <td>1.428289</td>\n",
       "      <td>2.117733</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>0</td>\n",
       "      <td>20.908785</td>\n",
       "      <td>1.700996</td>\n",
       "      <td>126.490236</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.242832</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.629191</td>\n",
       "      <td>104.826776</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.654702</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>0</td>\n",
       "      <td>25.982261</td>\n",
       "      <td>1.629225</td>\n",
       "      <td>104.838425</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.556068</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>1</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.774644</td>\n",
       "      <td>105.966894</td>\n",
       "      <td>1</td>\n",
       "      <td>2.312825</td>\n",
       "      <td>2.073497</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1477 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Gender        Age    Height      Weight  H_Cal_Consump  Veg_Consump  \\\n",
       "0          0  22.547298  1.722461   51.881263              1     2.663421   \n",
       "1          1  19.799054  1.743702   54.927529              1     2.000000   \n",
       "2          0  17.823438  1.708406   50.000000              1     1.642241   \n",
       "3          0  19.007177  1.690727   49.895716              1     1.212908   \n",
       "4          1  19.729250  1.793315   58.195150              1     2.508835   \n",
       "...      ...        ...       ...         ...            ...          ...   \n",
       "1472       1  26.774115  1.755938  112.287678              1     1.428289   \n",
       "1473       0  20.908785  1.700996  126.490236              1     3.000000   \n",
       "1474       0  26.000000  1.629191  104.826776              1     3.000000   \n",
       "1475       0  25.982261  1.629225  104.838425              1     3.000000   \n",
       "1476       1  23.000000  1.774644  105.966894              1     2.312825   \n",
       "\n",
       "      Water_Consump  Alcohol_Consump  Smoking  Meal_Count  ...  Body_Level  \\\n",
       "0          1.041110                0        0    3.000000  ...           0   \n",
       "1          2.847264                1        0    3.289260  ...           0   \n",
       "2          1.099231                1        0    3.452590  ...           0   \n",
       "3          1.029703                1        0    3.207071  ...           0   \n",
       "4          2.076933                0        0    3.435905  ...           0   \n",
       "...             ...              ...      ...         ...  ...         ...   \n",
       "1472       2.117733                1        0    3.000000  ...           3   \n",
       "1473       1.242832                1        0    3.000000  ...           3   \n",
       "1474       2.654702                1        0    3.000000  ...           3   \n",
       "1475       2.556068                1        0    3.000000  ...           3   \n",
       "1476       2.073497                1        0    3.000000  ...           3   \n",
       "\n",
       "      FBM__Always  FBM__Frequently  FBM__Sometimes  FBM__no  Automobile  Bike  \\\n",
       "0               0                1               0        0           0     0   \n",
       "1               0                0               1        0           0     0   \n",
       "2               0                0               1        0           0     0   \n",
       "3               0                0               1        0           0     0   \n",
       "4               0                0               1        0           1     0   \n",
       "...           ...              ...             ...      ...         ...   ...   \n",
       "1472            0                0               1        0           1     0   \n",
       "1473            0                0               1        0           0     0   \n",
       "1474            0                0               1        0           0     0   \n",
       "1475            0                0               1        0           0     0   \n",
       "1476            0                0               1        0           0     0   \n",
       "\n",
       "      Motorbike  Public_Transportation  Walking  \n",
       "0             0                      1        0  \n",
       "1             0                      1        0  \n",
       "2             0                      1        0  \n",
       "3             0                      1        0  \n",
       "4             0                      0        0  \n",
       "...         ...                    ...      ...  \n",
       "1472          0                      0        0  \n",
       "1473          0                      1        0  \n",
       "1474          0                      1        0  \n",
       "1475          0                      1        0  \n",
       "1476          0                      1        0  \n",
       "\n",
       "[1477 rows x 24 columns]"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48797019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634a1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d697983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d659a627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00574c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456372c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb976371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065349e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3f232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3d851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509aaa0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3cc09e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7449bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=EarlyStopping(monitor='val_accuracy',baseline=0.97,mode='max' , patience=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fcb4fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9fb5d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN=keras.Sequential([keras.Input(shape=(23)),\n",
    "                           layers.Dense(16,activation='relu'),\n",
    "                           layers.Dropout(0.4),\n",
    "                        layers.Dense(4,activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "71bc4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN.compile(loss=SparseCategoricalFocalLoss(from_logits=False,gamma=2),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "757f39bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.8595 - accuracy: 0.4795\n",
      "Epoch 1: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 1s 12ms/step - loss: 0.8187 - accuracy: 0.4908 - val_loss: 0.3899 - val_accuracy: 0.6622\n",
      "Epoch 2/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.4429 - accuracy: 0.5755\n",
      "Epoch 2: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4139 - accuracy: 0.6021 - val_loss: 0.2817 - val_accuracy: 0.7793\n",
      "Epoch 3/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.3306 - accuracy: 0.6974\n",
      "Epoch 3: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.3290 - accuracy: 0.6883 - val_loss: 0.2261 - val_accuracy: 0.8018\n",
      "Epoch 4/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.2698 - accuracy: 0.7287\n",
      "Epoch 4: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2787 - accuracy: 0.7280 - val_loss: 0.1994 - val_accuracy: 0.8221\n",
      "Epoch 5/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.2113 - accuracy: 0.7922\n",
      "Epoch 5: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.2101 - accuracy: 0.7841 - val_loss: 0.1708 - val_accuracy: 0.8761\n",
      "Epoch 6/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.1858 - accuracy: 0.8100\n",
      "Epoch 6: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 0.1844 - accuracy: 0.8132 - val_loss: 0.1497 - val_accuracy: 0.8829\n",
      "Epoch 7/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.1719 - accuracy: 0.8150\n",
      "Epoch 7: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.1692 - accuracy: 0.8199 - val_loss: 0.1262 - val_accuracy: 0.8919\n",
      "Epoch 8/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.1519 - accuracy: 0.8315\n",
      "Epoch 8: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.1514 - accuracy: 0.8287 - val_loss: 0.1149 - val_accuracy: 0.8941\n",
      "Epoch 9/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.8535\n",
      "Epoch 9: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.1305 - accuracy: 0.8538 - val_loss: 0.1051 - val_accuracy: 0.9212\n",
      "Epoch 10/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.1349 - accuracy: 0.8553\n",
      "Epoch 10: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1294 - accuracy: 0.8587 - val_loss: 0.0966 - val_accuracy: 0.9099\n",
      "Epoch 11/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.1158 - accuracy: 0.8783\n",
      "Epoch 11: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1180 - accuracy: 0.8722 - val_loss: 0.0911 - val_accuracy: 0.9279\n",
      "Epoch 12/200\n",
      "18/33 [===============>..............] - ETA: 0s - loss: 0.1177 - accuracy: 0.8663\n",
      "Epoch 12: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1199 - accuracy: 0.8693 - val_loss: 0.0885 - val_accuracy: 0.9189\n",
      "Epoch 13/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.1062 - accuracy: 0.8635\n",
      "Epoch 13: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1140 - accuracy: 0.8645 - val_loss: 0.0863 - val_accuracy: 0.9369\n",
      "Epoch 14/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.1073 - accuracy: 0.8892\n",
      "Epoch 14: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1076 - accuracy: 0.8867 - val_loss: 0.0786 - val_accuracy: 0.9392\n",
      "Epoch 15/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0985 - accuracy: 0.8735\n",
      "Epoch 15: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.8693 - val_loss: 0.0720 - val_accuracy: 0.9482\n",
      "Epoch 16/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0916 - accuracy: 0.8763\n",
      "Epoch 16: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.8896 - val_loss: 0.0700 - val_accuracy: 0.9550\n",
      "Epoch 17/200\n",
      "18/33 [===============>..............] - ETA: 0s - loss: 0.0825 - accuracy: 0.8906\n",
      "Epoch 17: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0829 - accuracy: 0.8974 - val_loss: 0.0699 - val_accuracy: 0.9527\n",
      "Epoch 18/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0922 - accuracy: 0.8914\n",
      "Epoch 18: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.8925 - val_loss: 0.0616 - val_accuracy: 0.9459\n",
      "Epoch 19/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0826 - accuracy: 0.8988\n",
      "Epoch 19: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0819 - accuracy: 0.8945 - val_loss: 0.0662 - val_accuracy: 0.9437\n",
      "Epoch 20/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0861 - accuracy: 0.8891\n",
      "Epoch 20: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0823 - accuracy: 0.8984 - val_loss: 0.0696 - val_accuracy: 0.9279\n",
      "Epoch 21/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.0868 - accuracy: 0.8882\n",
      "Epoch 21: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9042 - val_loss: 0.0645 - val_accuracy: 0.9505\n",
      "Epoch 22/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0655 - accuracy: 0.9247\n",
      "Epoch 22: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9274 - val_loss: 0.0681 - val_accuracy: 0.9347\n",
      "Epoch 23/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0697 - accuracy: 0.9203\n",
      "Epoch 23: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9119 - val_loss: 0.0663 - val_accuracy: 0.9392\n",
      "Epoch 24/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0689 - accuracy: 0.9250\n",
      "Epoch 24: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0725 - accuracy: 0.9177 - val_loss: 0.0649 - val_accuracy: 0.9527\n",
      "Epoch 25/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0734 - accuracy: 0.9012\n",
      "Epoch 25: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.9022 - val_loss: 0.0756 - val_accuracy: 0.9189\n",
      "Epoch 26/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0750 - accuracy: 0.9020\n",
      "Epoch 26: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.8993 - val_loss: 0.0665 - val_accuracy: 0.9527\n",
      "Epoch 27/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0618 - accuracy: 0.9233\n",
      "Epoch 27: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.9197 - val_loss: 0.0598 - val_accuracy: 0.9505\n",
      "Epoch 28/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0655 - accuracy: 0.9117\n",
      "Epoch 28: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0677 - accuracy: 0.9109 - val_loss: 0.0698 - val_accuracy: 0.9324\n",
      "Epoch 29/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0638 - accuracy: 0.9152\n",
      "Epoch 29: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.9177 - val_loss: 0.0678 - val_accuracy: 0.9505\n",
      "Epoch 30/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0661 - accuracy: 0.9208\n",
      "Epoch 30: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.9129 - val_loss: 0.0756 - val_accuracy: 0.9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0666 - accuracy: 0.9187\n",
      "Epoch 31: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9197 - val_loss: 0.0753 - val_accuracy: 0.9414\n",
      "Epoch 32/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0601 - accuracy: 0.9303\n",
      "Epoch 32: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9274 - val_loss: 0.0756 - val_accuracy: 0.9369\n",
      "Epoch 33/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0643 - accuracy: 0.9206\n",
      "Epoch 33: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9187 - val_loss: 0.0657 - val_accuracy: 0.9482\n",
      "Epoch 34/200\n",
      "17/33 [==============>...............] - ETA: 0s - loss: 0.0750 - accuracy: 0.8989\n",
      "Epoch 34: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.9061 - val_loss: 0.0732 - val_accuracy: 0.9437\n",
      "Epoch 35/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0570 - accuracy: 0.9276\n",
      "Epoch 35: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9235 - val_loss: 0.0694 - val_accuracy: 0.9505\n",
      "Epoch 36/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0591 - accuracy: 0.9271\n",
      "Epoch 36: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.9245 - val_loss: 0.0688 - val_accuracy: 0.9459\n",
      "Epoch 37/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0553 - accuracy: 0.9429\n",
      "Epoch 37: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0580 - accuracy: 0.9351 - val_loss: 0.0591 - val_accuracy: 0.9595\n",
      "Epoch 38/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0648 - accuracy: 0.9212\n",
      "Epoch 38: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9284 - val_loss: 0.0509 - val_accuracy: 0.9550\n",
      "Epoch 39/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0639 - accuracy: 0.9137\n",
      "Epoch 39: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.9177 - val_loss: 0.0501 - val_accuracy: 0.9550\n",
      "Epoch 40/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0636 - accuracy: 0.9212\n",
      "Epoch 40: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9313 - val_loss: 0.0550 - val_accuracy: 0.9550\n",
      "Epoch 41/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0563 - accuracy: 0.9307\n",
      "Epoch 41: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9274 - val_loss: 0.0604 - val_accuracy: 0.9572\n",
      "Epoch 42/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0512 - accuracy: 0.9403\n",
      "Epoch 42: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0517 - accuracy: 0.9351 - val_loss: 0.0652 - val_accuracy: 0.9437\n",
      "Epoch 43/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0629 - accuracy: 0.9250\n",
      "Epoch 43: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0618 - accuracy: 0.9274 - val_loss: 0.0595 - val_accuracy: 0.9482\n",
      "Epoch 44/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.0578 - accuracy: 0.9227\n",
      "Epoch 44: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0667 - accuracy: 0.9109 - val_loss: 0.0560 - val_accuracy: 0.9572\n",
      "Epoch 45/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0854 - accuracy: 0.8919\n",
      "Epoch 45: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.8993 - val_loss: 0.0780 - val_accuracy: 0.9212\n",
      "Epoch 46/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9150\n",
      "Epoch 46: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 0.0766 - accuracy: 0.9138 - val_loss: 0.0562 - val_accuracy: 0.9459\n",
      "Epoch 47/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0566 - accuracy: 0.9271\n",
      "Epoch 47: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 0.9313 - val_loss: 0.0614 - val_accuracy: 0.9414\n",
      "Epoch 48/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.0663 - accuracy: 0.9243\n",
      "Epoch 48: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0649 - accuracy: 0.9216 - val_loss: 0.0586 - val_accuracy: 0.9527\n",
      "Epoch 49/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0471 - accuracy: 0.9336\n",
      "Epoch 49: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.9332 - val_loss: 0.0608 - val_accuracy: 0.9392\n",
      "Epoch 50/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.0537 - accuracy: 0.9326\n",
      "Epoch 50: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9284 - val_loss: 0.0536 - val_accuracy: 0.9505\n",
      "Epoch 51/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0616 - accuracy: 0.9234\n",
      "Epoch 51: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 0.9245 - val_loss: 0.0534 - val_accuracy: 0.9617\n",
      "Epoch 52/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0609 - accuracy: 0.9241\n",
      "Epoch 52: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9264 - val_loss: 0.0801 - val_accuracy: 0.9527\n",
      "Epoch 53/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0635 - accuracy: 0.9339\n",
      "Epoch 53: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0619 - accuracy: 0.9342 - val_loss: 0.0585 - val_accuracy: 0.9550\n",
      "Epoch 54/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0504 - accuracy: 0.9402\n",
      "Epoch 54: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0514 - accuracy: 0.9342 - val_loss: 0.0661 - val_accuracy: 0.9505\n",
      "Epoch 55/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0625 - accuracy: 0.9323\n",
      "Epoch 55: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9322 - val_loss: 0.0595 - val_accuracy: 0.9505\n",
      "Epoch 56/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0580 - accuracy: 0.9232\n",
      "Epoch 56: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.9284 - val_loss: 0.0580 - val_accuracy: 0.9595\n",
      "Epoch 57/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0597 - accuracy: 0.9307\n",
      "Epoch 57: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.9284 - val_loss: 0.0639 - val_accuracy: 0.9572\n",
      "Epoch 58/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0502 - accuracy: 0.9312\n",
      "Epoch 58: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0515 - accuracy: 0.9322 - val_loss: 0.0650 - val_accuracy: 0.9505\n",
      "Epoch 59/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0510 - accuracy: 0.9494\n",
      "Epoch 59: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0532 - accuracy: 0.9390 - val_loss: 0.0662 - val_accuracy: 0.9482\n",
      "Epoch 60/200\n",
      "18/33 [===============>..............] - ETA: 0s - loss: 0.0581 - accuracy: 0.9306\n",
      "Epoch 60: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0497 - accuracy: 0.9400 - val_loss: 0.0578 - val_accuracy: 0.9572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0483 - accuracy: 0.9390\n",
      "Epoch 61: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0511 - accuracy: 0.9342 - val_loss: 0.0586 - val_accuracy: 0.9527\n",
      "Epoch 62/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0518 - accuracy: 0.9336\n",
      "Epoch 62: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 0.9351 - val_loss: 0.0651 - val_accuracy: 0.9459\n",
      "Epoch 63/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0538 - accuracy: 0.9321\n",
      "Epoch 63: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.9322 - val_loss: 0.0657 - val_accuracy: 0.9437\n",
      "Epoch 64/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0522 - accuracy: 0.9271\n",
      "Epoch 64: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 0.9255 - val_loss: 0.0711 - val_accuracy: 0.9505\n",
      "Epoch 65/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0614 - accuracy: 0.9261\n",
      "Epoch 65: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.9226 - val_loss: 0.0621 - val_accuracy: 0.9527\n",
      "Epoch 66/200\n",
      "17/33 [==============>...............] - ETA: 0s - loss: 0.0540 - accuracy: 0.9393\n",
      "Epoch 66: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0536 - accuracy: 0.9390 - val_loss: 0.0639 - val_accuracy: 0.9527\n",
      "Epoch 67/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0546 - accuracy: 0.9266\n",
      "Epoch 67: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0519 - accuracy: 0.9313 - val_loss: 0.0564 - val_accuracy: 0.9527\n",
      "Epoch 68/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0485 - accuracy: 0.9388\n",
      "Epoch 68: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9400 - val_loss: 0.0578 - val_accuracy: 0.9550\n",
      "Epoch 69/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0563 - accuracy: 0.9351\n",
      "Epoch 69: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.9313 - val_loss: 0.0574 - val_accuracy: 0.9640\n",
      "Epoch 70/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0568 - accuracy: 0.9206\n",
      "Epoch 70: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0501 - accuracy: 0.9313 - val_loss: 0.0503 - val_accuracy: 0.9527\n",
      "Epoch 71/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0591 - accuracy: 0.9198\n",
      "Epoch 71: val_accuracy did not improve from 0.96847\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.9158 - val_loss: 0.0563 - val_accuracy: 0.9595\n",
      "Epoch 72/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0588 - accuracy: 0.9375\n",
      "Epoch 72: val_accuracy improved from 0.96847 to 0.97072, saving model to best_model.h5\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0562 - accuracy: 0.9371 - val_loss: 0.0531 - val_accuracy: 0.9707\n",
      "Epoch 73/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0548 - accuracy: 0.9317\n",
      "Epoch 73: val_accuracy improved from 0.97072 to 0.97297, saving model to best_model.h5\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0544 - accuracy: 0.9342 - val_loss: 0.0592 - val_accuracy: 0.9730\n",
      "Epoch 74/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0529 - accuracy: 0.9435\n",
      "Epoch 74: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0475 - accuracy: 0.9487 - val_loss: 0.0565 - val_accuracy: 0.9505\n",
      "Epoch 75/200\n",
      "18/33 [===============>..............] - ETA: 0s - loss: 0.0521 - accuracy: 0.9444\n",
      "Epoch 75: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9293 - val_loss: 0.0595 - val_accuracy: 0.9595\n",
      "Epoch 76/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0457 - accuracy: 0.9411\n",
      "Epoch 76: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0428 - accuracy: 0.9439 - val_loss: 0.0623 - val_accuracy: 0.9595\n",
      "Epoch 77/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0491 - accuracy: 0.9323\n",
      "Epoch 77: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9361 - val_loss: 0.0530 - val_accuracy: 0.9595\n",
      "Epoch 78/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0452 - accuracy: 0.9418\n",
      "Epoch 78: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.9487 - val_loss: 0.0555 - val_accuracy: 0.9572\n",
      "Epoch 79/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0506 - accuracy: 0.9457\n",
      "Epoch 79: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.9497 - val_loss: 0.0622 - val_accuracy: 0.9459\n",
      "Epoch 80/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0405 - accuracy: 0.9497\n",
      "Epoch 80: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9468 - val_loss: 0.0619 - val_accuracy: 0.9527\n",
      "Epoch 81/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0479 - accuracy: 0.9361\n",
      "Epoch 81: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.9390 - val_loss: 0.0684 - val_accuracy: 0.9505\n",
      "Epoch 82/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0424 - accuracy: 0.9418\n",
      "Epoch 82: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9390 - val_loss: 0.0671 - val_accuracy: 0.9640\n",
      "Epoch 83/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0460 - accuracy: 0.9432\n",
      "Epoch 83: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.9468 - val_loss: 0.0653 - val_accuracy: 0.9617\n",
      "Epoch 84/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0491 - accuracy: 0.9438\n",
      "Epoch 84: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0516 - accuracy: 0.9419 - val_loss: 0.0737 - val_accuracy: 0.9505\n",
      "Epoch 85/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0518 - accuracy: 0.9361\n",
      "Epoch 85: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9371 - val_loss: 0.0583 - val_accuracy: 0.9550\n",
      "Epoch 86/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0434 - accuracy: 0.9496\n",
      "Epoch 86: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.9497 - val_loss: 0.0532 - val_accuracy: 0.9527\n",
      "Epoch 87/200\n",
      "18/33 [===============>..............] - ETA: 0s - loss: 0.0482 - accuracy: 0.9497\n",
      "Epoch 87: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0485 - accuracy: 0.9390 - val_loss: 0.0665 - val_accuracy: 0.9572\n",
      "Epoch 88/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0503 - accuracy: 0.9443\n",
      "Epoch 88: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9439 - val_loss: 0.0738 - val_accuracy: 0.9595\n",
      "Epoch 89/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0397 - accuracy: 0.9497\n",
      "Epoch 89: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.9487 - val_loss: 0.0745 - val_accuracy: 0.9572\n",
      "Epoch 90/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0463 - accuracy: 0.9267\n",
      "Epoch 90: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9274 - val_loss: 0.0763 - val_accuracy: 0.9527\n",
      "Epoch 91/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0527 - accuracy: 0.9275\n",
      "Epoch 91: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0562 - accuracy: 0.9284 - val_loss: 0.0809 - val_accuracy: 0.9257\n",
      "Epoch 92/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0547 - accuracy: 0.9226\n",
      "Epoch 92: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9216 - val_loss: 0.0697 - val_accuracy: 0.9347\n",
      "Epoch 93/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0576 - accuracy: 0.9387\n",
      "Epoch 93: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0556 - accuracy: 0.9419 - val_loss: 0.0976 - val_accuracy: 0.9234\n",
      "Epoch 94/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0634 - accuracy: 0.9112\n",
      "Epoch 94: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0669 - accuracy: 0.9177 - val_loss: 0.0725 - val_accuracy: 0.9482\n",
      "Epoch 95/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0507 - accuracy: 0.9375\n",
      "Epoch 95: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0522 - accuracy: 0.9313 - val_loss: 0.0725 - val_accuracy: 0.9595\n",
      "Epoch 96/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0491 - accuracy: 0.9435\n",
      "Epoch 96: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0537 - accuracy: 0.9400 - val_loss: 0.0768 - val_accuracy: 0.9550\n",
      "Epoch 97/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0567 - accuracy: 0.9399\n",
      "Epoch 97: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0539 - accuracy: 0.9439 - val_loss: 0.0679 - val_accuracy: 0.9595\n",
      "Epoch 98/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0579 - accuracy: 0.9206\n",
      "Epoch 98: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.9177 - val_loss: 0.0749 - val_accuracy: 0.9414\n",
      "Epoch 99/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0541 - accuracy: 0.9255\n",
      "Epoch 99: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9255 - val_loss: 0.0857 - val_accuracy: 0.9482\n",
      "Epoch 100/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0523 - accuracy: 0.9511\n",
      "Epoch 100: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0476 - accuracy: 0.9535 - val_loss: 0.0935 - val_accuracy: 0.9324\n",
      "Epoch 101/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0555 - accuracy: 0.9350\n",
      "Epoch 101: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9332 - val_loss: 0.0830 - val_accuracy: 0.9572\n",
      "Epoch 102/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0449 - accuracy: 0.9386\n",
      "Epoch 102: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9361 - val_loss: 0.0854 - val_accuracy: 0.9459\n",
      "Epoch 103/200\n",
      "31/33 [===========================>..] - ETA: 0s - loss: 0.0425 - accuracy: 0.9435\n",
      "Epoch 103: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0424 - accuracy: 0.9429 - val_loss: 0.0797 - val_accuracy: 0.9505\n",
      "Epoch 104/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0487 - accuracy: 0.9474\n",
      "Epoch 104: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9409 - val_loss: 0.0794 - val_accuracy: 0.9505\n",
      "Epoch 105/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0428 - accuracy: 0.9505\n",
      "Epoch 105: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0443 - accuracy: 0.9487 - val_loss: 0.0831 - val_accuracy: 0.9414\n",
      "Epoch 106/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0456 - accuracy: 0.9375\n",
      "Epoch 106: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0433 - accuracy: 0.9429 - val_loss: 0.0745 - val_accuracy: 0.9437\n",
      "Epoch 107/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0450 - accuracy: 0.9488\n",
      "Epoch 107: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0418 - accuracy: 0.9516 - val_loss: 0.0736 - val_accuracy: 0.9459\n",
      "Epoch 108/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0391 - accuracy: 0.9550\n",
      "Epoch 108: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0396 - accuracy: 0.9535 - val_loss: 0.0769 - val_accuracy: 0.9482\n",
      "Epoch 109/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0371 - accuracy: 0.9635\n",
      "Epoch 109: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0481 - accuracy: 0.9477 - val_loss: 0.0703 - val_accuracy: 0.9595\n",
      "Epoch 110/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0479 - accuracy: 0.9362\n",
      "Epoch 110: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0486 - accuracy: 0.9342 - val_loss: 0.0748 - val_accuracy: 0.9482\n",
      "Epoch 111/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0500 - accuracy: 0.9388\n",
      "Epoch 111: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0542 - accuracy: 0.9342 - val_loss: 0.0694 - val_accuracy: 0.9482\n",
      "Epoch 112/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0461 - accuracy: 0.9375\n",
      "Epoch 112: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9332 - val_loss: 0.0764 - val_accuracy: 0.9459\n",
      "Epoch 113/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0457 - accuracy: 0.9363\n",
      "Epoch 113: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0448 - accuracy: 0.9361 - val_loss: 0.0752 - val_accuracy: 0.9414\n",
      "Epoch 114/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0503 - accuracy: 0.9411\n",
      "Epoch 114: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0505 - accuracy: 0.9390 - val_loss: 0.0840 - val_accuracy: 0.9527\n",
      "Epoch 115/200\n",
      "16/33 [=============>................] - ETA: 0s - loss: 0.0528 - accuracy: 0.9297\n",
      "Epoch 115: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9390 - val_loss: 0.0715 - val_accuracy: 0.9482\n",
      "Epoch 116/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0382 - accuracy: 0.9538\n",
      "Epoch 116: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9526 - val_loss: 0.0720 - val_accuracy: 0.9505\n",
      "Epoch 117/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0373 - accuracy: 0.9539\n",
      "Epoch 117: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 0.9477 - val_loss: 0.0718 - val_accuracy: 0.9482\n",
      "Epoch 118/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0519 - accuracy: 0.9464\n",
      "Epoch 118: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0473 - accuracy: 0.9497 - val_loss: 0.0812 - val_accuracy: 0.9527\n",
      "Epoch 119/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0494 - accuracy: 0.9403\n",
      "Epoch 119: val_accuracy did not improve from 0.97297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0492 - accuracy: 0.9342 - val_loss: 0.0728 - val_accuracy: 0.9572\n",
      "Epoch 120/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0428 - accuracy: 0.9494\n",
      "Epoch 120: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0479 - accuracy: 0.9448 - val_loss: 0.0744 - val_accuracy: 0.9550\n",
      "Epoch 121/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0456 - accuracy: 0.9339\n",
      "Epoch 121: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9351 - val_loss: 0.0778 - val_accuracy: 0.9527\n",
      "Epoch 122/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0397 - accuracy: 0.9470\n",
      "Epoch 122: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9526 - val_loss: 0.0794 - val_accuracy: 0.9572\n",
      "Epoch 123/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0501 - accuracy: 0.9475\n",
      "Epoch 123: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9506 - val_loss: 0.0790 - val_accuracy: 0.9482\n",
      "Epoch 124/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0535 - accuracy: 0.9359\n",
      "Epoch 124: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.9458 - val_loss: 0.0819 - val_accuracy: 0.9527\n",
      "Epoch 125/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0496 - accuracy: 0.9492\n",
      "Epoch 125: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0451 - accuracy: 0.9506 - val_loss: 0.0821 - val_accuracy: 0.9527\n",
      "Epoch 126/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0468 - accuracy: 0.9389\n",
      "Epoch 126: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.9390 - val_loss: 0.0747 - val_accuracy: 0.9527\n",
      "Epoch 127/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0359 - accuracy: 0.9545\n",
      "Epoch 127: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.9468 - val_loss: 0.0681 - val_accuracy: 0.9595\n",
      "Epoch 128/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0443 - accuracy: 0.9563\n",
      "Epoch 128: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0412 - accuracy: 0.9555 - val_loss: 0.0782 - val_accuracy: 0.9527\n",
      "Epoch 129/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0559 - accuracy: 0.9362\n",
      "Epoch 129: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9332 - val_loss: 0.0750 - val_accuracy: 0.9527\n",
      "Epoch 130/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0410 - accuracy: 0.9468\n",
      "Epoch 130: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.9409 - val_loss: 0.0755 - val_accuracy: 0.9505\n",
      "Epoch 131/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0459 - accuracy: 0.9447\n",
      "Epoch 131: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.9477 - val_loss: 0.0749 - val_accuracy: 0.9505\n",
      "Epoch 132/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0380 - accuracy: 0.9583\n",
      "Epoch 132: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9555 - val_loss: 0.0759 - val_accuracy: 0.9369\n",
      "Epoch 133/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0313 - accuracy: 0.9563\n",
      "Epoch 133: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0454 - accuracy: 0.9458 - val_loss: 0.0787 - val_accuracy: 0.9369\n",
      "Epoch 134/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0359 - accuracy: 0.9588\n",
      "Epoch 134: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0427 - accuracy: 0.9477 - val_loss: 0.0919 - val_accuracy: 0.9437\n",
      "Epoch 135/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0455 - accuracy: 0.9488\n",
      "Epoch 135: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0410 - accuracy: 0.9545 - val_loss: 0.1019 - val_accuracy: 0.9347\n",
      "Epoch 136/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0397 - accuracy: 0.9443\n",
      "Epoch 136: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0416 - accuracy: 0.9419 - val_loss: 0.0847 - val_accuracy: 0.9482\n",
      "Epoch 137/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0404 - accuracy: 0.9588\n",
      "Epoch 137: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9584 - val_loss: 0.0925 - val_accuracy: 0.9527\n",
      "Epoch 138/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0330 - accuracy: 0.9616\n",
      "Epoch 138: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9574 - val_loss: 0.0869 - val_accuracy: 0.9482\n",
      "Epoch 139/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0393 - accuracy: 0.9470\n",
      "Epoch 139: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0405 - accuracy: 0.9468 - val_loss: 0.0919 - val_accuracy: 0.9527\n",
      "Epoch 140/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0400 - accuracy: 0.9488\n",
      "Epoch 140: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 0.9477 - val_loss: 0.1046 - val_accuracy: 0.9505\n",
      "Epoch 141/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0490 - accuracy: 0.9475\n",
      "Epoch 141: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0471 - accuracy: 0.9506 - val_loss: 0.0838 - val_accuracy: 0.9527\n",
      "Epoch 142/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0524 - accuracy: 0.9336\n",
      "Epoch 142: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0481 - accuracy: 0.9390 - val_loss: 0.0840 - val_accuracy: 0.9595\n",
      "Epoch 143/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0459 - accuracy: 0.9457\n",
      "Epoch 143: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9419 - val_loss: 0.0864 - val_accuracy: 0.9595\n",
      "Epoch 144/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0313 - accuracy: 0.9583\n",
      "Epoch 144: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.9516 - val_loss: 0.0870 - val_accuracy: 0.9617\n",
      "Epoch 145/200\n",
      "29/33 [=========================>....] - ETA: 0s - loss: 0.0551 - accuracy: 0.9343\n",
      "Epoch 145: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9351 - val_loss: 0.1104 - val_accuracy: 0.9279\n",
      "Epoch 146/200\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9371\n",
      "Epoch 146: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9371 - val_loss: 0.0919 - val_accuracy: 0.9505\n",
      "Epoch 147/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0474 - accuracy: 0.9421\n",
      "Epoch 147: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9390 - val_loss: 0.0939 - val_accuracy: 0.9459\n",
      "Epoch 148/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0633 - accuracy: 0.9225\n",
      "Epoch 148: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 0.9303 - val_loss: 0.1008 - val_accuracy: 0.9414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0566 - accuracy: 0.9307\n",
      "Epoch 149: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 0.9274 - val_loss: 0.0938 - val_accuracy: 0.9527\n",
      "Epoch 150/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0449 - accuracy: 0.9488\n",
      "Epoch 150: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.9477 - val_loss: 0.0917 - val_accuracy: 0.9505\n",
      "Epoch 151/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0598 - accuracy: 0.9275\n",
      "Epoch 151: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9322 - val_loss: 0.0842 - val_accuracy: 0.9459\n",
      "Epoch 152/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0367 - accuracy: 0.9474\n",
      "Epoch 152: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0407 - accuracy: 0.9419 - val_loss: 0.0918 - val_accuracy: 0.9414\n",
      "Epoch 153/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0360 - accuracy: 0.9500\n",
      "Epoch 153: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9448 - val_loss: 0.0920 - val_accuracy: 0.9482\n",
      "Epoch 154/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0363 - accuracy: 0.9570\n",
      "Epoch 154: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0368 - accuracy: 0.9555 - val_loss: 0.1042 - val_accuracy: 0.9392\n",
      "Epoch 155/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0481 - accuracy: 0.9344\n",
      "Epoch 155: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0462 - accuracy: 0.9342 - val_loss: 0.0960 - val_accuracy: 0.9550\n",
      "Epoch 156/200\n",
      "17/33 [==============>...............] - ETA: 0s - loss: 0.0491 - accuracy: 0.9485\n",
      "Epoch 156: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.9458 - val_loss: 0.1010 - val_accuracy: 0.9482\n",
      "Epoch 157/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0447 - accuracy: 0.9328\n",
      "Epoch 157: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0408 - accuracy: 0.9448 - val_loss: 0.0863 - val_accuracy: 0.9550\n",
      "Epoch 158/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0468 - accuracy: 0.9345\n",
      "Epoch 158: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0413 - accuracy: 0.9439 - val_loss: 0.0795 - val_accuracy: 0.9527\n",
      "Epoch 159/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0419 - accuracy: 0.9479\n",
      "Epoch 159: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0445 - accuracy: 0.9439 - val_loss: 0.0884 - val_accuracy: 0.9482\n",
      "Epoch 160/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0381 - accuracy: 0.9497\n",
      "Epoch 160: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.9555 - val_loss: 0.0880 - val_accuracy: 0.9437\n",
      "Epoch 161/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0353 - accuracy: 0.9598\n",
      "Epoch 161: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 0.9526 - val_loss: 0.0882 - val_accuracy: 0.9414\n",
      "Epoch 162/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.0427 - accuracy: 0.9507\n",
      "Epoch 162: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0413 - accuracy: 0.9458 - val_loss: 0.0918 - val_accuracy: 0.9482\n",
      "Epoch 163/200\n",
      "18/33 [===============>..............] - ETA: 0s - loss: 0.0395 - accuracy: 0.9531\n",
      "Epoch 163: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0391 - accuracy: 0.9555 - val_loss: 0.0820 - val_accuracy: 0.9482\n",
      "Epoch 164/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0525 - accuracy: 0.9362\n",
      "Epoch 164: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9361 - val_loss: 0.0791 - val_accuracy: 0.9437\n",
      "Epoch 165/200\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9453\n",
      "Epoch 165: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9458 - val_loss: 0.0774 - val_accuracy: 0.9595\n",
      "Epoch 166/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0353 - accuracy: 0.9688\n",
      "Epoch 166: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0466 - accuracy: 0.9351 - val_loss: 0.0838 - val_accuracy: 0.9505\n",
      "Epoch 167/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0119 - accuracy: 0.9688\n",
      "Epoch 167: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0440 - accuracy: 0.9439 - val_loss: 0.0826 - val_accuracy: 0.9414\n",
      "Epoch 168/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0369 - accuracy: 0.9375\n",
      "Epoch 168: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.9448 - val_loss: 0.0793 - val_accuracy: 0.9527\n",
      "Epoch 169/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0643 - accuracy: 0.9062\n",
      "Epoch 169: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 0.9419 - val_loss: 0.0822 - val_accuracy: 0.9459\n",
      "Epoch 170/200\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9468\n",
      "Epoch 170: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9468 - val_loss: 0.0764 - val_accuracy: 0.9437\n",
      "Epoch 171/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0694 - accuracy: 0.9375\n",
      "Epoch 171: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9526 - val_loss: 0.0791 - val_accuracy: 0.9527\n",
      "Epoch 172/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0489 - accuracy: 0.9375\n",
      "Epoch 172: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0489 - accuracy: 0.9351 - val_loss: 0.0839 - val_accuracy: 0.9527\n",
      "Epoch 173/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0505 - accuracy: 0.9447\n",
      "Epoch 173: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0486 - accuracy: 0.9458 - val_loss: 0.0758 - val_accuracy: 0.9459\n",
      "Epoch 174/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0496 - accuracy: 0.9400\n",
      "Epoch 174: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0497 - accuracy: 0.9419 - val_loss: 0.0777 - val_accuracy: 0.9505\n",
      "Epoch 175/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0519 - accuracy: 0.9340\n",
      "Epoch 175: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0517 - accuracy: 0.9371 - val_loss: 0.0950 - val_accuracy: 0.9279\n",
      "Epoch 176/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0546 - accuracy: 0.9315\n",
      "Epoch 176: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9380 - val_loss: 0.0983 - val_accuracy: 0.9572\n",
      "Epoch 177/200\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9332\n",
      "Epoch 177: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9332 - val_loss: 0.0805 - val_accuracy: 0.9459\n",
      "Epoch 178/200\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 178: val_accuracy did not improve from 0.97297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9332 - val_loss: 0.0689 - val_accuracy: 0.9617\n",
      "Epoch 179/200\n",
      "19/33 [================>.............] - ETA: 0s - loss: 0.0478 - accuracy: 0.9474\n",
      "Epoch 179: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0542 - accuracy: 0.9439 - val_loss: 0.0714 - val_accuracy: 0.9595\n",
      "Epoch 180/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0387 - accuracy: 0.9509\n",
      "Epoch 180: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9516 - val_loss: 0.0668 - val_accuracy: 0.9572\n",
      "Epoch 181/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0433 - accuracy: 0.9514\n",
      "Epoch 181: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0403 - accuracy: 0.9545 - val_loss: 0.0753 - val_accuracy: 0.9527\n",
      "Epoch 182/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0486 - accuracy: 0.9470\n",
      "Epoch 182: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0443 - accuracy: 0.9506 - val_loss: 0.0648 - val_accuracy: 0.9550\n",
      "Epoch 183/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0436 - accuracy: 0.9307\n",
      "Epoch 183: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0489 - accuracy: 0.9303 - val_loss: 0.0787 - val_accuracy: 0.9505\n",
      "Epoch 184/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0440 - accuracy: 0.9531\n",
      "Epoch 184: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0457 - accuracy: 0.9477 - val_loss: 0.0744 - val_accuracy: 0.9572\n",
      "Epoch 185/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0409 - accuracy: 0.9453\n",
      "Epoch 185: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9477 - val_loss: 0.0696 - val_accuracy: 0.9572\n",
      "Epoch 186/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0442 - accuracy: 0.9464\n",
      "Epoch 186: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.9458 - val_loss: 0.0768 - val_accuracy: 0.9550\n",
      "Epoch 187/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0653 - accuracy: 0.9391\n",
      "Epoch 187: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0579 - accuracy: 0.9400 - val_loss: 0.0828 - val_accuracy: 0.9437\n",
      "Epoch 188/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0464 - accuracy: 0.9416\n",
      "Epoch 188: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0448 - accuracy: 0.9390 - val_loss: 0.0779 - val_accuracy: 0.9437\n",
      "Epoch 189/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0518 - accuracy: 0.9403\n",
      "Epoch 189: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 0.9419 - val_loss: 0.0746 - val_accuracy: 0.9505\n",
      "Epoch 190/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0487 - accuracy: 0.9438\n",
      "Epoch 190: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0460 - accuracy: 0.9439 - val_loss: 0.0815 - val_accuracy: 0.9459\n",
      "Epoch 191/200\n",
      "27/33 [=======================>......] - ETA: 0s - loss: 0.0441 - accuracy: 0.9433\n",
      "Epoch 191: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 0.9400 - val_loss: 0.0817 - val_accuracy: 0.9572\n",
      "Epoch 192/200\n",
      "28/33 [========================>.....] - ETA: 0s - loss: 0.0639 - accuracy: 0.9275\n",
      "Epoch 192: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.9255 - val_loss: 0.0768 - val_accuracy: 0.9482\n",
      "Epoch 193/200\n",
      "23/33 [===================>..........] - ETA: 0s - loss: 0.0459 - accuracy: 0.9389\n",
      "Epoch 193: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0495 - accuracy: 0.9361 - val_loss: 0.0740 - val_accuracy: 0.9505\n",
      "Epoch 194/200\n",
      "20/33 [=================>............] - ETA: 0s - loss: 0.0519 - accuracy: 0.9344\n",
      "Epoch 194: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0485 - accuracy: 0.9380 - val_loss: 0.0674 - val_accuracy: 0.9617\n",
      "Epoch 195/200\n",
      "25/33 [=====================>........] - ETA: 0s - loss: 0.0391 - accuracy: 0.9463\n",
      "Epoch 195: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0406 - accuracy: 0.9477 - val_loss: 0.0691 - val_accuracy: 0.9527\n",
      "Epoch 196/200\n",
      "24/33 [====================>.........] - ETA: 0s - loss: 0.0474 - accuracy: 0.9531\n",
      "Epoch 196: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9458 - val_loss: 0.0769 - val_accuracy: 0.9617\n",
      "Epoch 197/200\n",
      "22/33 [===================>..........] - ETA: 0s - loss: 0.0546 - accuracy: 0.9190\n",
      "Epoch 197: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.0531 - accuracy: 0.9264 - val_loss: 0.0725 - val_accuracy: 0.9550\n",
      "Epoch 198/200\n",
      "21/33 [==================>...........] - ETA: 0s - loss: 0.0508 - accuracy: 0.9315\n",
      "Epoch 198: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9342 - val_loss: 0.0652 - val_accuracy: 0.9617\n",
      "Epoch 199/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0467 - accuracy: 0.9495\n",
      "Epoch 199: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0470 - accuracy: 0.9477 - val_loss: 0.0653 - val_accuracy: 0.9640\n",
      "Epoch 200/200\n",
      "26/33 [======================>.......] - ETA: 0s - loss: 0.0402 - accuracy: 0.9459\n",
      "Epoch 200: val_accuracy did not improve from 0.97297\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0380 - accuracy: 0.9497 - val_loss: 0.0687 - val_accuracy: 0.9640\n"
     ]
    }
   ],
   "source": [
    "history =model_NN.fit(X_train,Y_train,validation_data=[X_test,Y_test] ,batch_size=32, \n",
    "             epochs=200 , verbose =1,callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f2bff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "74649cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred= np.argmax(model.predict(X_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d4e31e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        49\n",
      "           1       0.96      0.89      0.92        72\n",
      "           2       0.95      0.98      0.97       120\n",
      "           3       1.00      1.00      1.00       203\n",
      "\n",
      "    accuracy                           0.97       444\n",
      "   macro avg       0.97      0.96      0.96       444\n",
      "weighted avg       0.97      0.97      0.97       444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef12de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c19628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b28f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef59c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460bf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8da12a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    680\n",
       "2    406\n",
       "1    201\n",
       "0    190\n",
       "Name: Body_Level, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Body_Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e4a5a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_=df[df[\"Body_Level\"]!=1]\n",
    "df_minority = df[df[\"Body_Level\"]==1]\n",
    "df_minority_upsample = resample(df_minority , replace =True , n_samples=340,random_state=42)\n",
    "df_ = pd.concat([df_,df_minority_upsample], axis=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "474f43b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    680\n",
       "2    406\n",
       "1    340\n",
       "0    190\n",
       "Name: Body_Level, dtype: int64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_[\"Body_Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "d1d58c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority = df[df[\"Body_Level\"]==0]\n",
    "df_minority_upsample = resample(df_minority , replace =True , n_samples=100,random_state=42)\n",
    "df_ = pd.concat([df_,df_minority_upsample], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0073a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority = df[df[\"Body_Level\"]==2]\n",
    "df_minority_upsample = resample(df_minority , replace =True , n_samples=100,random_state=42)\n",
    "df_ = pd.concat([df_,df_minority_upsample], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2c0267b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    680\n",
       "2    506\n",
       "1    340\n",
       "0    290\n",
       "Name: Body_Level, dtype: int64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_[\"Body_Level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "862dd499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1816, 23)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=df_.drop([\"Body_Level\"],axis=1).to_numpy()\n",
    "Y_train=df_[\"Body_Level\"].to_numpy()\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b2919d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1816, 23)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train_scaled\n",
    "                                                     , Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "850f1b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 23)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7117560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN=keras.Sequential([keras.Input(shape=(23)),\n",
    "                           layers.Dense(128,activation='relu'),\n",
    "                           layers.Dropout(0.4),\n",
    "                        layers.Dense(4,activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "241ee59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN.compile(loss=SparseCategoricalFocalLoss(from_logits=False,gamma=3),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a9639477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 0.0160 - accuracy: 0.9931   \n",
      "Epoch 1: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 1s 9ms/step - loss: 0.0142 - accuracy: 0.9917 - val_loss: 0.0298 - val_accuracy: 0.9973\n",
      "Epoch 2/200\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 0.0527 - accuracy: 0.9780  \n",
      "Epoch 2: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0523 - accuracy: 0.9793 - val_loss: 0.0489 - val_accuracy: 0.9863\n",
      "Epoch 3/200\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 0.0258 - accuracy: 0.9878\n",
      "Epoch 3: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.9897 - val_loss: 0.0669 - val_accuracy: 0.9835\n",
      "Epoch 4/200\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0336 - accuracy: 0.9903  \n",
      "Epoch 4: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0368 - accuracy: 0.9904 - val_loss: 0.0322 - val_accuracy: 0.9945\n",
      "Epoch 5/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0342 - accuracy: 0.9896    \n",
      "Epoch 5: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.9890 - val_loss: 0.0230 - val_accuracy: 0.9945\n",
      "Epoch 6/200\n",
      "28/46 [=================>............] - ETA: 0s - loss: 0.0314 - accuracy: 0.9888    \n",
      "Epoch 6: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.9904 - val_loss: 0.0400 - val_accuracy: 0.9890\n",
      "Epoch 7/200\n",
      "29/46 [=================>............] - ETA: 0s - loss: 0.0261 - accuracy: 0.9903    \n",
      "Epoch 7: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0348 - accuracy: 0.9883 - val_loss: 0.0573 - val_accuracy: 0.9890\n",
      "Epoch 8/200\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0266 - accuracy: 0.9893  \n",
      "Epoch 8: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.9890 - val_loss: 0.1074 - val_accuracy: 0.9808\n",
      "Epoch 9/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0348 - accuracy: 0.9861\n",
      "Epoch 9: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9862 - val_loss: 0.0432 - val_accuracy: 0.9945\n",
      "Epoch 10/200\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 0.0376 - accuracy: 0.9887  \n",
      "Epoch 10: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9855 - val_loss: 0.0466 - val_accuracy: 0.9890\n",
      "Epoch 11/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0437 - accuracy: 0.9879\n",
      "Epoch 11: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0424 - accuracy: 0.9883 - val_loss: 0.0484 - val_accuracy: 0.9890\n",
      "Epoch 12/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0167 - accuracy: 0.9915  \n",
      "Epoch 12: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9910 - val_loss: 0.0189 - val_accuracy: 0.9890\n",
      "Epoch 13/200\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 0.0285 - accuracy: 0.9888  \n",
      "Epoch 13: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.0257 - accuracy: 0.9890 - val_loss: 0.0569 - val_accuracy: 0.9890\n",
      "Epoch 14/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0439 - accuracy: 0.9872  \n",
      "Epoch 14: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0426 - accuracy: 0.9869 - val_loss: 0.1060 - val_accuracy: 0.9835\n",
      "Epoch 15/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9910  \n",
      "Epoch 15: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9910 - val_loss: 0.1044 - val_accuracy: 0.9835\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9883  \n",
      "Epoch 16: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.9883 - val_loss: 0.1136 - val_accuracy: 0.9835\n",
      "Epoch 17/200\n",
      "28/46 [=================>............] - ETA: 0s - loss: 0.0191 - accuracy: 0.9888\n",
      "Epoch 17: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.9883 - val_loss: 0.0773 - val_accuracy: 0.9835\n",
      "Epoch 18/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0252 - accuracy: 0.9870\n",
      "Epoch 18: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.9890 - val_loss: 0.1399 - val_accuracy: 0.9835\n",
      "Epoch 19/200\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0321 - accuracy: 0.9862\n",
      "Epoch 19: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.9869 - val_loss: 0.0765 - val_accuracy: 0.9780\n",
      "Epoch 20/200\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 0.0399 - accuracy: 0.9844  \n",
      "Epoch 20: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9862 - val_loss: 0.1000 - val_accuracy: 0.9835\n",
      "Epoch 21/200\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.0474 - accuracy: 0.9803  \n",
      "Epoch 21: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9800 - val_loss: 0.0555 - val_accuracy: 0.9890\n",
      "Epoch 22/200\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 0.0295 - accuracy: 0.9848  \n",
      "Epoch 22: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0275 - accuracy: 0.9848 - val_loss: 0.0651 - val_accuracy: 0.9863\n",
      "Epoch 23/200\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 0.0401 - accuracy: 0.9816  \n",
      "Epoch 23: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9835 - val_loss: 0.0442 - val_accuracy: 0.9808\n",
      "Epoch 24/200\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0352 - accuracy: 0.9886  \n",
      "Epoch 24: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.9883 - val_loss: 0.0838 - val_accuracy: 0.9835\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9876  \n",
      "Epoch 25: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.9876 - val_loss: 0.0577 - val_accuracy: 0.9835\n",
      "Epoch 26/200\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0290 - accuracy: 0.9874  \n",
      "Epoch 26: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.9883 - val_loss: 0.0518 - val_accuracy: 0.9835\n",
      "Epoch 27/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0263 - accuracy: 0.9864    \n",
      "Epoch 27: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.9862 - val_loss: 0.0442 - val_accuracy: 0.9863\n",
      "Epoch 28/200\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 0.0368 - accuracy: 0.9889    \n",
      "Epoch 28: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9883 - val_loss: 0.0485 - val_accuracy: 0.9945\n",
      "Epoch 29/200\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 0.0239 - accuracy: 0.9893\n",
      "Epoch 29: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 0.9890 - val_loss: 0.0503 - val_accuracy: 0.9918\n",
      "Epoch 30/200\n",
      "29/46 [=================>............] - ETA: 0s - loss: 0.0241 - accuracy: 0.9892\n",
      "Epoch 30: val_accuracy did not improve from 0.99725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.9910 - val_loss: 0.0220 - val_accuracy: 0.9945\n",
      "Epoch 31/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0123 - accuracy: 0.9925    \n",
      "Epoch 31: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.9890 - val_loss: 0.0674 - val_accuracy: 0.9863\n",
      "Epoch 32/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0304 - accuracy: 0.9870    \n",
      "Epoch 32: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.9904 - val_loss: 0.0513 - val_accuracy: 0.9808\n",
      "Epoch 33/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9882  \n",
      "Epoch 33: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.9883 - val_loss: 0.0801 - val_accuracy: 0.9890\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9910  \n",
      "Epoch 34: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.9910 - val_loss: 0.0737 - val_accuracy: 0.9808\n",
      "Epoch 35/200\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0202 - accuracy: 0.9927  \n",
      "Epoch 35: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.9924 - val_loss: 0.0820 - val_accuracy: 0.9863\n",
      "Epoch 36/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0338 - accuracy: 0.9808  \n",
      "Epoch 36: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.9814 - val_loss: 0.0753 - val_accuracy: 0.9890\n",
      "Epoch 37/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0431 - accuracy: 0.9800    \n",
      "Epoch 37: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.9835 - val_loss: 0.0632 - val_accuracy: 0.9918\n",
      "Epoch 38/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0110 - accuracy: 0.9940\n",
      "Epoch 38: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0083 - accuracy: 0.9938 - val_loss: 0.1035 - val_accuracy: 0.9890\n",
      "Epoch 39/200\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0305 - accuracy: 0.9884\n",
      "Epoch 39: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.9890 - val_loss: 0.0620 - val_accuracy: 0.9890\n",
      "Epoch 40/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9854\n",
      "Epoch 40: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9855 - val_loss: 0.0591 - val_accuracy: 0.9890\n",
      "Epoch 41/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0369 - accuracy: 0.9900    \n",
      "Epoch 41: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.9910 - val_loss: 0.0592 - val_accuracy: 0.9835\n",
      "Epoch 42/200\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 0.0603 - accuracy: 0.9899\n",
      "Epoch 42: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0445 - accuracy: 0.9897 - val_loss: 0.0877 - val_accuracy: 0.9835\n",
      "Epoch 43/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0329 - accuracy: 0.9900    \n",
      "Epoch 43: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9924 - val_loss: 0.0964 - val_accuracy: 0.9835\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9855\n",
      "Epoch 44: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.9855 - val_loss: 0.1031 - val_accuracy: 0.9890\n",
      "Epoch 45/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0441 - accuracy: 0.9858\n",
      "Epoch 45: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0428 - accuracy: 0.9862 - val_loss: 0.0612 - val_accuracy: 0.9890\n",
      "Epoch 46/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0249 - accuracy: 0.9901\n",
      "Epoch 46: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.9890 - val_loss: 0.0692 - val_accuracy: 0.9835\n",
      "Epoch 47/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0375 - accuracy: 0.9870    \n",
      "Epoch 47: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0383 - accuracy: 0.9862 - val_loss: 0.0679 - val_accuracy: 0.9835\n",
      "Epoch 48/200\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.0327 - accuracy: 0.9885  \n",
      "Epoch 48: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0274 - accuracy: 0.9904 - val_loss: 0.0598 - val_accuracy: 0.9863\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9897  \n",
      "Epoch 49: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.9897 - val_loss: 0.0901 - val_accuracy: 0.9863\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9862\n",
      "Epoch 50: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9862 - val_loss: 0.0971 - val_accuracy: 0.9863\n",
      "Epoch 51/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0172 - accuracy: 0.9905    \n",
      "Epoch 51: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9897 - val_loss: 0.1216 - val_accuracy: 0.9863\n",
      "Epoch 52/200\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 0.0619 - accuracy: 0.9844  \n",
      "Epoch 52: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0681 - accuracy: 0.9835 - val_loss: 0.1201 - val_accuracy: 0.9780\n",
      "Epoch 53/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0286 - accuracy: 0.9875    \n",
      "Epoch 53: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9842 - val_loss: 0.0995 - val_accuracy: 0.9780\n",
      "Epoch 54/200\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 0.0397 - accuracy: 0.9844    \n",
      "Epoch 54: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9862 - val_loss: 0.0886 - val_accuracy: 0.9835\n",
      "Epoch 55/200\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 0.0456 - accuracy: 0.9896\n",
      "Epoch 55: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0493 - accuracy: 0.9897 - val_loss: 0.0625 - val_accuracy: 0.9863\n",
      "Epoch 56/200\n",
      "27/46 [================>.............] - ETA: 0s - loss: 0.0281 - accuracy: 0.9907\n",
      "Epoch 56: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9890 - val_loss: 0.0436 - val_accuracy: 0.9863\n",
      "Epoch 57/200\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 0.0485 - accuracy: 0.9867  \n",
      "Epoch 57: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0535 - accuracy: 0.9835 - val_loss: 0.0949 - val_accuracy: 0.9835\n",
      "Epoch 58/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9729  \n",
      "Epoch 58: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.1062 - accuracy: 0.9731 - val_loss: 0.1141 - val_accuracy: 0.9863\n",
      "Epoch 59/200\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 0.0335 - accuracy: 0.9884\n",
      "Epoch 59: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0428 - accuracy: 0.9842 - val_loss: 0.0932 - val_accuracy: 0.9863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0428 - accuracy: 0.9818    \n",
      "Epoch 60: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9807 - val_loss: 0.1842 - val_accuracy: 0.9835\n",
      "Epoch 61/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0597 - accuracy: 0.9870    \n",
      "Epoch 61: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.9883 - val_loss: 0.0949 - val_accuracy: 0.9808\n",
      "Epoch 62/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0423 - accuracy: 0.9864\n",
      "Epoch 62: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0876 - accuracy: 0.9848 - val_loss: 0.1509 - val_accuracy: 0.9780\n",
      "Epoch 63/200\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0262 - accuracy: 0.9840  \n",
      "Epoch 63: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.9842 - val_loss: 0.1173 - val_accuracy: 0.9835\n",
      "Epoch 64/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0389 - accuracy: 0.9912    \n",
      "Epoch 64: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9876 - val_loss: 0.1553 - val_accuracy: 0.9808\n",
      "Epoch 65/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0514 - accuracy: 0.9837\n",
      "Epoch 65: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9842 - val_loss: 0.1588 - val_accuracy: 0.9753\n",
      "Epoch 66/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0401 - accuracy: 0.9892    \n",
      "Epoch 66: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0424 - accuracy: 0.9883 - val_loss: 0.1676 - val_accuracy: 0.9808\n",
      "Epoch 67/200\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 0.0407 - accuracy: 0.9886    \n",
      "Epoch 67: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0370 - accuracy: 0.9904 - val_loss: 0.1268 - val_accuracy: 0.9835\n",
      "Epoch 68/200\n",
      "28/46 [=================>............] - ETA: 0s - loss: 0.0477 - accuracy: 0.9888    \n",
      "Epoch 68: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9869 - val_loss: 0.1401 - val_accuracy: 0.9808\n",
      "Epoch 69/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0636 - accuracy: 0.9870    \n",
      "Epoch 69: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9821 - val_loss: 0.1277 - val_accuracy: 0.9863\n",
      "Epoch 70/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0333 - accuracy: 0.9875\n",
      "Epoch 70: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.1445 - val_accuracy: 0.9835\n",
      "Epoch 71/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0349 - accuracy: 0.9925    \n",
      "Epoch 71: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0352 - accuracy: 0.9917 - val_loss: 0.1170 - val_accuracy: 0.9835\n",
      "Epoch 72/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0417 - accuracy: 0.9917  \n",
      "Epoch 72: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.9917 - val_loss: 0.1469 - val_accuracy: 0.9835\n",
      "Epoch 73/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0429 - accuracy: 0.9868\n",
      "Epoch 73: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 0.9869 - val_loss: 0.1058 - val_accuracy: 0.9835\n",
      "Epoch 74/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0477 - accuracy: 0.9844    \n",
      "Epoch 74: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9869 - val_loss: 0.0627 - val_accuracy: 0.9835\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9890  \n",
      "Epoch 75: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.9890 - val_loss: 0.0698 - val_accuracy: 0.9863\n",
      "Epoch 76/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0222 - accuracy: 0.9878\n",
      "Epoch 76: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9876 - val_loss: 0.0495 - val_accuracy: 0.9945\n",
      "Epoch 77/200\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0361 - accuracy: 0.9896\n",
      "Epoch 77: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0408 - accuracy: 0.9890 - val_loss: 0.0863 - val_accuracy: 0.9753\n",
      "Epoch 78/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0302 - accuracy: 0.9917  \n",
      "Epoch 78: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9917 - val_loss: 0.0718 - val_accuracy: 0.9808\n",
      "Epoch 79/200\n",
      "28/46 [=================>............] - ETA: 0s - loss: 0.0275 - accuracy: 0.9900    \n",
      "Epoch 79: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.0610 - val_accuracy: 0.9835\n",
      "Epoch 80/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0228 - accuracy: 0.9952\n",
      "Epoch 80: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9876 - val_loss: 0.1206 - val_accuracy: 0.9808\n",
      "Epoch 81/200\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 0.0277 - accuracy: 0.9896    \n",
      "Epoch 81: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9890 - val_loss: 0.0688 - val_accuracy: 0.9890\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9966  \n",
      "Epoch 82: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 0.9966 - val_loss: 0.0525 - val_accuracy: 0.9890\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9910  \n",
      "Epoch 83: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.9910 - val_loss: 0.0320 - val_accuracy: 0.9890\n",
      "Epoch 84/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0247 - accuracy: 0.9903\n",
      "Epoch 84: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.9904 - val_loss: 0.0613 - val_accuracy: 0.9918\n",
      "Epoch 85/200\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0443 - accuracy: 0.9840  \n",
      "Epoch 85: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0420 - accuracy: 0.9848 - val_loss: 0.0771 - val_accuracy: 0.9863\n",
      "Epoch 86/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0363 - accuracy: 0.9886  \n",
      "Epoch 86: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.9890 - val_loss: 0.0676 - val_accuracy: 0.9863\n",
      "Epoch 87/200\n",
      "22/46 [=============>................] - ETA: 0s - loss: 0.0191 - accuracy: 0.9943    \n",
      "Epoch 87: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9952 - val_loss: 0.0233 - val_accuracy: 0.9890\n",
      "Epoch 88/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0020 - accuracy: 0.9975    \n",
      "Epoch 88: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9945 - val_loss: 0.0222 - val_accuracy: 0.9890\n",
      "Epoch 89/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0166 - accuracy: 0.9909    \n",
      "Epoch 89: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9890 - val_loss: 0.0427 - val_accuracy: 0.9890\n",
      "Epoch 90/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0226 - accuracy: 0.9916\n",
      "Epoch 90: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.9917 - val_loss: 0.0064 - val_accuracy: 0.9918\n",
      "Epoch 91/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0776 - accuracy: 0.9870    \n",
      "Epoch 91: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9904 - val_loss: 0.0245 - val_accuracy: 0.9945\n",
      "Epoch 92/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0209 - accuracy: 0.9904    \n",
      "Epoch 92: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9938 - val_loss: 0.1257 - val_accuracy: 0.9835\n",
      "Epoch 93/200\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 0.0398 - accuracy: 0.9932\n",
      "Epoch 93: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 0.9904 - val_loss: 0.0984 - val_accuracy: 0.9863\n",
      "Epoch 94/200\n",
      "27/46 [================>.............] - ETA: 0s - loss: 0.0255 - accuracy: 0.9919\n",
      "Epoch 94: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.9904 - val_loss: 0.0717 - val_accuracy: 0.9890\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9904  \n",
      "Epoch 95: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.9904 - val_loss: 0.0771 - val_accuracy: 0.9863\n",
      "Epoch 96/200\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0173 - accuracy: 0.9920\n",
      "Epoch 96: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 0.9924 - val_loss: 0.0607 - val_accuracy: 0.9918\n",
      "Epoch 97/200\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0178 - accuracy: 0.9935  \n",
      "Epoch 97: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.9931 - val_loss: 0.0488 - val_accuracy: 0.9835\n",
      "Epoch 98/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0331 - accuracy: 0.9922\n",
      "Epoch 98: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 0.9924 - val_loss: 0.1012 - val_accuracy: 0.9835\n",
      "Epoch 99/200\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 0.0390 - accuracy: 0.9914\n",
      "Epoch 99: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0378 - accuracy: 0.9917 - val_loss: 0.1120 - val_accuracy: 0.9835\n",
      "Epoch 100/200\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 0.0189 - accuracy: 0.9914  \n",
      "Epoch 100: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.9910 - val_loss: 0.0847 - val_accuracy: 0.9863\n",
      "Epoch 101/200\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.0204 - accuracy: 0.9934  \n",
      "Epoch 101: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9924 - val_loss: 0.1059 - val_accuracy: 0.9835\n",
      "Epoch 102/200\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 0.0077 - accuracy: 0.9896\n",
      "Epoch 102: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.9897 - val_loss: 0.1226 - val_accuracy: 0.9808\n",
      "Epoch 103/200\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0333 - accuracy: 0.9884  \n",
      "Epoch 103: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0341 - accuracy: 0.9883 - val_loss: 0.1052 - val_accuracy: 0.9863\n",
      "Epoch 104/200\n",
      "30/46 [==================>...........] - ETA: 0s - loss: 0.0177 - accuracy: 0.9927    \n",
      "Epoch 104: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.9917 - val_loss: 0.1029 - val_accuracy: 0.9835\n",
      "Epoch 105/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0189 - accuracy: 0.9935    \n",
      "Epoch 105: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.9931 - val_loss: 0.1019 - val_accuracy: 0.9835\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9938  \n",
      "Epoch 106: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9938 - val_loss: 0.1079 - val_accuracy: 0.9835\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9945    \n",
      "Epoch 107: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9945 - val_loss: 0.1236 - val_accuracy: 0.9808\n",
      "Epoch 108/200\n",
      "36/46 [======================>.......] - ETA: 0s - loss: 0.0162 - accuracy: 0.9896\n",
      "Epoch 108: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 0.9890 - val_loss: 0.1264 - val_accuracy: 0.9780\n",
      "Epoch 109/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0192 - accuracy: 0.9918    \n",
      "Epoch 109: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.9917 - val_loss: 0.0880 - val_accuracy: 0.9863\n",
      "Epoch 110/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0100 - accuracy: 0.9918    \n",
      "Epoch 110: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.9938 - val_loss: 0.0662 - val_accuracy: 0.9835\n",
      "Epoch 111/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0196 - accuracy: 0.9900    \n",
      "Epoch 111: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.9924 - val_loss: 0.0341 - val_accuracy: 0.9808\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9959  \n",
      "Epoch 112: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0076 - accuracy: 0.9959 - val_loss: 0.0409 - val_accuracy: 0.9863\n",
      "Epoch 113/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0085 - accuracy: 0.9932    \n",
      "Epoch 113: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0158 - accuracy: 0.9924 - val_loss: 0.0425 - val_accuracy: 0.9835\n",
      "Epoch 114/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0645 - accuracy: 0.9922\n",
      "Epoch 114: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0483 - accuracy: 0.9917 - val_loss: 0.1193 - val_accuracy: 0.9808\n",
      "Epoch 115/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0266 - accuracy: 0.9922    \n",
      "Epoch 115: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.9931 - val_loss: 0.0680 - val_accuracy: 0.9890\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9862  \n",
      "Epoch 116: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.9862 - val_loss: 0.0745 - val_accuracy: 0.9863\n",
      "Epoch 117/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0310 - accuracy: 0.9887\n",
      "Epoch 117: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0389 - accuracy: 0.9876 - val_loss: 0.0888 - val_accuracy: 0.9863\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9931  \n",
      "Epoch 118: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.0887 - val_accuracy: 0.9835\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9897\n",
      "Epoch 119: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.9897 - val_loss: 0.0620 - val_accuracy: 0.9863\n",
      "Epoch 120/200\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 0.0462 - accuracy: 0.9880\n",
      "Epoch 120: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 0.9890 - val_loss: 0.0987 - val_accuracy: 0.9890\n",
      "Epoch 121/200\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0267 - accuracy: 0.9916  \n",
      "Epoch 121: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.9924 - val_loss: 0.0805 - val_accuracy: 0.9863\n",
      "Epoch 122/200\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0193 - accuracy: 0.9870  \n",
      "Epoch 122: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9869 - val_loss: 0.0785 - val_accuracy: 0.9890\n",
      "Epoch 123/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0285 - accuracy: 0.9891\n",
      "Epoch 123: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.9876 - val_loss: 0.0719 - val_accuracy: 0.9835\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9945  \n",
      "Epoch 124: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.9945 - val_loss: 0.1275 - val_accuracy: 0.9808\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9904  \n",
      "Epoch 125: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0367 - accuracy: 0.9904 - val_loss: 0.1138 - val_accuracy: 0.9835\n",
      "Epoch 126/200\n",
      "43/46 [===========================>..] - ETA: 0s - loss: 0.0382 - accuracy: 0.9935  \n",
      "Epoch 126: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0426 - accuracy: 0.9924 - val_loss: 0.1242 - val_accuracy: 0.9753\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9897  \n",
      "Epoch 127: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.9897 - val_loss: 0.1336 - val_accuracy: 0.9698\n",
      "Epoch 128/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0401 - accuracy: 0.9870\n",
      "Epoch 128: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0385 - accuracy: 0.9869 - val_loss: 0.0985 - val_accuracy: 0.9808\n",
      "Epoch 129/200\n",
      "28/46 [=================>............] - ETA: 0s - loss: 0.0368 - accuracy: 0.9922    \n",
      "Epoch 129: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0370 - accuracy: 0.9917 - val_loss: 0.1395 - val_accuracy: 0.9753\n",
      "Epoch 130/200\n",
      "35/46 [=====================>........] - ETA: 0s - loss: 0.0460 - accuracy: 0.9884    \n",
      "Epoch 130: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0377 - accuracy: 0.9890 - val_loss: 0.1160 - val_accuracy: 0.9808\n",
      "Epoch 131/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0819 - accuracy: 0.9844    \n",
      "Epoch 131: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9869 - val_loss: 0.0977 - val_accuracy: 0.9808\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9938  \n",
      "Epoch 132: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9938 - val_loss: 0.1214 - val_accuracy: 0.9780\n",
      "Epoch 133/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0651 - accuracy: 0.9864\n",
      "Epoch 133: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9876 - val_loss: 0.0812 - val_accuracy: 0.9890\n",
      "Epoch 134/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9833  \n",
      "Epoch 134: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.9835 - val_loss: 0.1417 - val_accuracy: 0.9863\n",
      "Epoch 135/200\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0680 - accuracy: 0.9866  \n",
      "Epoch 135: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9848 - val_loss: 0.1271 - val_accuracy: 0.9753\n",
      "Epoch 136/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0307 - accuracy: 0.9912\n",
      "Epoch 136: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9862 - val_loss: 0.1051 - val_accuracy: 0.9780\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9883  \n",
      "Epoch 137: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0426 - accuracy: 0.9883 - val_loss: 0.1717 - val_accuracy: 0.9808\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9869\n",
      "Epoch 138: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0380 - accuracy: 0.9869 - val_loss: 0.1408 - val_accuracy: 0.9808\n",
      "Epoch 139/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0698 - accuracy: 0.9883    \n",
      "Epoch 139: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9904 - val_loss: 0.0918 - val_accuracy: 0.9863\n",
      "Epoch 140/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0495 - accuracy: 0.9883    \n",
      "Epoch 140: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.9931 - val_loss: 0.0644 - val_accuracy: 0.9780\n",
      "Epoch 141/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0189 - accuracy: 0.9918\n",
      "Epoch 141: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9904 - val_loss: 0.1314 - val_accuracy: 0.9725\n",
      "Epoch 142/200\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 0.0229 - accuracy: 0.9943\n",
      "Epoch 142: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9931 - val_loss: 0.0596 - val_accuracy: 0.9835\n",
      "Epoch 143/200\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 0.0183 - accuracy: 0.9924    \n",
      "Epoch 143: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9938 - val_loss: 0.0756 - val_accuracy: 0.9863\n",
      "Epoch 144/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0160 - accuracy: 0.9928\n",
      "Epoch 144: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.9924 - val_loss: 0.1210 - val_accuracy: 0.9863\n",
      "Epoch 145/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0027 - accuracy: 0.9948\n",
      "Epoch 145: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9952 - val_loss: 0.0642 - val_accuracy: 0.9808\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9917  \n",
      "Epoch 146: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9917 - val_loss: 0.1288 - val_accuracy: 0.9780\n",
      "Epoch 147/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0066 - accuracy: 0.9952    \n",
      "Epoch 147: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9931 - val_loss: 0.0997 - val_accuracy: 0.9835\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9890  \n",
      "Epoch 148: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9890 - val_loss: 0.1047 - val_accuracy: 0.9863\n",
      "Epoch 149/200\n",
      "22/46 [=============>................] - ETA: 0s - loss: 0.0130 - accuracy: 0.9886\n",
      "Epoch 149: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0158 - accuracy: 0.9890 - val_loss: 0.1305 - val_accuracy: 0.9863\n",
      "Epoch 150/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0402 - accuracy: 0.9912\n",
      "Epoch 150: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.9917 - val_loss: 0.0815 - val_accuracy: 0.9863\n",
      "Epoch 151/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0155 - accuracy: 0.9937    \n",
      "Epoch 151: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.9924 - val_loss: 0.0874 - val_accuracy: 0.9863\n",
      "Epoch 152/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0231 - accuracy: 0.9910\n",
      "Epoch 152: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 0.9910 - val_loss: 0.0862 - val_accuracy: 0.9835\n",
      "Epoch 153/200\n",
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0253 - accuracy: 0.9875    \n",
      "Epoch 153: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.9897 - val_loss: 0.0961 - val_accuracy: 0.9835\n",
      "Epoch 154/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9951\n",
      "Epoch 154: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 5ms/step - loss: 0.0196 - accuracy: 0.9952 - val_loss: 0.1030 - val_accuracy: 0.9808\n",
      "Epoch 155/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0282 - accuracy: 0.9935    \n",
      "Epoch 155: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.9931 - val_loss: 0.1344 - val_accuracy: 0.9808\n",
      "Epoch 156/200\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 0.9967  \n",
      "Epoch 156: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9959 - val_loss: 0.1495 - val_accuracy: 0.9835\n",
      "Epoch 157/200\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0143 - accuracy: 0.9948\n",
      "Epoch 157: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9945 - val_loss: 0.1068 - val_accuracy: 0.9863\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9917  \n",
      "Epoch 158: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.9917 - val_loss: 0.0907 - val_accuracy: 0.9835\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9959  \n",
      "Epoch 159: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9959 - val_loss: 0.1224 - val_accuracy: 0.9808\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9952  \n",
      "Epoch 160: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9952 - val_loss: 0.0751 - val_accuracy: 0.9835\n",
      "Epoch 161/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0512 - accuracy: 0.9893\n",
      "Epoch 161: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0496 - accuracy: 0.9897 - val_loss: 0.1363 - val_accuracy: 0.9780\n",
      "Epoch 162/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0209 - accuracy: 0.9909    \n",
      "Epoch 162: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9897 - val_loss: 0.0878 - val_accuracy: 0.9890\n",
      "Epoch 163/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0504 - accuracy: 0.9870    \n",
      "Epoch 163: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9848 - val_loss: 0.1674 - val_accuracy: 0.9835\n",
      "Epoch 164/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0269 - accuracy: 0.9932    \n",
      "Epoch 164: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0354 - accuracy: 0.9890 - val_loss: 0.1790 - val_accuracy: 0.9808\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9883  \n",
      "Epoch 165: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9883 - val_loss: 0.1173 - val_accuracy: 0.9835\n",
      "Epoch 166/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0576 - accuracy: 0.9868    \n",
      "Epoch 166: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 0.9890 - val_loss: 0.0832 - val_accuracy: 0.9863\n",
      "Epoch 167/200\n",
      "34/46 [=====================>........] - ETA: 0s - loss: 0.0087 - accuracy: 0.9963    \n",
      "Epoch 167: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9938 - val_loss: 0.1161 - val_accuracy: 0.9835\n",
      "Epoch 168/200\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 0.0150 - accuracy: 0.9951    \n",
      "Epoch 168: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.9938 - val_loss: 0.0933 - val_accuracy: 0.9835\n",
      "Epoch 169/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0306 - accuracy: 0.9909    \n",
      "Epoch 169: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0553 - accuracy: 0.9876 - val_loss: 0.0580 - val_accuracy: 0.9890\n",
      "Epoch 170/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0565 - accuracy: 0.9870    \n",
      "Epoch 170: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0419 - accuracy: 0.9883 - val_loss: 0.1264 - val_accuracy: 0.9835\n",
      "Epoch 171/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0503 - accuracy: 0.9868  \n",
      "Epoch 171: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0499 - accuracy: 0.9869 - val_loss: 0.1627 - val_accuracy: 0.9863\n",
      "Epoch 172/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0177 - accuracy: 0.9896    \n",
      "Epoch 172: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9883 - val_loss: 0.1313 - val_accuracy: 0.9808\n",
      "Epoch 173/200\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 0.0423 - accuracy: 0.9888\n",
      "Epoch 173: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9876 - val_loss: 0.0936 - val_accuracy: 0.9890\n",
      "Epoch 174/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0312 - accuracy: 0.9910  \n",
      "Epoch 174: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.9910 - val_loss: 0.0985 - val_accuracy: 0.9780\n",
      "Epoch 175/200\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.0206 - accuracy: 0.9942  \n",
      "Epoch 175: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9931 - val_loss: 0.1005 - val_accuracy: 0.9863\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/46 [===============>..............] - ETA: 0s - loss: 0.0347 - accuracy: 0.9912    \n",
      "Epoch 176: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 0.9917 - val_loss: 0.0850 - val_accuracy: 0.9890\n",
      "Epoch 177/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0373 - accuracy: 0.9946    \n",
      "Epoch 177: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0555 - accuracy: 0.9883 - val_loss: 0.1029 - val_accuracy: 0.9863\n",
      "Epoch 178/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0520 - accuracy: 0.9883    \n",
      "Epoch 178: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9883 - val_loss: 0.1365 - val_accuracy: 0.9808\n",
      "Epoch 179/200\n",
      "31/46 [===================>..........] - ETA: 0s - loss: 0.0183 - accuracy: 0.9899    \n",
      "Epoch 179: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9924 - val_loss: 0.1502 - val_accuracy: 0.9835\n",
      "Epoch 180/200\n",
      "33/46 [====================>.........] - ETA: 0s - loss: 0.0343 - accuracy: 0.9915    \n",
      "Epoch 180: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.9924 - val_loss: 0.1493 - val_accuracy: 0.9863\n",
      "Epoch 181/200\n",
      "26/46 [===============>..............] - ETA: 0s - loss: 0.0134 - accuracy: 0.9892\n",
      "Epoch 181: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.9862 - val_loss: 0.1305 - val_accuracy: 0.9863\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9917  \n",
      "Epoch 182: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.9917 - val_loss: 0.1111 - val_accuracy: 0.9863\n",
      "Epoch 183/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0319 - accuracy: 0.9935    \n",
      "Epoch 183: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.9945 - val_loss: 0.0974 - val_accuracy: 0.9890\n",
      "Epoch 184/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0374 - accuracy: 0.9857\n",
      "Epoch 184: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.9883 - val_loss: 0.1060 - val_accuracy: 0.9808\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9917\n",
      "Epoch 185: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 0.9917 - val_loss: 0.0584 - val_accuracy: 0.9863\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9883  \n",
      "Epoch 186: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.9883 - val_loss: 0.0891 - val_accuracy: 0.9808\n",
      "Epoch 187/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0387 - accuracy: 0.9893  \n",
      "Epoch 187: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9890 - val_loss: 0.0959 - val_accuracy: 0.9808\n",
      "Epoch 188/200\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9847  \n",
      "Epoch 188: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.9848 - val_loss: 0.0819 - val_accuracy: 0.9945\n",
      "Epoch 189/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0324 - accuracy: 0.9922    \n",
      "Epoch 189: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9876 - val_loss: 0.0849 - val_accuracy: 0.9918\n",
      "Epoch 190/200\n",
      "24/46 [==============>...............] - ETA: 0s - loss: 0.0585 - accuracy: 0.9883    \n",
      "Epoch 190: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0367 - accuracy: 0.9917 - val_loss: 0.0393 - val_accuracy: 0.9890\n",
      "Epoch 191/200\n",
      "23/46 [==============>...............] - ETA: 0s - loss: 0.0057 - accuracy: 0.9946    \n",
      "Epoch 191: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.9966 - val_loss: 0.0515 - val_accuracy: 0.9890\n",
      "Epoch 192/200\n",
      "32/46 [===================>..........] - ETA: 0s - loss: 0.0339 - accuracy: 0.9922    \n",
      "Epoch 192: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.9945 - val_loss: 0.0436 - val_accuracy: 0.9890\n",
      "Epoch 193/200\n",
      "28/46 [=================>............] - ETA: 0s - loss: 0.0285 - accuracy: 0.9944    \n",
      "Epoch 193: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.9924 - val_loss: 0.0737 - val_accuracy: 0.9863\n",
      "Epoch 194/200\n",
      "27/46 [================>.............] - ETA: 0s - loss: 0.0420 - accuracy: 0.9884    \n",
      "Epoch 194: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0350 - accuracy: 0.9869 - val_loss: 0.0316 - val_accuracy: 0.9863\n",
      "Epoch 195/200\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 0.0538 - accuracy: 0.9896  \n",
      "Epoch 195: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9897 - val_loss: 0.2358 - val_accuracy: 0.9670\n",
      "Epoch 196/200\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.0814 - accuracy: 0.9827\n",
      "Epoch 196: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.9848 - val_loss: 0.0898 - val_accuracy: 0.9753\n",
      "Epoch 197/200\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 0.0258 - accuracy: 0.9904  \n",
      "Epoch 197: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0316 - accuracy: 0.9897 - val_loss: 0.1297 - val_accuracy: 0.9835\n",
      "Epoch 198/200\n",
      "40/46 [=========================>....] - ETA: 0s - loss: 0.0301 - accuracy: 0.9891  \n",
      "Epoch 198: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9897 - val_loss: 0.1586 - val_accuracy: 0.9808\n",
      "Epoch 199/200\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 0.0542 - accuracy: 0.9886\n",
      "Epoch 199: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.9890 - val_loss: 0.1050 - val_accuracy: 0.9863\n",
      "Epoch 200/200\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.0432 - accuracy: 0.9855\n",
      "Epoch 200: val_accuracy did not improve from 0.99725\n",
      "46/46 [==============================] - 0s 4ms/step - loss: 0.0412 - accuracy: 0.9862 - val_loss: 0.0870 - val_accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "history =model_NN.fit(X_train,Y_train,validation_data=[X_test,Y_test] ,batch_size=32, \n",
    "             epochs=200 , verbose =1,callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b0d3a680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364, 23)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred= np.argmax(model_NN.predict(X_test),axis=1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "84cc9326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        52\n",
      "           1       1.00      0.99      0.99        67\n",
      "           2       1.00      0.98      0.99       118\n",
      "           3       0.98      1.00      0.99       127\n",
      "\n",
      "    accuracy                           0.99       364\n",
      "   macro avg       0.99      0.99      0.99       364\n",
      "weighted avg       0.99      0.99      0.99       364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dfc568fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9972527623176575"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.argmax(history.history[\"val_accuracy\"])\n",
    "history.history[\"val_accuracy\"][np.argmax(history.history[\"val_accuracy\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "484919f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9917355179786682"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"accuracy\"][np.argmax(history.history[\"val_accuracy\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419e06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
